2024-12-01 20:26:10
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-01 20:26:38
[
    {
        "arg": "dictionary",
        "description": "(OrderedDict) \u2013 The input dictionary to be mapped to CPU. (default: None)"
    },
    {
        "arg": "return_dict",
        "description": "(bool) \u2013 Whether to return the results in a new OrderedDict or not. (default: False)"
    }
]
2024-12-01 20:27:00
class DataCollectorBase description: 
[blue]This class is a base class for data collectors in PyTorch, providing a common interface for updating model weights and other state during training or testing. It allows for flexible customization of the data collection process through its methods, such as `update_policy_weights_` and `set_seed`, making it a crucial component in building robust and efficient machine learning pipelines.[/blue]
2024-12-01 20:27:27
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in PyTorch, ensuring consistency across devices when using data collectors and trained policies.[/blue]
2024-12-01 20:27:52
[
    {
        "arg": "policy_weights",
        "description": " (Optional[TensorDictBase]) \u2013 The policy weights to be updated. (default: None)"
    }
]
2024-12-01 20:28:09
function next description: 
[blue]This function is used to retrieve the next item from an iterator, handling cases where the iterator has reached its end.[/blue]
2024-12-01 20:28:30
[
    {
        "arg": "self",
        "description": "(DataCollectorBase instance) \u2013 The current data collector instance. (default: None)"
    }
]
2024-12-01 20:28:45
function shutdown description: 
[blue]This function is an abstract method that serves as a hook for customizing the shutdown process of a PyTorch module, allowing developers to implement specific cleanup actions before the module is unloaded.[/blue]
2024-12-01 20:29:07
[
    {
        "arg": "self",
        "description": "(DataCollectorBase) \u2013 The instance of DataCollectorBase class. (default: None)"
    }
]
2024-12-01 20:29:21
function iterator description: 
[blue]This function is an abstract base class method that defines the interface for creating an iterator over a TensorDict, allowing for iteration over its key-value pairs or values.[/blue]
2024-12-01 20:30:04
[
    {
        "arg": "self",
        "description": "(self) \u2013 The instance of DataCollectorBase class. (default: None)"
    },
    {
        "arg": "T",
        "type": "TypeVar",
        "description": "(T) \u2013 Type variable for the data type stored in TensorDictBase."
    },
    {
        "arg": "TensorDictBase",
        "type": "class",
        "description": "(TensorDictBase) \u2013 The base class of the dictionary containing tensors."
    },
    {
        "arg": "Iterator[TensorDictBase]",
        "type": "function return type",
        "description": "(Iterator[TensorDictBase]) \u2013 An iterator over TensorDictBase instances."
    }
]
2024-12-01 20:30:21
function set_seed description: 
[blue]This function is used to set a random seed for reproducibility in PyTorch models, ensuring that the same sequence of random numbers is generated during training and testing, thereby facilitating reliable comparisons and debugging.[/blue]
2024-12-01 20:30:49
[
    {
        "arg": "seed",
        "description": " (int) \u2013 The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) \u2013 Whether the static seed should be used. (default: False)"
    }
]
2024-12-01 20:31:03
function state_dict description: 
[blue]This function is used to serialize and retrieve the model's weights and biases into a dictionary, allowing for easy saving and loading of the model's state.[/blue]
2024-12-01 20:31:31
[
    {
        "arg": "self",
        "description": "(DataCollectorBase instance) \u2013 The DataCollectorBase instance to retrieve the state dictionary from. (default: None)"
    },
    {
        "arg": "OrderedDict",
        "description": " \u2013 The ordered dictionary containing the model's state. (default: None)"
    }
]
2024-12-01 20:31:46
function load_state_dict description: 
[blue]This function is used to load the state dictionary of a PyTorch model from a given state dictionary, allowing for model checkpointing and resuming training from a saved state.[/blue]
2024-12-01 20:32:16
[
    {
        "arg": "self",
        "description": "(None) \u2013 The instance of DataCollectorBase to update with the new state dictionary. (default: None)"
    },
    {
        "arg": "state_dict",
        "description": "(OrderedDict) \u2013 The state dictionary to load into the model. (default: None)"
    }
]
2024-12-01 20:32:32
class SyncDataCollector description: 
[blue]This class is a data collector used to synchronize and collect data from multiple workers in a distributed training setup, ensuring consistency across the model's parameters.[/blue]
2024-12-01 20:39:18
function __init__ description: 
[blue]This function is the constructor for a class that initializes various attributes and settings for an environment, policy, and rollout. It takes in numerous keyword arguments to customize its behavior, including batch size, frames per batch, total frames, exploration type, and more. The class appears to be designed for reinforcement learning tasks, particularly those involving batched environments and policies.[/blue]
2024-12-01 20:43:27
[
    {
        "arg": "create_env_fn",
        "description": "(Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]]) \u2013 Function to create environment. (default: None)"
    },
    {
        "arg": "policy",
        "description": "(Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]) \u2013 Policy function or module. (default: None)"
    },
    {
        "arg": "frames_per_batch",
        "description": "(int) \u2013 Number of frames per batch. (default: -1)"
    },
    {
        "arg": "total_frames",
        "description": "(int) \u2013 Total number of frames. (default: -1)"
    },
    {
        "arg": "device",
        "description": "(DEVICE_TYPING) \u2013 Device to use for computation. (default: None)"
    },
    {
        "arg": "storing_device",
        "description": "(DEVICE_TYPING) \u2013 Device to store data on. (default: None)"
    },
    {
        "arg": "policy_device",
        "description": "(DEVICE_TYPING) \u2013 Device for policy computation. (default: None)"
    },
    {
        "arg": "env_device",
        "description": "(DEVICE_TYPING) \u2013 Device for environment computation. (default: None)"
    },
    {
        "arg": "create_env_kwargs",
        "description": "(dict | None) \u2013 Keyword arguments to pass to create_env_fn. (default: None)"
    },
    {
        "arg": "max_frames_per_traj",
        "description": "(int | None) \u2013 Maximum number of frames per trajectory. (default: None)"
    },
    {
        "arg": "init_random_frames",
        "description": "(int | None) \u2013 Number of random frames to initialize with. (default: None)"
    },
    {
        "arg": "reset_at_each_iter",
        "description": "(bool) \u2013 Whether to reset at each iteration. (default: False)"
    },
    {
        "arg": "postproc",
        "description": "(Callable[[TensorDictBase], TensorDictBase] | None) \u2013 Post-processing function for tensor dict. (default: None)"
    },
    {
        "arg": "split_trajs",
        "description": "(bool | None) \u2013 Whether to split trajectories. (default: None)"
    },
    {
        "arg": "exploration_type",
        "description": "(ExplorationType) \u2013 Type of exploration. (default: DEFAULT_EXPLORATION_TYPE)"
    },
    {
        "arg": "return_same_td",
        "description": "(bool) \u2013 Whether to return same td. (default: False)"
    },
    {
        "arg": "reset_when_done",
        "description": "(bool) \u2013 Whether to reset when done. (default: True)"
    },
    {
        "arg": "interruptor",
        "description": "(None) \u2013 Interruptor object."
    },
    {
        "arg": "set_truncated",
        "description": "(bool) \u2013 Whether to set truncated. (default: False)"
    },
    {
        "arg": "use_buffers",
        "description": "(bool | None) \u2013 Whether to use buffers. (default: None)"
    },
    {
        "arg": "replay_buffer",
        "description": "(ReplayBuffer | None) \u2013 Replay buffer object."
    },
    {
        "arg": "trust_policy",
        "description": "(None) \u2013 Trust policy flag."
    },
    {
        "arg": "compile_policy",
        "description": "(Dict[str, Any] | bool | None) \u2013 Compile policy flag or dictionary. (default: None)"
    },
    {
        "arg": "cudagraph_policy",
        "description": "(Dict[str, Any] | bool | None) \u2013 Cuda graph policy flag or dictionary. (default: None)"
    }
]
2024-12-01 20:43:43
function next description: 
[blue]This function is a placeholder that delegates to the parent class's `next` method, effectively bypassing any custom implementation. It should be used when you want to maintain the default behavior of the parent class while still providing a consistent interface for subclasses.[/blue]
2024-12-01 20:44:06
[
    {
        "arg": "self",
        "description": "(torchvision.utils.data.Dataloader) \u2013 The data loader to get the next batch from. (default: None)"
    }
]
2024-12-01 20:44:22
function update_policy_weights_ description: 
[blue]This function is a method that updates the weights of a policy in a reinforcement learning model by calling the parent class's `update_policy_weights_` method.[/blue]
2024-12-01 20:44:52
[
    {
        "arg": "policy_weights",
        "description": " (Optional[TensorDictBase]) \u2013 The policy weights to be updated. (default: None)"
    },
    {
        "arg": "self",
        "description": " (<class 'SyncDataCollector'>) \u2013 The instance of SyncDataCollector"
    }
]
2024-12-01 20:45:42
function set_seed description: 
[blue]This function is used to set the seeds for environments in a DataCollector, allowing for reproducibility and incrementing of seeds across multiple environments.[/blue]
2024-12-01 20:46:09
[
    {
        "arg": "seed",
        "description": " (int) \u2013 The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) \u2013 Whether the static seed is used. (default: False)"
    }
]
2024-12-01 20:47:40
function iterator description: 
[blue]This function is used to iterate through a DataCollector, yielding TensorDictBase objects containing chunks of trajectories. It manages CUDA streams and events for efficient data collection on GPU devices.[/blue]
2024-12-01 20:48:14
[
    {
        "arg": "self",
        "description": "(Iterator[TensorDictBase]) \u2013 The data collector instance. (default: None)"
    },
    {
        "arg": "T",
        "description": "(type) \u2013 The type of the data to be collected. (default: None)"
    },
    {
        "arg": "D",
        "description": "(type) \u2013 The type of the dictionary in the TensorDictBase. (default: dict)"
    }
]
2024-12-01 20:50:46
function rollout description: 
[blue]This function is used to generate a rollout in the environment using the provided policy. It returns a TensorDictBase containing the computed rollout and can be used for training reinforcement learning agents by sampling multiple rollouts from the same policy and averaging their outputs.[/blue]
2024-12-01 20:51:24
[
    {
        "arg": "num_steps",
        "description": "(int) \u2013 Number of steps to roll out. (default: 1)"
    },
    {
        "arg": "batch_size",
        "description": "(int) \u2013 Batch size for rollout. (default: 1)"
    },
    {
        "arg": "device",
        "description": "(torch.device or str) \u2013 Device to run the rollout on. (default: None)"
    },
    {
        "arg": "deterministic",
        "description": "(bool) \u2013 Whether to use deterministic rollout. (default: False)"
    }
]
2024-12-01 20:52:02
function reset description: 
[blue]This function is used to reset environments in a PyTorch environment, updating the internal state and metadata accordingly.[/blue]
2024-12-01 20:52:33
[
    {
        "arg": "self",
        "description": "(NoneType) \u2013 The instance of SyncDataCollector class. (default: None)"
    },
    {
        "arg": "index",
        "description": "(int or NoneType) \u2013 The index to reset the data collector for. If None, resets all data collectors. (default: None)"
    }
]
2024-12-01 20:52:53
function shutdown description: 
[blue]This function is used to cleanly shut down a PyTorch environment, releasing system resources and ensuring proper cleanup.[/blue]
2024-12-01 20:53:16
[
    {
        "arg": "self",
        "description": "(NoneType) \u2013 The instance of SyncDataCollector class. (default: None)"
    }
]
2024-12-01 20:53:58
function state_dict description: 
[blue]This function is used to retrieve and combine the local state dictionaries of an environment and a policy in PyTorch RL. It returns an ordered dictionary containing both environment and policy state dictionaries, along with additional metadata such as frames and iteration number.[/blue]
2024-12-01 20:54:27
[
    {
        "arg": "self",
        "description": "(None) \u2013 The instance of SyncDataCollector class. (default: None)"
    },
    {
        "arg": "OrderedDict",
        "description": "(<type>) \u2013 A dictionary-like object used to store the model's state. (default: None)"
    }
]
2024-12-01 20:54:58
function load_state_dict description: 
[blue]This function is used to load a pre-trained model's state dictionary into the environment and policy, allowing for the resumption of training or inference from a previously saved checkpoint.[/blue]
2024-12-01 20:55:29
[
    {
        "arg": "state_dict",
        "description": "(OrderedDict) \u2013 The dictionary containing the model's state. (default: None)"
    },
    {
        "arg": "kwargs",
        "description": "(**kwargs) \u2013 Additional keyword arguments to be passed to the underlying PyTorch function. (default: None)"
    }
]
2024-12-01 20:55:45
class MultiSyncDataCollector description: 
[blue]This class is a PyTorch data collector that synchronizes and batches data across multiple workers, ensuring consistent data distribution for training and evaluation.[/blue]
2024-12-01 20:56:01
function next description: 
[blue]This function is a placeholder that delegates to the parent class's `next` method, effectively bypassing any custom implementation. It should be used when you want to maintain the default behavior of the parent class while still providing a consistent interface for subclasses.[/blue]
2024-12-01 20:56:31
[
    {
        "arg": "self",
        "description": "(MultiSyncDataCollector instance) \u2013 The current data collector instance. (default: None)"
    },
    {
        "arg": "data",
        "description": "(list of dict) \u2013 A list of dictionaries containing the data to be collected. Each dictionary should have 'key' and 'value' keys."
    }
]
2024-12-01 20:56:48
function shutdown description: 
[blue]This function is used to release resources held by a PyTorch model during shutdown, ensuring memory safety and preventing potential memory leaks.[/blue]
2024-12-01 20:57:09
[
    {
        "arg": "self",
        "description": "(None) \u2013 The instance of MultiSyncDataCollector. (default: None)"
    }
]
2024-12-01 20:57:28
function set_seed description: 
[blue]This function is used to set a random seed for reproducibility in PyTorch models. It ensures that the same sequence of random numbers is generated every time the model is run with the same seed value, making it easier to debug and test models.[/blue]
2024-12-01 20:57:58
[
    {
        "arg": "seed",
        "description": " (int) \u2013 The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) \u2013 Whether the static seed is used. (default: False)"
    }
]
2024-12-01 20:58:12
function state_dict description: 
[blue]This function is used to retrieve the model's state dictionary, which contains the learned parameters and their values, allowing for model loading and saving.[/blue]
2024-12-01 20:58:45
[
    {
        "arg": "self",
        "description": "(torchvision.utils.data.DataloaderIterator) \u2013 The data iterator to synchronize with. (default: None)"
    },
    {
        "arg": "prefix",
        "description": "(str) \u2013 Prefix for the keys in the state dictionary. (default: 'data')"
    },
    {
        "arg": "local_rank",
        "description": "(int) \u2013 Local rank of this process. (default: 0)"
    }
]
2024-12-01 20:59:03
function load_state_dict description: 
[blue]This function is used to load the state dictionary of a PyTorch model from an existing checkpoint or saved model. It allows for the transfer of learned parameters and other model state between different runs or environments, enabling faster development and deployment.[/blue]
2024-12-01 20:59:35
[
    {
        "arg": "self",
        "description": "(None) \u2013 The instance of MultiSyncDataCollector. (default: None)"
    },
    {
        "arg": "state_dict",
        "description": "(OrderedDict) \u2013 The model state dictionary to be loaded into the current state."
    },
    {
        "arg": "None",
        "description": "(None) \u2013 Not used in this function."
    }
]
2024-12-01 20:59:52
function update_policy_weights_ description: 
[blue]This function is a method that updates the weights of a policy in a reinforcement learning model by calling the parent class's `update_policy_weights_` method.[/blue]
2024-12-01 21:00:24
[
    {
        "arg": "policy_weights",
        "description": " (Optional[TensorDictBase]) \u2013 The policy weights to be updated. (default: None)"
    },
    {
        "arg": "self",
        "description": " (<class 'MultiSyncDataCollector'>) \u2013 The instance of the class."
    }
]
2024-12-01 21:00:53
function frames_per_batch_worker description: 
[blue]This function is used to calculate the effective `frames_per_batch` value for a batch of frames when using multiple collector workers, ensuring that the requested FPS is achieved despite the increased number of frames per iteration.[/blue]
2024-12-01 21:01:17
[
    {
        "arg": "frames_per_batch_worker",
        "description": "(int) \u2013 The number of frames to process in each batch. (default: 1)"
    }
]
2024-12-01 21:04:56
function iterator description: 
[blue]This function is a PyTorch iterator that generates batches of data from a collection of buffers, applying various transformations and filtering criteria along the way. It's used to process and combine data from multiple workers in a distributed environment, yielding a batched output for further processing or training.[/blue]
2024-12-01 21:05:59
[
    {
        "arg": "self",
        "description": "(Iterator[TensorDictBase]) \u2013 The data collector instance. (default: None)"
    },
    {
        "arg": "T",
        "description": "(type) \u2013 The type of the data to be collected."
    },
    {
        "arg": "data",
        "description": "(list[dict[str, Tensor]]) \u2013 A list of dictionaries containing the data to be collected."
    },
    {
        "arg": "batch_size",
        "description": "(int) \u2013 The batch size. (default: 1)"
    },
    {
        "arg": "shuffle",
        "description": "(bool) \u2013 Whether to shuffle the data. (default: False)"
    },
    {
        "arg": "drop_last",
        "description": "(bool) \u2013 Whether to drop the last incomplete batch. (default: True)"
    },
    {
        "arg": "num_workers",
        "description": "(int) \u2013 The number of worker threads. (default: 0)"
    },
    {
        "arg": "pin_memory",
        "description": "(bool) \u2013 Whether to pin the memory allocation for each tensor. (default: False)"
    }
]
2024-12-01 21:06:17
class MultiaSyncDataCollector description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models.[/blue]
2024-12-01 21:06:48
function __init__ description: 
[blue]This function is a special method in PyTorch classes that initializes the object's attributes when an instance of the class is created. It should be used to set up the initial state of the object, including any necessary computations or data structures, and is typically called automatically when an instance of the class is created.[/blue]
2024-12-01 21:07:16
[
    {
        "arg": "*args",
        "description": "(tuple) \u2013 Variable length argument list. (default: None)"
    },
    {
        "arg": "**kwargs",
        "description": "(dict) \u2013 Arbitrary keyword arguments. (default: None)"
    }
]
2024-12-01 21:07:32
function next description: 
[blue]This function is a placeholder that delegates to the parent class's `next` method, effectively bypassing any custom implementation. It should be used when you want to maintain the default behavior of the parent class while still providing a consistent interface for subclasses.[/blue]
2024-12-01 21:08:02
[
    {
        "arg": "self",
        "description": "(MultiaSyncDataCollector) \u2013 The instance of MultiaSyncDataCollector class. (default: None)"
    },
    {
        "arg": "data",
        "description": "(list or tuple) \u2013 A list or tuple containing the data to be collected. (default: None)"
    }
]
2024-12-01 21:08:18
function shutdown description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-01 21:08:41
[
    {
        "arg": "self",
        "description": "(None) \u2013 The instance of MultiaSyncDataCollector. (default: None)"
    }
]
2024-12-01 21:09:01
function set_seed description: 
[blue]This function is used to set a random seed for reproducibility in PyTorch models. It ensures that the same sequence of random numbers is generated every time the model is run with the same seed value, making it easier to debug and test models.[/blue]
2024-12-01 21:09:30
[
    {
        "arg": "seed",
        "description": " (int) \u2013 The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) \u2013 Whether the static seed is used. (default: False)"
    }
]
2024-12-01 21:09:43
function state_dict description: 
[blue]This function is used to retrieve the model's state dictionary, which contains the learned parameters and their values, allowing for model loading and saving.[/blue]
2024-12-01 21:10:12
[
    {
        "arg": "self",
        "description": "(torchvision.utils.data.DataloaderIterator) \u2013 The data iterator to retrieve the state dictionary from. (default: None)"
    },
    {
        "arg": "OrderedDict",
        "description": "\u2013 The ordered dictionary containing the model's state. (default: None)"
    }
]
2024-12-01 21:10:30
function load_state_dict description: 
[blue]This function is used to load the state dictionary of a PyTorch model from an existing checkpoint or saved model. It allows for the transfer of learned parameters and other model state between different runs or environments, enabling faster development and deployment.[/blue]
2024-12-01 21:11:07
[
    {
        "arg": "self",
        "description": "(None) \u2013 The instance of MultiaSyncDataCollector. (default: None)"
    },
    {
        "arg": "state_dict",
        "description": "(OrderedDict) \u2013 The model state dictionary to load into the current state, populating all and making no changes to it."
    },
    {
        "arg": "None",
        "description": "(None) \u2013 Not used in this function. (default: None)"
    }
]
2024-12-01 21:11:24
function update_policy_weights_ description: 
[blue]This function is a method that updates the weights of a policy in a reinforcement learning model by calling the parent class's `update_policy_weights_` method.[/blue]
2024-12-01 21:11:55
[
    {
        "arg": "policy_weights",
        "description": " (Optional[TensorDictBase]) \u2013 The policy weights to be updated. (default: None)"
    },
    {
        "arg": "self",
        "description": " (<class 'MultiaSyncDataCollector'>) \u2013 The instance of the class."
    }
]
2024-12-01 21:12:10
function frames_per_batch_worker description: 
[blue]This function is a read-only property that returns the requested frames per batch, indicating its significance as a configuration setting for video processing and batching in PyTorch.[/blue]
2024-12-01 21:12:35
[
    {
        "arg": "frames_per_batch_worker",
        "description": "(int) \u2013 The number of frames to process in each batch. (default: 1)"
    }
]
2024-12-01 21:13:36
function iterator description: 
[blue]This function is a PyTorch iterator that manages the data processing pipeline, handling tasks such as updating policy weights, sending messages to workers, and yielding processed output tensors.[/blue]
2024-12-01 21:14:31
[
    {
        "arg": "self",
        "description": "(Iterator[TensorDictBase]) \u2013 The data collector instance. (default: None)"
    },
    {
        "arg": "T",
        "description": "(type) \u2013 The type of the data to be collected. (default: None)"
    },
    {
        "arg": "batch_size",
        "description": "(int) \u2013 The batch size for collecting data. (default: 1)"
    },
    {
        "arg": "shuffle",
        "description": "(bool) \u2013 Whether to shuffle the data before collecting it. (default: False)"
    },
    {
        "arg": "drop_last",
        "description": "(bool) \u2013 Whether to drop the last batch if its size is less than the batch size. (default: True)"
    },
    {
        "arg": "pin_memory",
        "description": "(bool) \u2013 Whether to pin the data in memory. (default: False)"
    }
]
2024-12-01 21:15:00
function reset description: 
[blue]This function is used to reset the state of a worker in a multi-worker queue system, ensuring that workers continue processing tasks after a certain period or when new frames are available.[/blue]
2024-12-01 21:15:33
[
    {
        "arg": "reset_idx",
        "description": " (Optional[Sequence[bool]]) \u2013 Indexes of data to be reset. (default: None)"
    },
    {
        "arg": "self",
        "description": " \u2013 The instance of MultiaSyncDataCollector."
    },
    {
        "arg": "reset",
        "description": " () -> None"
    }
]
2024-12-01 21:15:48
class aSyncDataCollector description: 
[blue]This class is an asynchronous data collector, designed to efficiently collect and store data from various sources in a PyTorch environment, allowing for seamless integration with other components.[/blue]
2024-12-01 21:17:04
function __init__ description: 
[blue]This function is the constructor of a class in PyTorch, used to initialize an object with its attributes and parameters. It sets up the environment for training or testing a reinforcement learning algorithm.[/blue]
2024-12-01 21:48:20
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-01 21:48:48
[
    {
        "arg": "dictionary",
        "description": "(OrderedDict) \u2013 The input dictionary to be mapped to CPU. (default: None)"
    },
    {
        "arg": "return_dict",
        "description": "(bool) \u2013 Whether to return the results in a new OrderedDict or not. (default: False)"
    }
]
2024-12-01 21:49:11
class DataCollectorBase description: 
[blue]This class is a base class for data collectors in PyTorch, providing a common interface for updating model weights and other state during training or testing. It allows for flexible customization of the data collection process through its methods, such as `update_policy_weights_` and `set_seed`, making it a crucial component in building robust and efficient machine learning pipelines.[/blue]
2024-12-01 21:49:36
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in PyTorch, ensuring consistency across devices when using data collectors and trained policies.[/blue]
2024-12-01 21:50:00
[
    {
        "arg": "policy_weights",
        "description": " (Optional[TensorDictBase]) \u2013 The policy weights to be updated. (default: None)"
    }
]
2024-12-01 21:50:17
function next description: 
[blue]This function is used to retrieve the next item from an iterator, handling cases where the iterator has reached its end.[/blue]
2024-12-01 21:50:38
[
    {
        "arg": "self",
        "description": "(DataCollectorBase instance) \u2013 The current data collector instance. (default: None)"
    }
]
2024-12-01 21:50:52
function shutdown description: 
[blue]This function is an abstract method that serves as a hook for customizing the shutdown process of a PyTorch module, allowing developers to implement specific cleanup actions before the module is unloaded.[/blue]
2024-12-01 21:51:14
[
    {
        "arg": "self",
        "description": "(DataCollectorBase) \u2013 The instance of DataCollectorBase class. (default: None)"
    }
]
2024-12-01 21:51:28
function iterator description: 
[blue]This function is an abstract base class method that defines the interface for creating an iterator over a TensorDict, allowing for iteration over its key-value pairs or values.[/blue]
2024-12-01 21:52:11
[
    {
        "arg": "self",
        "description": "(self) \u2013 The instance of DataCollectorBase class. (default: None)"
    },
    {
        "arg": "T",
        "type": "TypeVar",
        "description": "(T) \u2013 Type variable for the data type stored in TensorDictBase."
    },
    {
        "arg": "TensorDictBase",
        "type": "class",
        "description": "(TensorDictBase) \u2013 The base class of the dictionary containing tensors."
    },
    {
        "arg": "Iterator[TensorDictBase]",
        "type": "function return type",
        "description": "(Iterator[TensorDictBase]) \u2013 An iterator over TensorDictBase instances."
    }
]
2024-12-01 21:52:27
function set_seed description: 
[blue]This function is used to set a random seed for reproducibility in PyTorch models, ensuring that the same sequence of random numbers is generated during training and testing, thereby facilitating reliable comparisons and debugging.[/blue]
2024-12-01 21:52:56
[
    {
        "arg": "seed",
        "description": " (int) \u2013 The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) \u2013 Whether the static seed should be used. (default: False)"
    }
]
2024-12-01 21:53:10
function state_dict description: 
[blue]This function is used to serialize and retrieve the model's weights and biases into a dictionary, allowing for easy saving and loading of the model's state.[/blue]
2024-12-01 21:53:38
[
    {
        "arg": "self",
        "description": "(DataCollectorBase instance) \u2013 The DataCollectorBase instance to retrieve the state dictionary from. (default: None)"
    },
    {
        "arg": "OrderedDict",
        "description": " \u2013 The ordered dictionary containing the model's state. (default: None)"
    }
]
2024-12-01 21:53:53
function load_state_dict description: 
[blue]This function is used to load the state dictionary of a PyTorch model from a given state dictionary, allowing for model checkpointing and resuming training from a saved state.[/blue]
2024-12-01 21:54:22
[
    {
        "arg": "self",
        "description": "(None) \u2013 The instance of DataCollectorBase to update with the new state dictionary. (default: None)"
    },
    {
        "arg": "state_dict",
        "description": "(OrderedDict) \u2013 The state dictionary to load into the model. (default: None)"
    }
]
2024-12-01 21:54:38
class SyncDataCollector description: 
[blue]This class is a data collector used to synchronize and collect data from multiple workers in a distributed training setup, ensuring consistency across the model's parameters.[/blue]
2024-12-01 22:01:21
function __init__ description: 
[blue]This function is the constructor for a class that initializes various attributes and settings for an environment, policy, and rollout. It takes in numerous keyword arguments to customize its behavior, including batch size, frames per batch, total frames, exploration type, and more. The class appears to be designed for reinforcement learning tasks, particularly those involving batched environments and policies.[/blue]
2024-12-01 22:05:27
[
    {
        "arg": "create_env_fn",
        "description": "(Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]]) \u2013 Function to create environment. (default: None)"
    },
    {
        "arg": "policy",
        "description": "(Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]) \u2013 Policy function or module. (default: None)"
    },
    {
        "arg": "frames_per_batch",
        "description": "(int) \u2013 Number of frames per batch. (default: -1)"
    },
    {
        "arg": "total_frames",
        "description": "(int) \u2013 Total number of frames. (default: -1)"
    },
    {
        "arg": "device",
        "description": "(DEVICE_TYPING) \u2013 Device to use for computation. (default: None)"
    },
    {
        "arg": "storing_device",
        "description": "(DEVICE_TYPING) \u2013 Device to store data on. (default: None)"
    },
    {
        "arg": "policy_device",
        "description": "(DEVICE_TYPING) \u2013 Device for policy computation. (default: None)"
    },
    {
        "arg": "env_device",
        "description": "(DEVICE_TYPING) \u2013 Device for environment computation. (default: None)"
    },
    {
        "arg": "create_env_kwargs",
        "description": "(dict | None) \u2013 Keyword arguments to pass to create_env_fn. (default: None)"
    },
    {
        "arg": "max_frames_per_traj",
        "description": "(int | None) \u2013 Maximum number of frames per trajectory. (default: None)"
    },
    {
        "arg": "init_random_frames",
        "description": "(int | None) \u2013 Number of random frames to initialize with. (default: None)"
    },
    {
        "arg": "reset_at_each_iter",
        "description": "(bool) \u2013 Whether to reset at each iteration. (default: False)"
    },
    {
        "arg": "postproc",
        "description": "(Callable[[TensorDictBase], TensorDictBase] | None) \u2013 Post-processing function for tensor dict. (default: None)"
    },
    {
        "arg": "split_trajs",
        "description": "(bool | None) \u2013 Whether to split trajectories. (default: None)"
    },
    {
        "arg": "exploration_type",
        "description": "(ExplorationType) \u2013 Type of exploration. (default: DEFAULT_EXPLORATION_TYPE)"
    },
    {
        "arg": "return_same_td",
        "description": "(bool) \u2013 Whether to return same td. (default: False)"
    },
    {
        "arg": "reset_when_done",
        "description": "(bool) \u2013 Whether to reset when done. (default: True)"
    },
    {
        "arg": "interruptor",
        "description": "(None) \u2013 Interruptor object."
    },
    {
        "arg": "set_truncated",
        "description": "(bool) \u2013 Whether to set truncated. (default: False)"
    },
    {
        "arg": "use_buffers",
        "description": "(bool | None) \u2013 Whether to use buffers. (default: None)"
    },
    {
        "arg": "replay_buffer",
        "description": "(ReplayBuffer | None) \u2013 Replay buffer object."
    },
    {
        "arg": "trust_policy",
        "description": "(None) \u2013 Trust policy flag."
    },
    {
        "arg": "compile_policy",
        "description": "(Dict[str, Any] | bool | None) \u2013 Compile policy flag or dictionary. (default: None)"
    },
    {
        "arg": "cudagraph_policy",
        "description": "(Dict[str, Any] | bool | None) \u2013 Cuda graph policy flag or dictionary. (default: None)"
    }
]
2024-12-01 22:05:43
function next description: 
[blue]This function is a placeholder that delegates to the parent class's `next` method, effectively bypassing any custom implementation. It should be used when you want to maintain the default behavior of the parent class while still providing a consistent interface for subclasses.[/blue]
2024-12-01 22:06:06
[
    {
        "arg": "self",
        "description": "(torchvision.utils.data.Dataloader) \u2013 The data loader to get the next batch from. (default: None)"
    }
]
2024-12-01 22:06:22
function update_policy_weights_ description: 
[blue]This function is a method that updates the weights of a policy in a reinforcement learning model by calling the parent class's `update_policy_weights_` method.[/blue]
2024-12-01 22:06:51
[
    {
        "arg": "policy_weights",
        "description": " (Optional[TensorDictBase]) \u2013 The policy weights to be updated. (default: None)"
    },
    {
        "arg": "self",
        "description": " (<class 'SyncDataCollector'>) \u2013 The instance of SyncDataCollector"
    }
]
2024-12-01 22:07:40
function set_seed description: 
[blue]This function is used to set the seeds for environments in a DataCollector, allowing for reproducibility and incrementing of seeds across multiple environments.[/blue]
2024-12-01 22:08:09
[
    {
        "arg": "seed",
        "description": " (int) \u2013 The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) \u2013 Whether the static seed is used. (default: False)"
    }
]
2024-12-01 22:09:39
function iterator description: 
[blue]This function is used to iterate through a DataCollector, yielding TensorDictBase objects containing chunks of trajectories. It manages CUDA streams and events for efficient data collection on GPU devices.[/blue]
2024-12-01 22:10:12
[
    {
        "arg": "self",
        "description": "(Iterator[TensorDictBase]) \u2013 The data collector instance. (default: None)"
    },
    {
        "arg": "T",
        "description": "(type) \u2013 The type of the data to be collected. (default: None)"
    },
    {
        "arg": "D",
        "description": "(type) \u2013 The type of the dictionary in the TensorDictBase. (default: dict)"
    }
]
2024-12-01 22:12:30
function rollout description: 
[blue]This function is used to generate a rollout in the environment using the provided policy. It returns a TensorDictBase containing the computed rollout and can be used for training reinforcement learning agents by sampling multiple rollouts from the same policy and averaging their outputs.[/blue]
2024-12-01 22:13:07
[
    {
        "arg": "num_steps",
        "description": "(int) \u2013 Number of steps to roll out. (default: 1)"
    },
    {
        "arg": "batch_size",
        "description": "(int) \u2013 Batch size for rollout. (default: 1)"
    },
    {
        "arg": "device",
        "description": "(torch.device or str) \u2013 Device to run the rollout on. (default: None)"
    },
    {
        "arg": "deterministic",
        "description": "(bool) \u2013 Whether to use deterministic rollout. (default: False)"
    }
]
2024-12-01 22:13:42
function reset description: 
[blue]This function is used to reset environments in a PyTorch environment, updating the internal state and metadata accordingly.[/blue]
2024-12-01 22:14:11
[
    {
        "arg": "self",
        "description": "(NoneType) \u2013 The instance of SyncDataCollector class. (default: None)"
    },
    {
        "arg": "index",
        "description": "(int or NoneType) \u2013 The index to reset the data collector for. If None, resets all data collectors. (default: None)"
    }
]
2024-12-01 22:14:30
function shutdown description: 
[blue]This function is used to cleanly shut down a PyTorch environment, releasing system resources and ensuring proper cleanup.[/blue]
2024-12-01 22:14:50
[
    {
        "arg": "self",
        "description": "(NoneType) \u2013 The instance of SyncDataCollector class. (default: None)"
    }
]
2024-12-01 22:15:28
function state_dict description: 
[blue]This function is used to retrieve and combine the local state dictionaries of an environment and a policy in PyTorch RL. It returns an ordered dictionary containing both environment and policy state dictionaries, along with additional metadata such as frames and iteration number.[/blue]
2024-12-01 22:15:55
[
    {
        "arg": "self",
        "description": "(None) \u2013 The instance of SyncDataCollector class. (default: None)"
    },
    {
        "arg": "OrderedDict",
        "description": "(<type>) \u2013 A dictionary-like object used to store the model's state. (default: None)"
    }
]
2024-12-01 22:16:25
function load_state_dict description: 
[blue]This function is used to load a pre-trained model's state dictionary into the environment and policy, allowing for the resumption of training or inference from a previously saved checkpoint.[/blue]
2024-12-01 22:16:52
[
    {
        "arg": "state_dict",
        "description": "(OrderedDict) \u2013 The dictionary containing the model's state. (default: None)"
    },
    {
        "arg": "kwargs",
        "description": "(**kwargs) \u2013 Additional keyword arguments to be passed to the underlying PyTorch function. (default: None)"
    }
]
2024-12-01 22:17:07
class MultiSyncDataCollector description: 
[blue]This class is a PyTorch data collector that synchronizes and batches data across multiple workers, ensuring consistent data distribution for training and evaluation.[/blue]
2024-12-01 22:17:21
function next description: 
[blue]This function is a placeholder that delegates to the parent class's `next` method, effectively bypassing any custom implementation. It should be used when you want to maintain the default behavior of the parent class while still providing a consistent interface for subclasses.[/blue]
2024-12-01 22:17:50
[
    {
        "arg": "self",
        "description": "(MultiSyncDataCollector instance) \u2013 The current data collector instance. (default: None)"
    },
    {
        "arg": "data",
        "description": "(list of dict) \u2013 A list of dictionaries containing the data to be collected. Each dictionary should have 'key' and 'value' keys."
    }
]
2024-12-01 22:18:05
function shutdown description: 
[blue]This function is used to release resources held by a PyTorch model during shutdown, ensuring memory safety and preventing potential memory leaks.[/blue]
2024-12-01 22:18:25
[
    {
        "arg": "self",
        "description": "(None) \u2013 The instance of MultiSyncDataCollector. (default: None)"
    }
]
2024-12-01 22:18:43
function set_seed description: 
[blue]This function is used to set a random seed for reproducibility in PyTorch models. It ensures that the same sequence of random numbers is generated every time the model is run with the same seed value, making it easier to debug and test models.[/blue]
2024-12-01 22:19:10
[
    {
        "arg": "seed",
        "description": " (int) \u2013 The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) \u2013 Whether the static seed is used. (default: False)"
    }
]
2024-12-01 22:19:24
function state_dict description: 
[blue]This function is used to retrieve the model's state dictionary, which contains the learned parameters and their values, allowing for model loading and saving.[/blue]
2024-12-01 22:19:55
[
    {
        "arg": "self",
        "description": "(torchvision.utils.data.DataloaderIterator) \u2013 The data iterator to synchronize with. (default: None)"
    },
    {
        "arg": "prefix",
        "description": "(str) \u2013 Prefix for the keys in the state dictionary. (default: 'data')"
    },
    {
        "arg": "local_rank",
        "description": "(int) \u2013 Local rank of this process. (default: 0)"
    }
]
2024-12-01 22:20:12
function load_state_dict description: 
[blue]This function is used to load the state dictionary of a PyTorch model from an existing checkpoint or saved model. It allows for the transfer of learned parameters and other model state between different runs or environments, enabling faster development and deployment.[/blue]
2024-12-01 22:20:43
[
    {
        "arg": "self",
        "description": "(None) \u2013 The instance of MultiSyncDataCollector. (default: None)"
    },
    {
        "arg": "state_dict",
        "description": "(OrderedDict) \u2013 The model state dictionary to be loaded into the current state."
    },
    {
        "arg": "None",
        "description": "(None) \u2013 Not used in this function."
    }
]
2024-12-01 22:20:58
function update_policy_weights_ description: 
[blue]This function is a method that updates the weights of a policy in a reinforcement learning model by calling the parent class's `update_policy_weights_` method.[/blue]
2024-12-01 22:21:26
[
    {
        "arg": "policy_weights",
        "description": " (Optional[TensorDictBase]) \u2013 The policy weights to be updated. (default: None)"
    },
    {
        "arg": "self",
        "description": " (<class 'MultiSyncDataCollector'>) \u2013 The instance of the class."
    }
]
2024-12-01 22:21:52
function frames_per_batch_worker description: 
[blue]This function is used to calculate the effective `frames_per_batch` value for a batch of frames when using multiple collector workers, ensuring that the requested FPS is achieved despite the increased number of frames per iteration.[/blue]
2024-12-01 22:22:15
[
    {
        "arg": "frames_per_batch_worker",
        "description": "(int) \u2013 The number of frames to process in each batch. (default: 1)"
    }
]
2024-12-01 22:25:39
function iterator description: 
[blue]This function is a PyTorch iterator that generates batches of data from a collection of buffers, applying various transformations and filtering criteria along the way. It's used to process and combine data from multiple workers in a distributed environment, yielding a batched output for further processing or training.[/blue]
2024-12-01 22:26:39
[
    {
        "arg": "self",
        "description": "(Iterator[TensorDictBase]) \u2013 The data collector instance. (default: None)"
    },
    {
        "arg": "T",
        "description": "(type) \u2013 The type of the data to be collected."
    },
    {
        "arg": "data",
        "description": "(list[dict[str, Tensor]]) \u2013 A list of dictionaries containing the data to be collected."
    },
    {
        "arg": "batch_size",
        "description": "(int) \u2013 The batch size. (default: 1)"
    },
    {
        "arg": "shuffle",
        "description": "(bool) \u2013 Whether to shuffle the data. (default: False)"
    },
    {
        "arg": "drop_last",
        "description": "(bool) \u2013 Whether to drop the last incomplete batch. (default: True)"
    },
    {
        "arg": "num_workers",
        "description": "(int) \u2013 The number of worker threads. (default: 0)"
    },
    {
        "arg": "pin_memory",
        "description": "(bool) \u2013 Whether to pin the memory allocation for each tensor. (default: False)"
    }
]
2024-12-01 22:26:55
class MultiaSyncDataCollector description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models.[/blue]
2024-12-01 22:27:24
function __init__ description: 
[blue]This function is a special method in PyTorch classes that initializes the object's attributes when an instance of the class is created. It should be used to set up the initial state of the object, including any necessary computations or data structures, and is typically called automatically when an instance of the class is created.[/blue]
2024-12-01 22:27:50
[
    {
        "arg": "*args",
        "description": "(tuple) \u2013 Variable length argument list. (default: None)"
    },
    {
        "arg": "**kwargs",
        "description": "(dict) \u2013 Arbitrary keyword arguments. (default: None)"
    }
]
2024-12-01 22:28:05
function next description: 
[blue]This function is a placeholder that delegates to the parent class's `next` method, effectively bypassing any custom implementation. It should be used when you want to maintain the default behavior of the parent class while still providing a consistent interface for subclasses.[/blue]
2024-12-01 22:28:33
[
    {
        "arg": "self",
        "description": "(MultiaSyncDataCollector) \u2013 The instance of MultiaSyncDataCollector class. (default: None)"
    },
    {
        "arg": "data",
        "description": "(list or tuple) \u2013 A list or tuple containing the data to be collected. (default: None)"
    }
]
2024-12-01 22:28:48
function shutdown description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-01 22:29:09
[
    {
        "arg": "self",
        "description": "(None) \u2013 The instance of MultiaSyncDataCollector. (default: None)"
    }
]
2024-12-01 22:29:27
function set_seed description: 
[blue]This function is used to set a random seed for reproducibility in PyTorch models. It ensures that the same sequence of random numbers is generated every time the model is run with the same seed value, making it easier to debug and test models.[/blue]
2024-12-01 22:29:54
[
    {
        "arg": "seed",
        "description": " (int) \u2013 The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) \u2013 Whether the static seed is used. (default: False)"
    }
]
2024-12-01 22:30:07
function state_dict description: 
[blue]This function is used to retrieve the model's state dictionary, which contains the learned parameters and their values, allowing for model loading and saving.[/blue]
2024-12-01 22:30:34
[
    {
        "arg": "self",
        "description": "(torchvision.utils.data.DataloaderIterator) \u2013 The data iterator to retrieve the state dictionary from. (default: None)"
    },
    {
        "arg": "OrderedDict",
        "description": "\u2013 The ordered dictionary containing the model's state. (default: None)"
    }
]
2024-12-01 22:30:51
function load_state_dict description: 
[blue]This function is used to load the state dictionary of a PyTorch model from an existing checkpoint or saved model. It allows for the transfer of learned parameters and other model state between different runs or environments, enabling faster development and deployment.[/blue]
2024-12-01 22:31:25
[
    {
        "arg": "self",
        "description": "(None) \u2013 The instance of MultiaSyncDataCollector. (default: None)"
    },
    {
        "arg": "state_dict",
        "description": "(OrderedDict) \u2013 The model state dictionary to load into the current state, populating all and making no changes to it."
    },
    {
        "arg": "None",
        "description": "(None) \u2013 Not used in this function. (default: None)"
    }
]
2024-12-01 22:31:40
function update_policy_weights_ description: 
[blue]This function is a method that updates the weights of a policy in a reinforcement learning model by calling the parent class's `update_policy_weights_` method.[/blue]
2024-12-01 22:32:08
[
    {
        "arg": "policy_weights",
        "description": " (Optional[TensorDictBase]) \u2013 The policy weights to be updated. (default: None)"
    },
    {
        "arg": "self",
        "description": " (<class 'MultiaSyncDataCollector'>) \u2013 The instance of the class."
    }
]
2024-12-01 22:32:22
function frames_per_batch_worker description: 
[blue]This function is a read-only property that returns the requested frames per batch, indicating its significance as a configuration setting for video processing and batching in PyTorch.[/blue]
2024-12-01 22:32:44
[
    {
        "arg": "frames_per_batch_worker",
        "description": "(int) \u2013 The number of frames to process in each batch. (default: 1)"
    }
]
2024-12-01 22:33:41
function iterator description: 
[blue]This function is a PyTorch iterator that manages the data processing pipeline, handling tasks such as updating policy weights, sending messages to workers, and yielding processed output tensors.[/blue]
2024-12-01 22:34:32
[
    {
        "arg": "self",
        "description": "(Iterator[TensorDictBase]) \u2013 The data collector instance. (default: None)"
    },
    {
        "arg": "T",
        "description": "(type) \u2013 The type of the data to be collected. (default: None)"
    },
    {
        "arg": "batch_size",
        "description": "(int) \u2013 The batch size for collecting data. (default: 1)"
    },
    {
        "arg": "shuffle",
        "description": "(bool) \u2013 Whether to shuffle the data before collecting it. (default: False)"
    },
    {
        "arg": "drop_last",
        "description": "(bool) \u2013 Whether to drop the last batch if its size is less than the batch size. (default: True)"
    },
    {
        "arg": "pin_memory",
        "description": "(bool) \u2013 Whether to pin the data in memory. (default: False)"
    }
]
2024-12-01 22:34:58
function reset description: 
[blue]This function is used to reset the state of a worker in a multi-worker queue system, ensuring that workers continue processing tasks after a certain period or when new frames are available.[/blue]
2024-12-01 22:35:28
[
    {
        "arg": "reset_idx",
        "description": " (Optional[Sequence[bool]]) \u2013 Indexes of data to be reset. (default: None)"
    },
    {
        "arg": "self",
        "description": " \u2013 The instance of MultiaSyncDataCollector."
    },
    {
        "arg": "reset",
        "description": " () -> None"
    }
]
2024-12-01 22:35:43
class aSyncDataCollector description: 
[blue]This class is an asynchronous data collector, designed to efficiently collect and store data from various sources in a PyTorch environment, allowing for seamless integration with other components.[/blue]
2024-12-01 22:36:49
function __init__ description: 
[blue]This function is the constructor of a class in PyTorch, used to initialize an object with its attributes and parameters. It sets up the environment for training or testing a reinforcement learning algorithm.[/blue]
2024-12-02 08:04:31
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 08:10:05
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 08:13:13
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 08:14:06
[
    {
        "arg": "dictionary",
        "description": "(OrderedDict) \u2013 The input dictionary to be mapped to CPU."
    },
    {
        "arg": "None",
        "description": " (None) \u2013 No description available."
    }
]
2024-12-02 08:14:28
class DataCollectorBase description: 
[blue]This class is a base class for data collectors in PyTorch, providing a common interface for updating model weights and other state during training or testing. It allows for flexible customization of the data collection process through its methods, such as `update_policy_weights_` and `set_seed`, making it a crucial component in building robust and efficient machine learning pipelines.[/blue]
2024-12-02 08:14:54
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in PyTorch, ensuring consistency across devices when using data collectors and trained policies.[/blue]
2024-12-02 08:15:45
[
    {
        "arg": "policy_weights",
        "description": "(Optional[TensorDictBase]) \u2013 The policy weights to be updated."
    }
]
2024-12-02 08:16:01
function next description: 
[blue]This function is used to retrieve the next item from an iterator, handling cases where the iterator has reached its end.[/blue]
2024-12-02 08:17:07
[
    {
        "arg": "self",
        "description": "(object) \u2013 The model instance."
    },
    {
        "arg": "state_dict",
        "description": "(OrderedDict) \u2013 The model state dictionary to be loaded into the current state."
    },
    {
        "arg": "seed",
        "description": " (int) \u2013 The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) \u2013 Whether the static seed is used. (default: False)"
    }
]
2024-12-02 08:17:22
function shutdown description: 
[blue]This function is an abstract method that serves as a hook for customizing the shutdown process of a PyTorch module, allowing developers to implement specific cleanup actions before the module is unloaded.[/blue]
2024-12-02 08:18:23
[
    {
        "arg": "state_dict",
        "description": "(OrderedDict) \u2013 The model state dictionary to be loaded into the current state."
    },
    {
        "arg": "seed",
        "description": " (int) \u2013 The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) \u2013 Whether the static seed is used. (default: False)"
    }
]
2024-12-02 08:18:37
function iterator description: 
[blue]This function is an abstract base class method that defines the interface for creating an iterator over a TensorDict, allowing for iteration over its key-value pairs or values.[/blue]
2024-12-02 08:19:56
[
    {
        "arg": "self",
        "description": "(object) \u2013 The model instance."
    },
    {
        "arg": "args",
        "description": "(tuple or list of any type) \u2013 Additional positional arguments passed to the module."
    },
    {
        "arg": "kwargs",
        "description": "(dict) \u2013 Additional keyword arguments passed to the module."
    },
    {
        "arg": "state_dict",
        "description": "(OrderedDict) \u2013 The model state dictionary to be loaded into the current state."
    },
    {
        "arg": "seed",
        "description": " (int) \u2013 The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) \u2013 Whether the static seed is used. (default: False)"
    }
]
2024-12-02 08:29:08
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 08:37:36
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 09:14:58
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 09:18:53
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 09:20:12
[
    {
        "arg": "dictionary",
        "description": "(OrderedDict) - The dictionary to be mapped to CPU."
    },
    {
        "arg": "state_dict",
        "description": "(OrderedDict) - The model state dictionary to be loaded into the current state."
    },
    {
        "arg": "seed",
        "description": " (int) - The random seed to be used."
    },
    {
        "arg": "static_seed",
        "description": " (bool) - Whether the static seed is used."
    }
]
2024-12-02 09:20:37
class DataCollectorBase description: 
[blue]This class is a base class for data collectors in PyTorch, providing a common interface for updating model weights and other state during training or testing. It allows for flexible customization of the data collection process through its methods, such as `update_policy_weights_` and `set_seed`, making it a crucial component in building robust and efficient machine learning pipelines.[/blue]
2024-12-02 09:21:05
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in PyTorch, ensuring consistency across devices when using data collectors and trained policies.[/blue]
2024-12-02 09:22:10
[
    {
        "arg": "policy_weights",
        "description": "(Optional[TensorDictBase]) - The policy weights to be updated. (default: None)"
    }
]
2024-12-02 09:22:27
function next description: 
[blue]This function is used to retrieve the next item from an iterator, handling cases where the iterator has reached its end.[/blue]
2024-12-02 09:23:37
[
    {
        "arg": "self",
        "description": "(object) - The instance of the class."
    },
    {
        "arg": "args",
        "description": " (*) - Variable length argument list."
    },
    {
        "arg": "kwargs",
        "description": " (**kwargs) - Arbitrary keyword arguments."
    }
]
2024-12-02 09:23:53
function shutdown description: 
[blue]This function is an abstract method that serves as a hook for customizing the shutdown process of a PyTorch module, allowing developers to implement specific cleanup actions before the module is unloaded.[/blue]
2024-12-02 09:25:09
[
    {
        "arg": "state_dict",
        "description": "(OrderedDict) - The model state dictionary to be loaded into the current state."
    },
    {
        "arg": "seed",
        "description": " (int) - The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) - Whether the static seed is used. (default: False)"
    }
]
2024-12-02 09:25:25
function iterator description: 
[blue]This function is an abstract base class method that defines the interface for creating an iterator over a TensorDict, allowing for iteration over its key-value pairs or values.[/blue]
2024-12-02 09:26:36
[
    {
        "arg": "self",
        "description": "(object) - The instance of the class."
    },
    {
        "arg": "*args",
        "description": "(tuple) - Variable length argument list."
    },
    {
        "arg": "**kwargs",
        "description": "(dict) - Arbitrary keyword arguments."
    }
]
2024-12-02 09:26:55
function set_seed description: 
[blue]This function is used to set a random seed for reproducibility in PyTorch models, ensuring that the same sequence of random numbers is generated during training and testing, thereby facilitating reliable comparisons and debugging.[/blue]
2024-12-02 09:28:50
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 09:29:54
[
    {
        "arg": "dictionary",
        "description": "(OrderedDict) - The input dictionary to be mapped to CPU."
    },
    {
        "arg": "self",
        "description": " (None) - This parameter is ignored as it's the only argument in the list."
    }
]
2024-12-02 09:30:19
class DataCollectorBase description: 
[blue]This class is a base class for data collectors in PyTorch, providing a common interface for updating model weights and other state during training or testing. It allows for flexible customization of the data collection process through its methods, such as `update_policy_weights_` and `set_seed`, making it a crucial component in building robust and efficient machine learning pipelines.[/blue]
2024-12-02 09:30:47
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in PyTorch, ensuring consistency across devices when using data collectors and trained policies.[/blue]
2024-12-02 09:31:59
[
    {
        "arg": "policy_weights",
        "description": "(Optional[TensorDictBase]) - The policy weights to be updated. (default: None)"
    }
]
2024-12-02 09:32:17
function next description: 
[blue]This function is used to retrieve the next item from an iterator, handling cases where the iterator has reached its end.[/blue]
2024-12-02 09:33:41
[
    {
        "arg": "self",
        "description": "(object) - The instance of the class."
    },
    {
        "arg": "*args",
        "description": "[...]"
    },
    {
        "arg": "**kwargs",
        "description": "[...]"
    },
    {
        "arg": "state_dict",
        "description": "(OrderedDict) - The model state dictionary to be loaded into the current state."
    },
    {
        "arg": "seed",
        "description": " (int) - The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) - Whether the static seed is used. (default: False)"
    }
]
2024-12-02 09:33:57
function shutdown description: 
[blue]This function is an abstract method that serves as a hook for customizing the shutdown process of a PyTorch module, allowing developers to implement specific cleanup actions before the module is unloaded.[/blue]
2024-12-02 09:35:20
[
    {
        "arg": "self",
        "description": "(object) - The instance of the class."
    },
    {
        "arg": "*args",
        "description": "[...]"
    },
    {
        "arg": "**kwargs",
        "description": "[...]"
    },
    {
        "arg": "state_dict",
        "description": "(OrderedDict) - The model state dictionary to be loaded into the current state."
    },
    {
        "arg": "seed",
        "description": " (int) - The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) - Whether the static seed is used. (default: False)"
    }
]
2024-12-02 09:35:37
function iterator description: 
[blue]This function is an abstract base class method that defines the interface for creating an iterator over a TensorDict, allowing for iteration over its key-value pairs or values.[/blue]
2024-12-02 09:59:26
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 10:00:17
[
    {
        "arg": "dictionary",
        "description": "(OrderedDict) - The dictionary to be mapped to CPU."
    },
    {
        "arg": "None",
        "description": " () - No description provided."
    }
]
2024-12-02 10:00:39
class DataCollectorBase description: 
[blue]This class is a base class for data collectors in PyTorch, providing a common interface for updating model weights and other state during training or testing. It allows for flexible customization of the data collection process through its methods, such as `update_policy_weights_` and `set_seed`, making it a crucial component in building robust and efficient machine learning pipelines.[/blue]
2024-12-02 10:01:04
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in PyTorch, ensuring consistency across devices when using data collectors and trained policies.[/blue]
2024-12-02 10:02:00
[
    {
        "arg": "policy_weights",
        "description": "(Optional[TensorDictBase]) - The policy weights to be updated. (default: None)"
    },
    {
        "arg": "self",
        "description": "(object) - The instance of the class that owns this method."
    }
]
2024-12-02 10:02:16
function next description: 
[blue]This function is used to retrieve the next item from an iterator, handling cases where the iterator has reached its end.[/blue]
2024-12-02 10:03:19
[
    {
        "arg": "self",
        "description": "(object) - The instance of the class."
    },
    {
        "arg": "state_dict",
        "description": "(OrderedDict) - The model state dictionary to be loaded into the current state."
    },
    {
        "arg": "seed",
        "description": " (int) - The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) - Whether the static seed is used. (default: False)"
    }
]
2024-12-02 10:03:33
function shutdown description: 
[blue]This function is an abstract method that serves as a hook for customizing the shutdown process of a PyTorch module, allowing developers to implement specific cleanup actions before the module is unloaded.[/blue]
2024-12-02 10:04:35
[
    {
        "arg": "self",
        "description": "(object) - The instance of the class."
    },
    {
        "arg": "state_dict",
        "description": "(OrderedDict) - The model state dictionary to be loaded into the current state."
    },
    {
        "arg": "seed",
        "description": " (int) - The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) - Whether the static seed is used. (default: False)"
    }
]
2024-12-02 10:04:49
function iterator description: 
[blue]This function is an abstract base class method that defines the interface for creating an iterator over a TensorDict, allowing for iteration over its key-value pairs or values.[/blue]
2024-12-02 10:05:49
[
    {
        "arg": "self",
        "description": "(object) - The instance of the class."
    },
    {
        "arg": "state_dict",
        "description": "(OrderedDict) - The model state dictionary to be loaded into the current state."
    },
    {
        "arg": "seed",
        "description": " (int) - The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) - Whether the static seed is used. (default: False)"
    }
]
2024-12-02 10:06:06
function set_seed description: 
[blue]This function is used to set a random seed for reproducibility in PyTorch models, ensuring that the same sequence of random numbers is generated during training and testing, thereby facilitating reliable comparisons and debugging.[/blue]
2024-12-02 10:06:58
[
    {
        "arg": "seed",
        "description": " (int) - The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) - Whether the static seed is used. (default: False)"
    }
]
2024-12-02 10:07:10
function state_dict description: 
[blue]This function is used to serialize and retrieve the model's weights and biases into a dictionary, allowing for easy saving and loading of the model's state.[/blue]
2024-12-02 10:07:58
[
    {
        "arg": "state_dict",
        "description": "(OrderedDict) - The model state dictionary to be loaded into the current state."
    }
]
2024-12-02 10:08:14
function load_state_dict description: 
[blue]This function is used to load the state dictionary of a PyTorch model from a given state dictionary, allowing for model checkpointing and resuming training from a saved state.[/blue]
2024-12-02 10:09:03
[
    {
        "arg": "state_dict",
        "description": "(OrderedDict) - The model state dictionary to be loaded into the current state."
    }
]
2024-12-02 10:09:19
class SyncDataCollector description: 
[blue]This class is a data collector used to synchronize and collect data from multiple workers in a distributed training setup, ensuring consistency across the model's parameters.[/blue]
2024-12-02 10:21:28
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 10:22:21
[
    {
        "arg": "dictionary",
        "description": "(OrderedDict) - The ordered dictionary to be mapped to CPU."
    },
    {
        "arg": "None",
        "description": " () - No description available."
    }
]
2024-12-02 10:22:43
class DataCollectorBase description: 
[blue]This class is a base class for data collectors in PyTorch, providing a common interface for updating model weights and other state during training or testing. It allows for flexible customization of the data collection process through its methods, such as `update_policy_weights_` and `set_seed`, making it a crucial component in building robust and efficient machine learning pipelines.[/blue]
2024-12-02 10:23:10
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in PyTorch, ensuring consistency across devices when using data collectors and trained policies.[/blue]
2024-12-02 10:24:04
[
    {
        "arg": "policy_weights",
        "description": "(Optional[TensorDictBase]) - The policy weights to be updated. (default: None)"
    },
    {
        "arg": "_",
        "description": " (...) -> None"
    }
]
2024-12-02 10:24:20
function next description: 
[blue]This function is used to retrieve the next item from an iterator, handling cases where the iterator has reached its end.[/blue]
2024-12-02 10:25:26
[
    {
        "arg": "self",
        "description": "(object) - The instance of the class."
    },
    {
        "arg": "state_dict",
        "description": "(OrderedDict) - The model state dictionary to be loaded into the current state."
    },
    {
        "arg": "seed",
        "description": " (int) - The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) - Whether the static seed is used. (default: False)"
    }
]
2024-12-02 10:25:40
function shutdown description: 
[blue]This function is an abstract method that serves as a hook for customizing the shutdown process of a PyTorch module, allowing developers to implement specific cleanup actions before the module is unloaded.[/blue]
2024-12-02 10:26:28
[
    {
        "arg": "shutdown",
        "description": "(None) - Shutdown the PyTorch engine and exit the process."
    }
]
2024-12-02 10:26:43
function iterator description: 
[blue]This function is an abstract base class method that defines the interface for creating an iterator over a TensorDict, allowing for iteration over its key-value pairs or values.[/blue]
2024-12-02 10:29:20
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 10:30:13
[
    {
        "arg": "dictionary",
        "description": "(OrderedDict) - The ordered dictionary to be mapped to CPU."
    },
    {
        "arg": "None",
        "description": " () - No description available."
    }
]
2024-12-02 10:30:36
class DataCollectorBase description: 
[blue]This class is a base class for data collectors in PyTorch, providing a common interface for updating model weights and other state during training or testing. It allows for flexible customization of the data collection process through its methods, such as `update_policy_weights_` and `set_seed`, making it a crucial component in building robust and efficient machine learning pipelines.[/blue]
2024-12-02 10:31:01
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in PyTorch, ensuring consistency across devices when using data collectors and trained policies.[/blue]
2024-12-02 10:40:45
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 10:41:38
[
    {
        "arg": "dictionary",
        "description": "(OrderedDict) - The ordered dictionary to be mapped to CPU."
    },
    {
        "arg": "None",
        "description": " () - No description available."
    }
]
2024-12-02 10:42:00
class DataCollectorBase description: 
[blue]This class is a base class for data collectors in PyTorch, providing a common interface for updating model weights and other state during training or testing. It allows for flexible customization of the data collection process through its methods, such as `update_policy_weights_` and `set_seed`, making it a crucial component in building robust and efficient machine learning pipelines.[/blue]
2024-12-02 10:42:26
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in PyTorch, ensuring consistency across devices when using data collectors and trained policies.[/blue]
2024-12-02 16:01:15
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 16:02:09
[
    {
        "arg": "dictionary",
        "description": "(OrderedDict) - The ordered dictionary to be mapped to CPU."
    },
    {
        "arg": "None",
        "description": " () - No description available."
    }
]
2024-12-02 16:02:32
class DataCollectorBase description: 
[blue]This class is a base class for data collectors in PyTorch, providing a common interface for updating model weights and other state during training or testing. It allows for flexible customization of the data collection process through its methods, such as `update_policy_weights_` and `set_seed`, making it a crucial component in building robust and efficient machine learning pipelines.[/blue]
2024-12-02 16:02:57
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in PyTorch, ensuring consistency across devices when using data collectors and trained policies.[/blue]
2024-12-02 16:05:44
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 16:06:26
[
    {
        "arg": "dictionary",
        "description": "(OrderedDict) - The ordered dictionary to be mapped to CPU."
    }
]
2024-12-02 16:06:48
class DataCollectorBase description: 
[blue]This class is a base class for data collectors in PyTorch, providing a common interface for updating model weights and other state during training or testing. It allows for flexible customization of the data collection process through its methods, such as `update_policy_weights_` and `set_seed`, making it a crucial component in building robust and efficient machine learning pipelines.[/blue]
2024-12-02 16:07:13
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in PyTorch, ensuring consistency across devices when using data collectors and trained policies.[/blue]
2024-12-02 16:12:31
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 16:13:14
[
    {
        "arg": "dictionary",
        "description": "(OrderedDict) - The ordered dictionary to be mapped to CPU."
    }
]
2024-12-02 16:13:36
class DataCollectorBase description: 
[blue]This class is a base class for data collectors in PyTorch, providing a common interface for updating model weights and other state during training or testing. It allows for flexible customization of the data collection process through its methods, such as `update_policy_weights_` and `set_seed`, making it a crucial component in building robust and efficient machine learning pipelines.[/blue]
2024-12-02 16:14:01
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in PyTorch, ensuring consistency across devices when using data collectors and trained policies.[/blue]
2024-12-02 16:14:01
[
    {
        "arg": "None"
    }
]
2024-12-02 16:14:17
function next description: 
[blue]This function is used to retrieve the next item from an iterator, handling cases where the iterator has reached its end.[/blue]
2024-12-02 16:14:17
[
    {
        "arg": "None"
    }
]
2024-12-02 16:14:30
function shutdown description: 
[blue]This function is an abstract method that serves as a hook for customizing the shutdown process of a PyTorch module, allowing developers to implement specific cleanup actions before the module is unloaded.[/blue]
2024-12-02 16:14:30
[
    {
        "arg": "None"
    }
]
2024-12-02 16:14:44
function iterator description: 
[blue]This function is an abstract base class method that defines the interface for creating an iterator over a TensorDict, allowing for iteration over its key-value pairs or values.[/blue]
2024-12-02 16:14:44
[
    {
        "arg": "None"
    }
]
2024-12-02 16:15:00
function set_seed description: 
[blue]This function is used to set a random seed for reproducibility in PyTorch models, ensuring that the same sequence of random numbers is generated during training and testing, thereby facilitating reliable comparisons and debugging.[/blue]
2024-12-02 16:15:00
[
    {
        "arg": "None"
    }
]
2024-12-02 16:15:12
function state_dict description: 
[blue]This function is used to serialize and retrieve the model's weights and biases into a dictionary, allowing for easy saving and loading of the model's state.[/blue]
2024-12-02 16:15:12
[
    {
        "arg": "None"
    }
]
2024-12-02 16:15:27
function load_state_dict description: 
[blue]This function is used to load the state dictionary of a PyTorch model from a given state dictionary, allowing for model checkpointing and resuming training from a saved state.[/blue]
2024-12-02 16:15:27
[
    {
        "arg": "None"
    }
]
2024-12-02 16:15:42
class SyncDataCollector description: 
[blue]This class is a data collector used to synchronize and collect data from multiple workers in a distributed training setup, ensuring consistency across the model's parameters.[/blue]
2024-12-02 16:32:36
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 16:33:18
[
    {
        "arg": "dictionary",
        "description": "(OrderedDict) - The ordered dictionary to be mapped to CPU."
    }
]
2024-12-02 16:33:40
class DataCollectorBase description: 
[blue]This class is a base class for data collectors in PyTorch, providing a common interface for updating model weights and other state during training or testing. It allows for flexible customization of the data collection process through its methods, such as `update_policy_weights_` and `set_seed`, making it a crucial component in building robust and efficient machine learning pipelines.[/blue]
2024-12-02 16:34:05
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in PyTorch, ensuring consistency across devices when using data collectors and trained policies.[/blue]
2024-12-02 16:34:55
[
    {
        "arg": "policy_weights",
        "description": "(Optional[TensorDictBase]) - The policy weights to be updated. (default: None)"
    },
    {
        "arg": "_",
        "description": " (...) -> None"
    }
]
2024-12-02 16:35:11
function next description: 
[blue]This function is used to retrieve the next item from an iterator, handling cases where the iterator has reached its end.[/blue]
2024-12-02 16:35:11
[
    {
        "arg": "None"
    }
]
2024-12-02 16:35:24
function shutdown description: 
[blue]This function is an abstract method that serves as a hook for customizing the shutdown process of a PyTorch module, allowing developers to implement specific cleanup actions before the module is unloaded.[/blue]
2024-12-02 16:35:24
[
    {
        "arg": "None"
    }
]
2024-12-02 16:35:38
function iterator description: 
[blue]This function is an abstract base class method that defines the interface for creating an iterator over a TensorDict, allowing for iteration over its key-value pairs or values.[/blue]
2024-12-02 16:35:38
[
    {
        "arg": "None"
    }
]
2024-12-02 16:35:54
function set_seed description: 
[blue]This function is used to set a random seed for reproducibility in PyTorch models, ensuring that the same sequence of random numbers is generated during training and testing, thereby facilitating reliable comparisons and debugging.[/blue]
2024-12-02 16:36:46
[
    {
        "arg": "seed",
        "description": " (int) - The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) - Whether the static seed is used. (default: False)"
    }
]
2024-12-02 16:36:59
function state_dict description: 
[blue]This function is used to serialize and retrieve the model's weights and biases into a dictionary, allowing for easy saving and loading of the model's state.[/blue]
2024-12-02 16:36:59
[
    {
        "arg": "None"
    }
]
2024-12-02 16:37:14
function load_state_dict description: 
[blue]This function is used to load the state dictionary of a PyTorch model from a given state dictionary, allowing for model checkpointing and resuming training from a saved state.[/blue]
2024-12-02 16:37:57
[
    {
        "arg": "state_dict",
        "description": "(OrderedDict) - The model state dictionary to be loaded into the current state."
    }
]
2024-12-02 16:38:13
class SyncDataCollector description: 
[blue]This class is a data collector used to synchronize and collect data from multiple workers in a distributed training setup, ensuring consistency across the model's parameters.[/blue]
2024-12-02 16:45:11
function __init__ description: 
[blue]This function is the constructor for a class that initializes various attributes and settings for an environment, policy, and rollout. It takes in numerous keyword arguments to customize its behavior, including batch size, frames per batch, total frames, exploration type, and more. The class appears to be designed for reinforcement learning tasks, particularly those involving batched environments and policies.[/blue]
2024-12-02 16:49:35
[
    {
        "arg": "create_env_fn",
        "description": "(Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]]) - The function to create an environment. (default: None)"
    },
    {
        "arg": "policy",
        "description": "(Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]) - The policy to be used. (default: None)"
    },
    {
        "arg": "frames_per_batch",
        "description": "(int) - The number of frames per batch. (default: -1)"
    },
    {
        "arg": "total_frames",
        "description": "(int) - The total number of frames. (default: -1)"
    },
    {
        "arg": "device",
        "description": "(DEVICE_TYPING) - The device to use for the environment. (default: None)"
    },
    {
        "arg": "storing_device",
        "description": "(DEVICE_TYPING) - The device to store the environment. (default: None)"
    },
    {
        "arg": "policy_device",
        "description": "(DEVICE_TYPING) - The device to use for the policy. (default: None)"
    },
    {
        "arg": "env_device",
        "description": "(DEVICE_TYPING) - The device to use for the environment. (default: None)"
    },
    {
        "arg": "create_env_kwargs",
        "description": "(dict | None) - The keyword arguments to pass to create_env_fn. (default: None)"
    },
    {
        "arg": "max_frames_per_traj",
        "description": "(int | None) - The maximum number of frames per trajectory. (default: None)"
    },
    {
        "arg": "init_random_frames",
        "description": "(int | None) - The initial random frames. (default: None)"
    },
    {
        "arg": "reset_at_each_iter",
        "description": "(bool) - Whether to reset at each iteration. (default: False)"
    },
    {
        "arg": "postproc",
        "description": "(Callable[[TensorDictBase], TensorDictBase] | None) - The post-processing function for the environment. (default: None)"
    },
    {
        "arg": "split_trajs",
        "description": "(bool | None) - Whether to split trajectories. (default: None)"
    },
    {
        "arg": "exploration_type",
        "description": "(ExplorationType=DEFAULT_EXPLORATION_TYPE) - The type of exploration to use."
    },
    {
        "arg": "return_same_td",
        "description": "(bool) - Whether to return the same TD. (default: False)"
    },
    {
        "arg": "reset_when_done",
        "description": "(bool) - Whether to reset when done. (default: True)"
    },
    {
        "arg": "interruptor",
        "description": "(None) - The interruptor."
    },
    {
        "arg": "set_truncated",
        "description": "(bool) - Whether to set truncated. (default: False)"
    },
    {
        "arg": "use_buffers",
        "description": "(bool | None) - Whether to use buffers. (default: None)"
    },
    {
        "arg": "replay_buffer",
        "description": "(ReplayBuffer | None) - The replay buffer."
    },
    {
        "arg": "trust_policy",
        "description": "(None) - The trust policy."
    },
    {
        "arg": "compile_policy",
        "description": "(bool | Dict[str, Any] | None) - Whether to compile the policy. (default: None)"
    },
    {
        "arg": "cudagraph_policy",
        "description": "(bool | Dict[str, Any] | None) - The cudagraph policy."
    }
]
2024-12-02 16:49:51
function next description: 
[blue]This function is a placeholder that delegates to the parent class's `next` method, effectively bypassing any custom implementation. It should be used when you want to maintain the default behavior of the parent class while still providing a consistent interface for subclasses.[/blue]
2024-12-02 16:49:51
[
    {
        "arg": "None"
    }
]
2024-12-02 16:50:05
function update_policy_weights_ description: 
[blue]This function is a method that updates the weights of a policy in a reinforcement learning model by calling the parent class's `update_policy_weights_` method.[/blue]
2024-12-02 16:50:55
[
    {
        "arg": "policy_weights",
        "description": "(Optional[TensorDictBase]) - The policy weights to be updated. (default: None)"
    },
    {
        "arg": "_",
        "description": " (...) -> None"
    }
]
2024-12-02 16:51:46
function set_seed description: 
[blue]This function is used to set the seeds for environments in a DataCollector, allowing for reproducibility and incrementing of seeds across multiple environments.[/blue]
2024-12-02 16:52:38
[
    {
        "arg": "seed",
        "description": " (int) - The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) - Whether the static seed is used. (default: False)"
    }
]
2024-12-02 16:54:17
function iterator description: 
[blue]This function is used to iterate through a DataCollector, yielding TensorDictBase objects containing chunks of trajectories. It manages CUDA streams and events for efficient data collection on GPU devices.[/blue]
2024-12-02 16:54:17
[
    {
        "arg": "None"
    }
]
2024-12-02 16:56:51
function rollout description: 
[blue]This function is used to generate a rollout in the environment using the provided policy. It returns a TensorDictBase containing the computed rollout and can be used for training reinforcement learning agents by sampling multiple rollouts from the same policy and averaging their outputs.[/blue]
2024-12-02 16:56:51
[
    {
        "arg": "None"
    }
]
2024-12-02 16:57:28
function reset description: 
[blue]This function is used to reset environments in a PyTorch environment, updating the internal state and metadata accordingly.[/blue]
2024-12-02 16:58:16
[
    {
        "arg": "index",
        "description": "(int or tuple of ints) - The index at which to reset the model state."
    },
    {
        "arg": "kwargs",
        "description": " (*) - Additional keyword arguments. (default: None)"
    }
]
2024-12-02 16:58:36
function shutdown description: 
[blue]This function is used to cleanly shut down a PyTorch environment, releasing system resources and ensuring proper cleanup.[/blue]
2024-12-02 16:58:36
[
    {
        "arg": "None"
    }
]
2024-12-02 16:59:16
function state_dict description: 
[blue]This function is used to retrieve and combine the local state dictionaries of an environment and a policy in PyTorch RL. It returns an ordered dictionary containing both environment and policy state dictionaries, along with additional metadata such as frames and iteration number.[/blue]
2024-12-02 16:59:16
[
    {
        "arg": "None"
    }
]
2024-12-02 16:59:47
function load_state_dict description: 
[blue]This function is used to load a pre-trained model's state dictionary into the environment and policy, allowing for the resumption of training or inference from a previously saved checkpoint.[/blue]
2024-12-02 17:00:36
[
    {
        "arg": "state_dict",
        "description": "(OrderedDict) - The model state dictionary to be loaded into the current state."
    },
    {
        "arg": "kwargs",
        "description": " (Any) - Additional keyword arguments. (default: None)"
    }
]
2024-12-02 17:00:52
class MultiSyncDataCollector description: 
[blue]This class is a PyTorch data collector that synchronizes and batches data across multiple workers, ensuring consistent data distribution for training and evaluation.[/blue]
2024-12-02 17:01:08
function next description: 
[blue]This function is a placeholder that delegates to the parent class's `next` method, effectively bypassing any custom implementation. It should be used when you want to maintain the default behavior of the parent class while still providing a consistent interface for subclasses.[/blue]
2024-12-02 17:01:08
[
    {
        "arg": "None"
    }
]
2024-12-02 17:01:23
function shutdown description: 
[blue]This function is used to release resources held by a PyTorch model during shutdown, ensuring memory safety and preventing potential memory leaks.[/blue]
2024-12-02 17:01:23
[
    {
        "arg": "None"
    }
]
2024-12-02 17:01:41
function set_seed description: 
[blue]This function is used to set a random seed for reproducibility in PyTorch models. It ensures that the same sequence of random numbers is generated every time the model is run with the same seed value, making it easier to debug and test models.[/blue]
2024-12-02 17:02:31
[
    {
        "arg": "seed",
        "description": " (int) - The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) - Whether the static seed is used. (default: False)"
    }
]
2024-12-02 17:02:45
function state_dict description: 
[blue]This function is used to retrieve the model's state dictionary, which contains the learned parameters and their values, allowing for model loading and saving.[/blue]
2024-12-02 17:02:45
[
    {
        "arg": "None"
    }
]
2024-12-02 17:03:02
function load_state_dict description: 
[blue]This function is used to load the state dictionary of a PyTorch model from an existing checkpoint or saved model. It allows for the transfer of learned parameters and other model state between different runs or environments, enabling faster development and deployment.[/blue]
2024-12-02 17:03:45
[
    {
        "arg": "state_dict",
        "description": "(OrderedDict) - The model state dictionary to be loaded into the current state."
    }
]
2024-12-02 17:04:02
function update_policy_weights_ description: 
[blue]This function is a method that updates the weights of a policy in a reinforcement learning model by calling the parent class's `update_policy_weights_` method.[/blue]
2024-12-02 17:04:49
[
    {
        "arg": "policy_weights",
        "description": "(Optional[TensorDictBase]) - The policy weights to be updated. (default: None)"
    },
    {
        "arg": "_",
        "description": " (...) -> None"
    }
]
2024-12-02 17:05:16
function frames_per_batch_worker description: 
[blue]This function is used to calculate the effective `frames_per_batch` value for a batch of frames when using multiple collector workers, ensuring that the requested FPS is achieved despite the increased number of frames per iteration.[/blue]
2024-12-02 17:05:16
[
    {
        "arg": "None"
    }
]
2024-12-02 17:08:57
function iterator description: 
[blue]This function is a PyTorch iterator that generates batches of data from a collection of buffers, applying various transformations and filtering criteria along the way. It's used to process and combine data from multiple workers in a distributed environment, yielding a batched output for further processing or training.[/blue]
2024-12-02 17:08:57
[
    {
        "arg": "None"
    }
]
2024-12-02 17:09:15
class MultiaSyncDataCollector description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models.[/blue]
2024-12-02 17:09:45
function __init__ description: 
[blue]This function is a special method in PyTorch classes that initializes the object's attributes when an instance of the class is created. It should be used to set up the initial state of the object, including any necessary computations or data structures, and is typically called automatically when an instance of the class is created.[/blue]
2024-12-02 17:09:45
[
    {
        "arg": "None"
    }
]
2024-12-02 17:10:01
function next description: 
[blue]This function is a placeholder that delegates to the parent class's `next` method, effectively bypassing any custom implementation. It should be used when you want to maintain the default behavior of the parent class while still providing a consistent interface for subclasses.[/blue]
2024-12-02 17:10:01
[
    {
        "arg": "None"
    }
]
2024-12-02 17:10:16
function shutdown description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-02 17:10:16
[
    {
        "arg": "None"
    }
]
2024-12-02 17:10:34
function set_seed description: 
[blue]This function is used to set a random seed for reproducibility in PyTorch models. It ensures that the same sequence of random numbers is generated every time the model is run with the same seed value, making it easier to debug and test models.[/blue]
2024-12-02 17:11:24
[
    {
        "arg": "seed",
        "description": " (int) - The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) - Whether the static seed is used. (default: False)"
    }
]
2024-12-02 17:11:38
function state_dict description: 
[blue]This function is used to retrieve the model's state dictionary, which contains the learned parameters and their values, allowing for model loading and saving.[/blue]
2024-12-02 17:11:38
[
    {
        "arg": "None"
    }
]
2024-12-02 17:11:55
function load_state_dict description: 
[blue]This function is used to load the state dictionary of a PyTorch model from an existing checkpoint or saved model. It allows for the transfer of learned parameters and other model state between different runs or environments, enabling faster development and deployment.[/blue]
2024-12-02 17:12:38
[
    {
        "arg": "state_dict",
        "description": "(OrderedDict) - The model state dictionary to be loaded into the current state."
    }
]
2024-12-02 17:12:55
function update_policy_weights_ description: 
[blue]This function is a method that updates the weights of a policy in a reinforcement learning model by calling the parent class's `update_policy_weights_` method.[/blue]
2024-12-02 17:13:44
[
    {
        "arg": "policy_weights",
        "description": "(Optional[TensorDictBase]) - The policy weights to be updated. (default: None)"
    },
    {
        "arg": "_",
        "description": " (...) -> None"
    }
]
2024-12-02 17:13:59
function frames_per_batch_worker description: 
[blue]This function is a read-only property that returns the requested frames per batch, indicating its significance as a configuration setting for video processing and batching in PyTorch.[/blue]
2024-12-02 17:13:59
[
    {
        "arg": "None"
    }
]
2024-12-02 17:14:59
function iterator description: 
[blue]This function is a PyTorch iterator that manages the data processing pipeline, handling tasks such as updating policy weights, sending messages to workers, and yielding processed output tensors.[/blue]
2024-12-02 17:14:59
[
    {
        "arg": "None"
    }
]
2024-12-02 17:15:26
function reset description: 
[blue]This function is used to reset the state of a worker in a multi-worker queue system, ensuring that workers continue processing tasks after a certain period or when new frames are available.[/blue]
2024-12-02 17:16:14
[
    {
        "arg": "reset_idx",
        "description": "(Optional[Sequence[bool]]) - The index of the model to be reset."
    },
    {
        "arg": "None",
        "description": " () - No description available."
    }
]
2024-12-02 17:16:30
class aSyncDataCollector description: 
[blue]This class is an asynchronous data collector, designed to efficiently collect and store data from various sources in a PyTorch environment, allowing for seamless integration with other components.[/blue]
2024-12-02 17:17:40
function __init__ description: 
[blue]This function is the constructor of a class in PyTorch, used to initialize an object with its attributes and parameters. It sets up the environment for training or testing a reinforcement learning algorithm.[/blue]
2024-12-02 17:21:30
[
    {
        "arg": "create_env_fn",
        "description": "(Callable[[], EnvBase]) - The function to create an environment."
    },
    {
        "arg": "policy",
        "description": "(Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]) - The policy used for exploration."
    },
    {
        "arg": "frames_per_batch",
        "description": "(int) - The number of frames per batch."
    },
    {
        "arg": "total_frames",
        "description": "(Optional[int]) - The total number of frames. (default: None)"
    },
    {
        "arg": "device",
        "description": "(DEVICE_TYPING | Sequence[DEVICE_TYPING] | None) - The device to use for the environment."
    },
    {
        "arg": "storing_device",
        "description": "(DEVICE_TYPING | Sequence[DEVICE_TYPING] | None) - The device to store the data on."
    },
    {
        "arg": "env_device",
        "description": "(DEVICE_TYPING | Sequence[DEVICE_TYPING] | None) - The device for the environment."
    },
    {
        "arg": "policy_device",
        "description": "(DEVICE_TYPING | Sequence[DEVICE_TYPING] | None) - The device for the policy."
    },
    {
        "arg": "create_env_kwargs",
        "description": "(Optional[Sequence[dict]]) - Keyword arguments to pass to create_env_fn."
    },
    {
        "arg": "max_frames_per_traj",
        "description": "(int | None) - The maximum number of frames per trajectory."
    },
    {
        "arg": "init_random_frames",
        "description": "(int | None) - The initial random frames."
    },
    {
        "arg": "reset_at_each_iter",
        "description": "(bool) - Whether to reset at each iteration."
    },
    {
        "arg": "postproc",
        "description": "(Optional[Callable[[TensorDictBase], TensorDictBase]]) - A post-processing function for the data."
    },
    {
        "arg": "split_trajs",
        "description": "(Optional[bool]) - Whether to split trajectories."
    },
    {
        "arg": "exploration_type",
        "description": "(ExplorationType) - The type of exploration used."
    },
    {
        "arg": "reset_when_done",
        "description": "(bool) - Whether to reset when done."
    },
    {
        "arg": "update_at_each_batch",
        "description": "(bool) - Whether to update at each batch."
    },
    {
        "arg": "preemptive_threshold",
        "description": "(float) - The preemptive threshold."
    },
    {
        "arg": "num_threads",
        "description": "(int) - The number of threads used."
    },
    {
        "arg": "num_sub_threads",
        "description": "(int) - The number of sub-threads used. (default: 1)"
    },
    {
        "arg": "set_truncated",
        "description": "(bool) - Whether to set truncated."
    }
]
2024-12-02 17:21:46
function next description: 
[blue]This function is a placeholder that delegates to the parent class's `next` method, effectively bypassing any custom implementation. It should be used when you want to maintain the default behavior of the parent class while still providing a consistent interface for subclasses.[/blue]
2024-12-02 17:21:46
[
    {
        "arg": "None"
    }
]
2024-12-02 17:21:58
function shutdown description: 
[blue]This function is a placeholder for overriding the `shutdown` method in PyTorch models, allowing developers to perform custom cleanup actions before model destruction.[/blue]
2024-12-02 17:21:58
[
    {
        "arg": "None"
    }
]
2024-12-02 17:22:16
function set_seed description: 
[blue]This function is used to set a random seed for reproducibility in PyTorch models. It ensures that the same sequence of random numbers is generated every time the model is run with the same seed value, making it easier to debug and test models.[/blue]
2024-12-02 17:23:07
[
    {
        "arg": "seed",
        "description": " (int) - The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) - Whether the static seed is used. (default: False)"
    }
]
2024-12-02 17:23:20
function state_dict description: 
[blue]This function is used to retrieve the model's state dictionary, which contains the learned parameters and their values, allowing for model loading and saving.[/blue]
2024-12-02 17:23:20
[
    {
        "arg": "None"
    }
]
2024-12-02 17:23:37
function load_state_dict description: 
[blue]This function is used to load the state dictionary of a PyTorch model from an existing checkpoint or saved model. It allows for the transfer of learned parameters and other model state between different runs or environments, enabling faster development and deployment.[/blue]
2024-12-02 17:24:21
[
    {
        "arg": "state_dict",
        "description": "(OrderedDict) - The model state dictionary to be loaded into the current state."
    }
]
2024-12-02 17:33:22
function split_trajectories description: 
[blue]This is a PyTorch implementation of the `split` function from the `torch.nn.utils.rnn` module. The purpose of this function is to split a tensor into multiple tensors, each containing a subset of the original data.

Here's a breakdown of what the code does:

1. It first checks if a `trajectory_key` is provided. If it is, it uses that key to determine the shape of the output tensors.
2. If no `trajectory_key` is provided, it falls back to using the `'traj_ids'` key and sets the corresponding mask tensor to all ones.
3. It then splits the input tensor into multiple tensors, each containing a subset of the original data. The number of subsets depends on the length of the input tensor.
4. For each subset, it pads the tensor with zeros to ensure that all subsets have the same shape.
5. Finally, it returns a stack of the padded tensors.

The code uses several PyTorch-specific features, such as:

* `torch._nested_view_from_buffer`: This function is used to create a nested view of a tensor, which allows for efficient computation on tensors with complex structures.
* `torch.nested.nested_tensor: This function is used to create a nested tensor from a list of tensors.
* `torch.ones`: This function is used to create a tensor filled with ones.

The code also uses some PyTorch-specific functions and variables, such as:

* `out_splits`
* `as_nested`
* `layout`

Overall, this code appears to be part of a larger library or framework for working with tensors in PyTorch.[/blue]
2024-12-02 17:34:47
[
    {
        "arg": "rollout_tensordict",
        "description": "(TensorDictBase) - The rollout tensor dictionary to be split into trajectories."
    },
    {
        "arg": "prefix",
        "description": "(str | None) - The prefix for the trajectory keys. (default: None)"
    },
    {
        "arg": "trajectory_key",
        "description": "(NestedKey | None) - The key for the trajectory data in the rollout tensor dictionary."
    },
    {
        "arg": "done_key",
        "description": "(NestedKey | None) - The key for the done signal in the rollout tensor dictionary."
    },
    {
        "arg": "as_nested",
        "description": "(bool) - Whether to return the trajectory data as a nested dictionary. (default: False)"
    }
]
2024-12-02 17:34:47
Total elapsed time (GenDescription Workflow): -62.74447028636932 min
2024-12-02 17:35:11
function recursive_map_to_cpu description: 
[blue]This function is used to recursively map PyTorch tensors and ordered dictionaries from the GPU to the CPU. It's significant for transferring models or data between devices, ensuring compatibility with different hardware configurations.[/blue]
2024-12-02 17:35:51
[
    {
        "arg": "dictionary",
        "description": "(OrderedDict) - The ordered dictionary to be mapped to CPU."
    }
]
2024-12-02 17:36:12
class DataCollectorBase description: 
[blue]This class is a base class for data collectors in PyTorch, providing a common interface for updating model weights and other state during training or testing. It allows for flexible customization of the data collection process through its methods, such as `update_policy_weights_` and `set_seed`, making it a crucial component in building robust and efficient machine learning pipelines.[/blue]
2024-12-02 17:36:35
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in PyTorch, ensuring consistency across devices when using data collectors and trained policies.[/blue]
2024-12-02 17:37:18
[
    {
        "arg": "policy_weights",
        "description": "(Optional[TensorDictBase]) - The policy weights to be updated. (default: None)"
    },
    {
        "arg": "_",
        "description": " (...) -> None"
    }
]
2024-12-02 17:37:33
function next description: 
[blue]This function is used to retrieve the next item from an iterator, handling cases where the iterator has reached its end.[/blue]
2024-12-02 17:37:33
[
    {
        "arg": "None"
    }
]
2024-12-02 17:37:46
function shutdown description: 
[blue]This function is an abstract method that serves as a hook for customizing the shutdown process of a PyTorch module, allowing developers to implement specific cleanup actions before the module is unloaded.[/blue]
2024-12-02 17:37:46
[
    {
        "arg": "None"
    }
]
2024-12-02 17:37:58
function iterator description: 
[blue]This function is an abstract base class method that defines the interface for creating an iterator over a TensorDict, allowing for iteration over its key-value pairs or values.[/blue]
2024-12-02 17:37:58
[
    {
        "arg": "None"
    }
]
2024-12-02 17:38:13
function set_seed description: 
[blue]This function is used to set a random seed for reproducibility in PyTorch models, ensuring that the same sequence of random numbers is generated during training and testing, thereby facilitating reliable comparisons and debugging.[/blue]
2024-12-02 17:38:57
[
    {
        "arg": "seed",
        "description": " (int) - The random seed to be used. (default: None)"
    },
    {
        "arg": "static_seed",
        "description": " (bool) - Whether the static seed is used. (default: False)"
    }
]
2024-12-02 17:39:10
function state_dict description: 
[blue]This function is used to serialize and retrieve the model's weights and biases into a dictionary, allowing for easy saving and loading of the model's state.[/blue]
2024-12-02 17:39:10
[
    {
        "arg": "None"
    }
]
2024-12-03 15:57:48
collectors.py: [
    {
        "function_name": "recursive_map_to_cpu",
        "args": [
            "dictionary"
        ],
        "signature": "recursive_map_to_cpu(dictionary: OrderedDict) -> OrderedDict",
        "function_code": "def recursive_map_to_cpu(dictionary: OrderedDict) -> OrderedDict:\n    \"\"\"Maps the tensors to CPU through a nested dictionary.\"\"\"\n    return OrderedDict(**{k: recursive_map_to_cpu(item) if isinstance(item, OrderedDict) else item.cpu() if isinstance(item, torch.Tensor) else item for k, item in dictionary.items()})",
        "description": ""
    },
    {
        "class_name": "DataCollectorBase",
        "bases": [
            "IterableDataset"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "update_policy_weights_",
                "args": [
                    "self",
                    "policy_weights"
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    \"\"\"Updates the policy weights if the policy of the data collector and the trained policy live on different devices.\n\n        Args:\n            policy_weights (TensorDictBase, optional): if provided, a TensorDict containing\n                the weights of the policy to be used for the udpdate.\n\n        \"\"\"\n    if policy_weights is not None:\n        self.policy_weights.data.update_(policy_weights)\n    elif self.get_weights_fn is not None:\n        self.policy_weights.data.update_(self.get_weights_fn())",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [
                    "self"
                ],
                "signature": "next(self)",
                "function_code": "def next(self):\n    try:\n        if self._iterator is None:\n            self._iterator = iter(self)\n        out = next(self._iterator)\n        out.clear_device_()\n        return out\n    except StopIteration:\n        return None",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [
                    "self"
                ],
                "signature": "shutdown(self)",
                "function_code": "@abc.abstractmethod\ndef shutdown(self):\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [
                    "self"
                ],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "@abc.abstractmethod\ndef iterator(self) -> Iterator[TensorDictBase]:\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    "self",
                    "seed",
                    "static_seed"
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "@abc.abstractmethod\ndef set_seed(self, seed: int, static_seed: bool=False) -> int:\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [
                    "self"
                ],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "@abc.abstractmethod\ndef state_dict(self) -> OrderedDict:\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    "self",
                    "state_dict"
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "@abc.abstractmethod\ndef load_state_dict(self, state_dict: OrderedDict) -> None:\n    raise NotImplementedError",
                "description": ""
            }
        ]
    },
    {
        "class_name": "SyncDataCollector",
        "bases": [
            "DataCollectorBase"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "__init__",
                "args": [
                    "self",
                    "create_env_fn",
                    "policy",
                    "frames_per_batch",
                    "total_frames",
                    "device",
                    "storing_device",
                    "policy_device",
                    "env_device",
                    "create_env_kwargs",
                    "max_frames_per_traj",
                    "init_random_frames",
                    "reset_at_each_iter",
                    "postproc",
                    "split_trajs",
                    "exploration_type",
                    "return_same_td",
                    "reset_when_done",
                    "interruptor",
                    "set_truncated",
                    "use_buffers",
                    "replay_buffer",
                    "trust_policy",
                    "compile_policy",
                    "cudagraph_policy"
                ],
                "signature": "__init__(self, create_env_fn: Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]=None, *, frames_per_batch: int, total_frames: int=-1, device: DEVICE_TYPING=None, storing_device: DEVICE_TYPING=None, policy_device: DEVICE_TYPING=None, env_device: DEVICE_TYPING=None, create_env_kwargs: dict | None=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Callable[[TensorDictBase], TensorDictBase] | None=None, split_trajs: bool | None=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, return_same_td: bool=False, reset_when_done: bool=True, interruptor=None, set_truncated: bool=False, use_buffers: bool | None=None, replay_buffer: ReplayBuffer | None=None, trust_policy: bool=None, compile_policy: bool | Dict[str, Any] | None=None, cudagraph_policy: bool | Dict[str, Any] | None=None, **kwargs)",
                "function_code": "def __init__(self, create_env_fn: Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]=None, *, frames_per_batch: int, total_frames: int=-1, device: DEVICE_TYPING=None, storing_device: DEVICE_TYPING=None, policy_device: DEVICE_TYPING=None, env_device: DEVICE_TYPING=None, create_env_kwargs: dict | None=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Callable[[TensorDictBase], TensorDictBase] | None=None, split_trajs: bool | None=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, return_same_td: bool=False, reset_when_done: bool=True, interruptor=None, set_truncated: bool=False, use_buffers: bool | None=None, replay_buffer: ReplayBuffer | None=None, trust_policy: bool=None, compile_policy: bool | Dict[str, Any] | None=None, cudagraph_policy: bool | Dict[str, Any] | None=None, **kwargs):\n    from torchrl.envs.batched_envs import BatchedEnvBase\n    self.closed = True\n    if create_env_kwargs is None:\n        create_env_kwargs = {}\n    if not isinstance(create_env_fn, EnvBase):\n        env = create_env_fn(**create_env_kwargs)\n    else:\n        env = create_env_fn\n        if create_env_kwargs:\n            if not isinstance(env, BatchedEnvBase):\n                raise RuntimeError(f\"kwargs were passed to SyncDataCollector but they can't be set on environment of type {type(create_env_fn)}.\")\n            env.update_kwargs(create_env_kwargs)\n    if policy is None:\n        policy = RandomPolicy(env.full_action_spec)\n    if trust_policy is None:\n        trust_policy = isinstance(policy, (RandomPolicy, CudaGraphModule))\n    self.trust_policy = trust_policy\n    self._read_compile_kwargs(compile_policy, cudagraph_policy)\n    self._traj_pool_val = kwargs.pop('traj_pool', None)\n    if kwargs:\n        raise TypeError(f'Keys {list(kwargs.keys())} are unknown to {type(self).__name__}.')\n    storing_device, policy_device, env_device = self._get_devices(storing_device=storing_device, policy_device=policy_device, env_device=env_device, device=device)\n    self.storing_device = storing_device\n    if self.storing_device is not None and self.storing_device.type != 'cuda':\n        if torch.cuda.is_available():\n            self._sync_storage = torch.cuda.synchronize\n        elif torch.backends.mps.is_available() and hasattr(torch, 'mps'):\n            self._sync_storage = torch.mps.synchronize\n        elif self.storing_device.type == 'cpu':\n            self._sync_storage = _do_nothing\n        else:\n            raise RuntimeError('Non supported device')\n    else:\n        self._sync_storage = _do_nothing\n    self.env_device = env_device\n    if self.env_device is not None and self.env_device.type != 'cuda':\n        if torch.cuda.is_available():\n            self._sync_env = torch.cuda.synchronize\n        elif torch.backends.mps.is_available() and hasattr(torch, 'mps'):\n            self._sync_env = torch.mps.synchronize\n        elif self.env_device.type == 'cpu':\n            self._sync_env = _do_nothing\n        else:\n            raise RuntimeError('Non supported device')\n    else:\n        self._sync_env = _do_nothing\n    self.policy_device = policy_device\n    if self.policy_device is not None and self.policy_device.type != 'cuda':\n        if torch.cuda.is_available():\n            self._sync_policy = torch.cuda.synchronize\n        elif torch.backends.mps.is_available() and hasattr(torch, 'mps'):\n            self._sync_policy = torch.mps.synchronize\n        elif self.policy_device.type == 'cpu':\n            self._sync_policy = _do_nothing\n        else:\n            raise RuntimeError('Non supported device')\n    else:\n        self._sync_policy = _do_nothing\n    self.device = device\n    self._cast_to_policy_device = self.policy_device != self.env_device\n    self.env: EnvBase = env\n    del env\n    self.replay_buffer = replay_buffer\n    if self.replay_buffer is not None:\n        if postproc is not None:\n            raise TypeError('postproc must be None when a replay buffer is passed.')\n        if use_buffers:\n            raise TypeError('replay_buffer is exclusive with use_buffers.')\n    if use_buffers is None:\n        use_buffers = not self.env._has_dynamic_specs and self.replay_buffer is None\n    self._use_buffers = use_buffers\n    self.replay_buffer = replay_buffer\n    self.closed = False\n    if not reset_when_done:\n        raise ValueError('reset_when_done is deprectated.')\n    self.reset_when_done = reset_when_done\n    self.n_env = self.env.batch_size.numel()\n    self.policy, self.get_weights_fn = self._get_policy_and_device(policy=policy, observation_spec=self.env.observation_spec)\n    if isinstance(self.policy, nn.Module):\n        self.policy_weights = TensorDict.from_module(self.policy, as_module=True)\n    else:\n        self.policy_weights = TensorDict()\n    if self.compiled_policy:\n        self.policy = torch.compile(self.policy, **self.compiled_policy_kwargs)\n    if self.cudagraphed_policy:\n        self.policy = CudaGraphModule(self.policy, **self.cudagraphed_policy_kwargs)\n    if self.env_device:\n        self.env: EnvBase = self.env.to(self.env_device)\n    elif self.env.device is not None:\n        self.env_device = self.env.device\n    self._cast_to_env_device = self._cast_to_policy_device or self.env.device != self.storing_device\n    self.max_frames_per_traj = int(max_frames_per_traj) if max_frames_per_traj is not None else 0\n    if self.max_frames_per_traj is not None and self.max_frames_per_traj > 0:\n        for key in self.env.output_spec.keys(True, True):\n            if isinstance(key, str):\n                key = (key,)\n            if 'step_count' in key:\n                raise ValueError(\"A 'step_count' key is already present in the environment and the 'max_frames_per_traj' argument may conflict with a 'StepCounter' that has already been set. Possible solutions: Set max_frames_per_traj to 0 or remove the StepCounter limit from the environment transforms.\")\n        self.env = TransformedEnv(self.env, StepCounter(max_steps=self.max_frames_per_traj))\n    if total_frames is None or total_frames < 0:\n        total_frames = float('inf')\n    else:\n        remainder = total_frames % frames_per_batch\n        if remainder != 0 and RL_WARNINGS:\n            warnings.warn(f'total_frames ({total_frames}) is not exactly divisible by frames_per_batch ({frames_per_batch}).This means {frames_per_batch - remainder} additional frames will be collected.To silence this message, set the environment variable RL_WARNINGS to False.')\n    self.total_frames = int(total_frames) if total_frames != float('inf') else total_frames\n    self.reset_at_each_iter = reset_at_each_iter\n    self.init_random_frames = int(init_random_frames) if init_random_frames is not None else 0\n    if init_random_frames is not None and init_random_frames % frames_per_batch != 0 and RL_WARNINGS:\n        warnings.warn(f'init_random_frames ({init_random_frames}) is not exactly a multiple of frames_per_batch ({frames_per_batch}),  this results in more init_random_frames than requested ({-(-init_random_frames // frames_per_batch) * frames_per_batch}).To silence this message, set the environment variable RL_WARNINGS to False.')\n    self.postproc = postproc\n    if self.postproc is not None and hasattr(self.postproc, 'to') and self.storing_device:\n        self.postproc.to(self.storing_device)\n    if frames_per_batch % self.n_env != 0 and RL_WARNINGS:\n        warnings.warn(f'frames_per_batch ({frames_per_batch}) is not exactly divisible by the number of batched environments ({self.n_env}),  this results in more frames_per_batch per iteration that requested ({-(-frames_per_batch // self.n_env) * self.n_env}).To silence this message, set the environment variable RL_WARNINGS to False.')\n    self.requested_frames_per_batch = int(frames_per_batch)\n    self.frames_per_batch = -(-frames_per_batch // self.n_env)\n    self.exploration_type = exploration_type if exploration_type else DEFAULT_EXPLORATION_TYPE\n    self.return_same_td = return_same_td\n    self.set_truncated = set_truncated\n    self._make_shuttle()\n    if self._use_buffers:\n        self._make_final_rollout()\n    self._set_truncated_keys()\n    if split_trajs is None:\n        split_trajs = False\n    self.split_trajs = split_trajs\n    self._exclude_private_keys = True\n    self.interruptor = interruptor\n    self._frames = 0\n    self._iter = -1",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [
                    "self"
                ],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "update_policy_weights_",
                "args": [
                    "self",
                    "policy_weights"
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    super().update_policy_weights_(policy_weights)",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    "self",
                    "seed",
                    "static_seed"
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    \"\"\"Sets the seeds of the environments stored in the DataCollector.\n\n        Args:\n            seed (int): integer representing the seed to be used for the environment.\n            static_seed(bool, optional): if ``True``, the seed is not incremented.\n                Defaults to False\n\n        Returns:\n            Output seed. This is useful when more than one environment is contained in the DataCollector, as the\n            seed will be incremented for each of these. The resulting seed is the seed of the last environment.\n\n        Examples:\n            >>> from torchrl.envs import ParallelEnv\n            >>> from torchrl.envs.libs.gym import GymEnv\n            >>> from tensordict.nn import TensorDictModule\n            >>> from torch import nn\n            >>> env_fn = lambda: GymEnv(\"Pendulum-v1\")\n            >>> env_fn_parallel = ParallelEnv(6, env_fn)\n            >>> policy = TensorDictModule(nn.Linear(3, 1), in_keys=[\"observation\"], out_keys=[\"action\"])\n            >>> collector = SyncDataCollector(env_fn_parallel, policy, total_frames=300, frames_per_batch=100)\n            >>> out_seed = collector.set_seed(1)  # out_seed = 6\n\n        \"\"\"\n    out = self.env.set_seed(seed, static_seed=static_seed)\n    return out",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [
                    "self"
                ],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "def iterator(self) -> Iterator[TensorDictBase]:\n    \"\"\"Iterates through the DataCollector.\n\n        Yields: TensorDictBase objects containing (chunks of) trajectories\n\n        \"\"\"\n    if self.storing_device and self.storing_device.type == 'cuda':\n        stream = torch.cuda.Stream(self.storing_device, priority=-1)\n        event = stream.record_event()\n        streams = [stream]\n        events = [event]\n    elif self.storing_device is None:\n        streams = []\n        events = []\n        cuda_devices = set()\n\n        def cuda_check(tensor: torch.Tensor):\n            if tensor.is_cuda:\n                cuda_devices.add(tensor.device)\n        if not self._use_buffers:\n            for spec in self.env.specs.values(True, True):\n                if spec.device.type == 'cuda':\n                    if ':' not in str(spec.device):\n                        raise RuntimeError(\"A cuda spec did not have a device associated. Make sure to pass `'cuda:device_num'` to each spec device.\")\n                    cuda_devices.add(spec.device)\n        else:\n            self._final_rollout.apply(cuda_check, filter_empty=True)\n        for device in cuda_devices:\n            streams.append(torch.cuda.Stream(device, priority=-1))\n            events.append(streams[-1].record_event())\n    else:\n        streams = []\n        events = []\n    with contextlib.ExitStack() as stack:\n        for stream in streams:\n            stack.enter_context(torch.cuda.stream(stream))\n        while self._frames < self.total_frames:\n            self._iter += 1\n            tensordict_out = self.rollout()\n            if tensordict_out is None:\n                yield\n                continue\n            self._increment_frames(tensordict_out.numel())\n            if self.split_trajs:\n                tensordict_out = split_trajectories(tensordict_out, prefix='collector')\n            if self.postproc is not None:\n                tensordict_out = self.postproc(tensordict_out)\n            if self._exclude_private_keys:\n\n                def is_private(key):\n                    if isinstance(key, str) and key.startswith('_'):\n                        return True\n                    if isinstance(key, tuple) and any((_key.startswith('_') for _key in key)):\n                        return True\n                    return False\n                excluded_keys = [key for key in tensordict_out.keys(True) if is_private(key)]\n                tensordict_out = tensordict_out.exclude(*excluded_keys, inplace=True)\n            if self.return_same_td:\n                if events:\n                    for event in events:\n                        event.record()\n                        event.synchronize()\n                yield tensordict_out\n            else:\n                yield tensordict_out.clone()",
                "description": ""
            },
            {
                "function_name": "rollout",
                "args": [
                    "self"
                ],
                "signature": "rollout(self) -> TensorDictBase",
                "function_code": "@torch.no_grad()\ndef rollout(self) -> TensorDictBase:\n    \"\"\"Computes a rollout in the environment using the provided policy.\n\n        Returns:\n            TensorDictBase containing the computed rollout.\n\n        \"\"\"\n    if self.reset_at_each_iter:\n        self._shuttle.update(self.env.reset())\n    if self._use_buffers:\n        self._final_rollout.fill_(('collector', 'traj_ids'), -1)\n    else:\n        pass\n    tensordicts = []\n    with set_exploration_type(self.exploration_type):\n        for t in range(self.frames_per_batch):\n            if self.init_random_frames is not None and self._frames < self.init_random_frames:\n                self.env.rand_action(self._shuttle)\n            else:\n                if self._cast_to_policy_device:\n                    if self.policy_device is not None:\n                        policy_input = self._shuttle.to(self.policy_device, non_blocking=True)\n                        self._sync_policy()\n                    elif self.policy_device is None:\n                        policy_input = self._shuttle\n                else:\n                    policy_input = self._shuttle\n                policy_output = self.policy(policy_input)\n                if self._shuttle is not policy_output:\n                    self._shuttle.update(policy_output, keys_to_update=self._policy_output_keys)\n            if self._cast_to_env_device:\n                if self.env_device is not None:\n                    env_input = self._shuttle.to(self.env_device, non_blocking=True)\n                    self._sync_env()\n                elif self.env_device is None:\n                    env_input = self._shuttle\n            else:\n                env_input = self._shuttle\n            env_output, env_next_output = self.env.step_and_maybe_reset(env_input)\n            if self._shuttle is not env_output:\n                next_data = env_output.get('next')\n                if self._shuttle_has_no_device:\n                    next_data.clear_device_()\n                self._shuttle.set('next', next_data)\n            if self.replay_buffer is not None:\n                self.replay_buffer.add(self._shuttle)\n                if self._increment_frames(self._shuttle.numel()):\n                    return\n            elif self.storing_device is not None:\n                tensordicts.append(self._shuttle.to(self.storing_device, non_blocking=True))\n                self._sync_storage()\n            else:\n                tensordicts.append(self._shuttle)\n            collector_data = self._shuttle.get('collector').copy()\n            self._shuttle = env_next_output\n            if self._shuttle_has_no_device:\n                self._shuttle.clear_device_()\n            self._shuttle.set('collector', collector_data)\n            self._update_traj_ids(env_output)\n            if self.interruptor is not None and self.interruptor.collection_stopped():\n                if self.replay_buffer is not None:\n                    return\n                result = self._final_rollout\n                if self._use_buffers:\n                    try:\n                        torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout[..., :t + 1])\n                    except RuntimeError:\n                        with self._final_rollout.unlock_():\n                            torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout[..., :t + 1])\n                else:\n                    result = TensorDict.maybe_dense_stack(tensordicts, dim=-1)\n                break\n        else:\n            if self._use_buffers:\n                result = self._final_rollout\n                try:\n                    result = torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout)\n                except RuntimeError:\n                    with self._final_rollout.unlock_():\n                        result = torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout)\n            elif self.replay_buffer is not None:\n                return\n            else:\n                result = TensorDict.maybe_dense_stack(tensordicts, dim=-1)\n                result.refine_names(..., 'time')\n    return self._maybe_set_truncated(result)",
                "description": ""
            },
            {
                "function_name": "reset",
                "args": [
                    "self",
                    "index"
                ],
                "signature": "reset(self, index=None, **kwargs) -> None",
                "function_code": "@torch.no_grad()\ndef reset(self, index=None, **kwargs) -> None:\n    \"\"\"Resets the environments to a new initial state.\"\"\"\n    collector_metadata = self._shuttle.get('collector').clone()\n    if index is not None:\n        if prod(self.env.batch_size) == 0:\n            raise RuntimeError('resetting unique env with index is not permitted.')\n        for reset_key, done_keys in zip(self.env.reset_keys, self.env.done_keys_groups):\n            _reset = torch.zeros(self.env.full_done_spec[done_keys[0]].shape, dtype=torch.bool, device=self.env.device)\n            _reset[index] = 1\n            self._shuttle.set(reset_key, _reset)\n    else:\n        _reset = None\n        self._shuttle.zero_()\n    self._shuttle.update(self.env.reset(**kwargs), inplace=True)\n    collector_metadata['traj_ids'] = collector_metadata['traj_ids'] - collector_metadata['traj_ids'].min()\n    self._shuttle['collector'] = collector_metadata",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [
                    "self"
                ],
                "signature": "shutdown(self) -> None",
                "function_code": "def shutdown(self) -> None:\n    \"\"\"Shuts down all workers and/or closes the local environment.\"\"\"\n    if not self.closed:\n        self.closed = True\n        del self._shuttle\n        if self._use_buffers:\n            del self._final_rollout\n        if not self.env.is_closed:\n            self.env.close()\n        del self.env\n    return",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [
                    "self"
                ],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    \"\"\"Returns the local state_dict of the data collector (environment and policy).\n\n        Returns:\n            an ordered dictionary with fields :obj:`\"policy_state_dict\"` and\n            `\"env_state_dict\"`.\n\n        \"\"\"\n    from torchrl.envs.batched_envs import BatchedEnvBase\n    if isinstance(self.env, TransformedEnv):\n        env_state_dict = self.env.transform.state_dict()\n    elif isinstance(self.env, BatchedEnvBase):\n        env_state_dict = self.env.state_dict()\n    else:\n        env_state_dict = OrderedDict()\n    if hasattr(self.policy, 'state_dict'):\n        policy_state_dict = self.policy.state_dict()\n        state_dict = OrderedDict(policy_state_dict=policy_state_dict, env_state_dict=env_state_dict)\n    else:\n        state_dict = OrderedDict(env_state_dict=env_state_dict)\n    state_dict.update({'frames': self._frames, 'iter': self._iter})\n    return state_dict",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    "self",
                    "state_dict"
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict, **kwargs) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict, **kwargs) -> None:\n    \"\"\"Loads a state_dict on the environment and policy.\n\n        Args:\n            state_dict (OrderedDict): ordered dictionary containing the fields\n                `\"policy_state_dict\"` and :obj:`\"env_state_dict\"`.\n\n        \"\"\"\n    strict = kwargs.get('strict', True)\n    if strict or 'env_state_dict' in state_dict:\n        self.env.load_state_dict(state_dict['env_state_dict'], **kwargs)\n    if strict or 'policy_state_dict' in state_dict:\n        self.policy.load_state_dict(state_dict['policy_state_dict'], **kwargs)\n    self._frames = state_dict['frames']\n    self._iter = state_dict['iter']",
                "description": ""
            }
        ]
    },
    {
        "class_name": "MultiSyncDataCollector",
        "bases": [
            "_MultiDataCollector"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "next",
                "args": [
                    "self"
                ],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [
                    "self"
                ],
                "signature": "shutdown(self)",
                "function_code": "def shutdown(self):\n    if hasattr(self, 'out_buffer'):\n        del self.out_buffer\n    if hasattr(self, 'buffers'):\n        del self.buffers\n    return super().shutdown()",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    "self",
                    "seed",
                    "static_seed"
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    return super().set_seed(seed, static_seed)",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [
                    "self"
                ],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    return super().state_dict()",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    "self",
                    "state_dict"
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict) -> None:\n    return super().load_state_dict(state_dict)",
                "description": ""
            },
            {
                "function_name": "update_policy_weights_",
                "args": [
                    "self",
                    "policy_weights"
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    super().update_policy_weights_(policy_weights)",
                "description": ""
            },
            {
                "function_name": "frames_per_batch_worker",
                "args": [
                    "self"
                ],
                "signature": "frames_per_batch_worker(self)",
                "function_code": "@property\ndef frames_per_batch_worker(self):\n    if self.requested_frames_per_batch % self.num_workers != 0 and RL_WARNINGS:\n        warnings.warn(f'frames_per_batch {self.requested_frames_per_batch} is not exactly divisible by the number of collector workers {self.num_workers}, this results in more frames_per_batch per iteration that requested.To silence this message, set the environment variable RL_WARNINGS to False.')\n    frames_per_batch_worker = -(-self.requested_frames_per_batch // self.num_workers)\n    return frames_per_batch_worker",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [
                    "self"
                ],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "def iterator(self) -> Iterator[TensorDictBase]:\n    cat_results = self.cat_results\n    if cat_results is None:\n        cat_results = 'stack'\n    self.buffers = {}\n    dones = [False for _ in range(self.num_workers)]\n    workers_frames = [0 for _ in range(self.num_workers)]\n    same_device = None\n    self.out_buffer = None\n    preempt = self.interruptor is not None and self.preemptive_threshold < 1.0\n    while not all(dones) and self._frames < self.total_frames:\n        _check_for_faulty_process(self.procs)\n        if self.update_at_each_batch:\n            self.update_policy_weights_()\n        for idx in range(self.num_workers):\n            if self.init_random_frames is not None and self._frames < self.init_random_frames:\n                msg = 'continue_random'\n            else:\n                msg = 'continue'\n            self.pipes[idx].send((None, msg))\n        self._iter += 1\n        if preempt:\n            self.interruptor.start_collection()\n            while self.queue_out.qsize() < int(self.num_workers * self.preemptive_threshold):\n                continue\n            self.interruptor.stop_collection()\n            while self.queue_out.qsize() < int(self.num_workers):\n                continue\n        for _ in range(self.num_workers):\n            new_data, j = self.queue_out.get()\n            use_buffers = self._use_buffers\n            if self.replay_buffer is not None:\n                idx = new_data\n                workers_frames[idx] = workers_frames[idx] + self.frames_per_batch_worker\n                continue\n            elif j == 0 or not use_buffers:\n                try:\n                    data, idx = new_data\n                    self.buffers[idx] = data\n                    if use_buffers is None and j > 0:\n                        self._use_buffers = False\n                except TypeError:\n                    if use_buffers is None:\n                        self._use_buffers = True\n                        idx = new_data\n                    else:\n                        raise\n            else:\n                idx = new_data\n            if preempt:\n                if cat_results != 'stack':\n                    buffers = {}\n                    for idx, buffer in self.buffers.items():\n                        valid = buffer.get(('collector', 'traj_ids')) != -1\n                        if valid.ndim > 2:\n                            valid = valid.flatten(0, -2)\n                        if valid.ndim == 2:\n                            valid = valid.any(0)\n                        buffers[idx] = buffer[..., valid]\n                else:\n                    for buffer in self.buffers.values():\n                        with buffer.unlock_():\n                            buffer.set(('collector', 'mask'), buffer.get(('collector', 'traj_ids')) != -1)\n                    buffers = self.buffers\n            else:\n                buffers = self.buffers\n            workers_frames[idx] = workers_frames[idx] + buffers[idx].numel()\n            if workers_frames[idx] >= self.total_frames:\n                dones[idx] = True\n        if self.replay_buffer is not None:\n            yield\n            self._frames += self.frames_per_batch_worker * self.num_workers\n            continue\n        n_collected = 0\n        for idx in range(self.num_workers):\n            buffer = buffers[idx]\n            traj_ids = buffer.get(('collector', 'traj_ids'))\n            if preempt:\n                if cat_results == 'stack':\n                    mask_frames = buffer.get(('collector', 'traj_ids')) != -1\n                    n_collected += mask_frames.sum().cpu()\n                else:\n                    n_collected += traj_ids.numel()\n            else:\n                n_collected += traj_ids.numel()\n        if same_device is None:\n            prev_device = None\n            same_device = True\n            for item in self.buffers.values():\n                if prev_device is None:\n                    prev_device = item.device\n                else:\n                    same_device = same_device and item.device == prev_device\n        if cat_results == 'stack':\n            stack = torch.stack if self._use_buffers else TensorDict.maybe_dense_stack\n            if same_device:\n                self.out_buffer = stack(list(buffers.values()), 0)\n            else:\n                self.out_buffer = stack([item.cpu() for item in buffers.values()], 0)\n        else:\n            if self._use_buffers is None:\n                torchrl_logger.warning('use_buffer not specified and not yet inferred from data, assuming `True`.')\n            elif not self._use_buffers:\n                raise RuntimeError('Cannot concatenate results with use_buffers=False')\n            try:\n                if same_device:\n                    self.out_buffer = torch.cat(list(buffers.values()), cat_results)\n                else:\n                    self.out_buffer = torch.cat([item.cpu() for item in buffers.values()], cat_results)\n            except RuntimeError as err:\n                if preempt and cat_results != -1 and ('Sizes of tensors must match' in str(err)):\n                    raise RuntimeError(\"The value provided to cat_results isn't compatible with the collectors outputs. Consider using `cat_results=-1`.\")\n                raise\n        if self.split_trajs:\n            out = split_trajectories(self.out_buffer, prefix='collector')\n        else:\n            out = self.out_buffer\n        if cat_results in (-1, 'stack'):\n            out.refine_names(*[None] * (out.ndim - 1) + ['time'])\n        self._frames += n_collected\n        if self.postprocs:\n            self.postprocs = self.postprocs.to(out.device)\n            out = self.postprocs(out)\n        if self._exclude_private_keys:\n            excluded_keys = [key for key in out.keys() if key.startswith('_')]\n            if excluded_keys:\n                out = out.exclude(*excluded_keys)\n        yield out\n        del out\n    del self.buffers\n    self.out_buffer = None",
                "description": ""
            }
        ]
    },
    {
        "class_name": "MultiaSyncDataCollector",
        "bases": [
            "_MultiDataCollector"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "__init__",
                "args": [
                    "self"
                ],
                "signature": "__init__(self, *args, **kwargs)",
                "function_code": "def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.out_tensordicts = defaultdict(lambda: None)\n    self.running = False\n    if self.postprocs is not None:\n        postproc = self.postprocs\n        self.postprocs = {}\n        for _device in self.storing_device:\n            if _device not in self.postprocs:\n                self.postprocs[_device] = deepcopy(postproc).to(_device)",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [
                    "self"
                ],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [
                    "self"
                ],
                "signature": "shutdown(self)",
                "function_code": "def shutdown(self):\n    if hasattr(self, 'out_tensordicts'):\n        del self.out_tensordicts\n    return super().shutdown()",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    "self",
                    "seed",
                    "static_seed"
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    return super().set_seed(seed, static_seed)",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [
                    "self"
                ],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    return super().state_dict()",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    "self",
                    "state_dict"
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict) -> None:\n    return super().load_state_dict(state_dict)",
                "description": ""
            },
            {
                "function_name": "update_policy_weights_",
                "args": [
                    "self",
                    "policy_weights"
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    super().update_policy_weights_(policy_weights)",
                "description": ""
            },
            {
                "function_name": "frames_per_batch_worker",
                "args": [
                    "self"
                ],
                "signature": "frames_per_batch_worker(self)",
                "function_code": "@property\ndef frames_per_batch_worker(self):\n    return self.requested_frames_per_batch",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [
                    "self"
                ],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "def iterator(self) -> Iterator[TensorDictBase]:\n    if self.update_at_each_batch:\n        self.update_policy_weights_()\n    for i in range(self.num_workers):\n        if self.init_random_frames is not None and self.init_random_frames > 0:\n            self.pipes[i].send((None, 'continue_random'))\n        else:\n            self.pipes[i].send((None, 'continue'))\n    self.running = True\n    workers_frames = [0 for _ in range(self.num_workers)]\n    while self._frames < self.total_frames:\n        self._iter += 1\n        while True:\n            try:\n                idx, j, out = self._get_from_queue(timeout=10.0)\n                break\n            except TimeoutError:\n                _check_for_faulty_process(self.procs)\n        if self.replay_buffer is None:\n            worker_frames = out.numel()\n            if self.split_trajs:\n                out = split_trajectories(out, prefix='collector')\n        else:\n            worker_frames = self.frames_per_batch_worker\n        self._frames += worker_frames\n        workers_frames[idx] = workers_frames[idx] + worker_frames\n        if self.postprocs:\n            out = self.postprocs[out.device](out)\n        if self.init_random_frames is not None and self._frames < self.init_random_frames:\n            msg = 'continue_random'\n        else:\n            msg = 'continue'\n        self.pipes[idx].send((idx, msg))\n        if out is not None and self._exclude_private_keys:\n            excluded_keys = [key for key in out.keys() if key.startswith('_')]\n            out = out.exclude(*excluded_keys)\n        yield out\n    self.running = False",
                "description": ""
            },
            {
                "function_name": "reset",
                "args": [
                    "self",
                    "reset_idx"
                ],
                "signature": "reset(self, reset_idx: Optional[Sequence[bool]]=None) -> None",
                "function_code": "def reset(self, reset_idx: Optional[Sequence[bool]]=None) -> None:\n    super().reset(reset_idx)\n    if self.queue_out.full():\n        time.sleep(_TIMEOUT)\n    if self.queue_out.full():\n        raise Exception('self.queue_out is full')\n    if self.running:\n        for idx in range(self.num_workers):\n            if self.init_random_frames is not None and self._frames < self.init_random_frames:\n                self.pipes[idx].send((idx, 'continue_random'))\n            else:\n                self.pipes[idx].send((idx, 'continue'))",
                "description": ""
            }
        ]
    },
    {
        "class_name": "aSyncDataCollector",
        "bases": [
            "MultiaSyncDataCollector"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "__init__",
                "args": [
                    "self",
                    "create_env_fn",
                    "policy",
                    "frames_per_batch",
                    "total_frames",
                    "device",
                    "storing_device",
                    "env_device",
                    "policy_device",
                    "create_env_kwargs",
                    "max_frames_per_traj",
                    "init_random_frames",
                    "reset_at_each_iter",
                    "postproc",
                    "split_trajs",
                    "exploration_type",
                    "reset_when_done",
                    "update_at_each_batch",
                    "preemptive_threshold",
                    "num_threads",
                    "num_sub_threads",
                    "set_truncated"
                ],
                "signature": "__init__(self, create_env_fn: Callable[[], EnvBase], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]], *, frames_per_batch: int, total_frames: Optional[int]=-1, device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, storing_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, env_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, policy_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, create_env_kwargs: Optional[Sequence[dict]]=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Optional[Callable[[TensorDictBase], TensorDictBase]]=None, split_trajs: Optional[bool]=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, reset_when_done: bool=True, update_at_each_batch: bool=False, preemptive_threshold: float=None, num_threads: int=None, num_sub_threads: int=1, set_truncated: bool=False, **kwargs)",
                "function_code": "def __init__(self, create_env_fn: Callable[[], EnvBase], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]], *, frames_per_batch: int, total_frames: Optional[int]=-1, device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, storing_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, env_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, policy_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, create_env_kwargs: Optional[Sequence[dict]]=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Optional[Callable[[TensorDictBase], TensorDictBase]]=None, split_trajs: Optional[bool]=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, reset_when_done: bool=True, update_at_each_batch: bool=False, preemptive_threshold: float=None, num_threads: int=None, num_sub_threads: int=1, set_truncated: bool=False, **kwargs):\n    super().__init__(create_env_fn=[create_env_fn], policy=policy, total_frames=total_frames, create_env_kwargs=[create_env_kwargs], max_frames_per_traj=max_frames_per_traj, frames_per_batch=frames_per_batch, reset_at_each_iter=reset_at_each_iter, init_random_frames=init_random_frames, postproc=postproc, split_trajs=split_trajs, device=device, policy_device=policy_device, env_device=env_device, storing_device=storing_device, exploration_type=exploration_type, reset_when_done=reset_when_done, update_at_each_batch=update_at_each_batch, preemptive_threshold=preemptive_threshold, num_threads=num_threads, num_sub_threads=num_sub_threads, set_truncated=set_truncated)",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [
                    "self"
                ],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [
                    "self"
                ],
                "signature": "shutdown(self)",
                "function_code": "def shutdown(self):\n    return super().shutdown()",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    "self",
                    "seed",
                    "static_seed"
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    return super().set_seed(seed, static_seed)",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [
                    "self"
                ],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    return super().state_dict()",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    "self",
                    "state_dict"
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict) -> None:\n    return super().load_state_dict(state_dict)",
                "description": ""
            }
        ]
    }
]
2024-12-03 15:57:48
utils.py: [
    {
        "function_name": "split_trajectories",
        "args": [
            "rollout_tensordict",
            "prefix",
            "trajectory_key",
            "done_key",
            "as_nested"
        ],
        "signature": "split_trajectories(rollout_tensordict: TensorDictBase, *, prefix=None, trajectory_key: NestedKey | None=None, done_key: NestedKey | None=None, as_nested: bool=False) -> TensorDictBase",
        "function_code": "@set_lazy_legacy(False)\ndef split_trajectories(rollout_tensordict: TensorDictBase, *, prefix=None, trajectory_key: NestedKey | None=None, done_key: NestedKey | None=None, as_nested: bool=False) -> TensorDictBase:\n    \"\"\"A util function for trajectory separation.\n\n    Takes a tensordict with a key traj_ids that indicates the id of each trajectory.\n\n    From there, builds a B x T x ... zero-padded tensordict with B batches on max duration T\n\n    Args:\n        rollout_tensordict (TensorDictBase): a rollout with adjacent trajectories\n            along the last dimension.\n\n    Keyword Args:\n        prefix (NestedKey, optional): the prefix used to read and write meta-data,\n            such as ``\"traj_ids\"`` (the optional integer id of each trajectory)\n            and the ``\"mask\"`` entry indicating which data are valid and which\n            aren't. Defaults to ``\"collector\"`` if the input has a ``\"collector\"``\n            entry, ``()`` (no prefix) otherwise.\n            ``prefix`` is kept as a legacy feature and will be deprecated eventually.\n            Prefer ``trajectory_key`` or ``done_key`` whenever possible.\n        trajectory_key (NestedKey, optional): the key pointing to the trajectory\n            ids. Supersedes ``done_key`` and ``prefix``. If not provided, defaults\n            to ``(prefix, \"traj_ids\")``.\n        done_key (NestedKey, optional): the key pointing to the ``\"done\"\"`` signal,\n            if the trajectory could not be directly recovered. Defaults to ``\"done\"``.\n        as_nested (bool or torch.layout, optional): whether to return the results as nested\n            tensors. Defaults to ``False``. If a ``torch.layout`` is provided, it will be used\n            to construct the nested tensor, otherwise the default layout will be used.\n\n            .. note:: Using ``split_trajectories(tensordict, as_nested=True).to_padded_tensor(mask=mask_key)``\n                should result in the exact same result as ``as_nested=False``. Since this is an experimental\n                feature and relies on nested_tensors, which API may change in the future, we made this\n                an optional feature. The runtime should be faster with ``as_nested=True``.\n\n            .. note:: Providing a layout lets the user control whether the nested tensor is to be used\n                with ``torch.strided`` or ``torch.jagged`` layout. While the former has slightly more\n                capabilities at the time of writing, the second will be the main focus of the PyTorch team\n                in the future due to its better compatibility with :func:`~torch.compile`.\n\n    Returns:\n        A new tensordict with a leading dimension corresponding to the trajectory.\n        A ``\"mask\"`` boolean entry sharing the ``trajectory_key`` prefix\n        and the tensordict shape is also added. It indicated the valid elements of the tensordict,\n        as well as a ``\"traj_ids\"`` entry if ``trajectory_key`` could not be found.\n\n    Examples:\n        >>> from tensordict import TensorDict\n        >>> import torch\n        >>> from torchrl.collectors.utils import split_trajectories\n        >>> obs = torch.cat([torch.arange(10), torch.arange(5)])\n        >>> obs_ = torch.cat([torch.arange(1, 11), torch.arange(1, 6)])\n        >>> done = torch.zeros(15, dtype=torch.bool)\n        >>> done[9] = True\n        >>> trajectory_id = torch.cat([torch.zeros(10, dtype=torch.int32),\n        ...     torch.ones(5, dtype=torch.int32)])\n        >>> data = TensorDict({\"obs\": obs, (\"next\", \"obs\"): obs_, (\"next\", \"done\"): done, \"trajectory\": trajectory_id}, batch_size=[15])\n        >>> data_split = split_trajectories(data, done_key=\"done\")\n        >>> print(data_split)\n        TensorDict(\n            fields={\n                mask: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                next: TensorDict(\n                    fields={\n                        done: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                        obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False)},\n                    batch_size=torch.Size([2, 10]),\n                    device=None,\n                    is_shared=False),\n                obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n                traj_ids: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n                trajectory: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int32, is_shared=False)},\n            batch_size=torch.Size([2, 10]),\n            device=None,\n            is_shared=False)\n        >>> # check that split_trajectories got the trajectories right with the done signal\n        >>> assert (data_split[\"traj_ids\"] == data_split[\"trajectory\"]).all()\n        >>> print(data_split[\"mask\"])\n        tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n                [ True,  True,  True,  True,  True, False, False, False, False, False]])\n        >>> data_split = split_trajectories(data, trajectory_key=\"trajectory\")\n        >>> print(data_split)\n        TensorDict(\n            fields={\n                mask: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                next: TensorDict(\n                    fields={\n                        done: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                        obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False)},\n                    batch_size=torch.Size([2, 10]),\n                    device=None,\n                    is_shared=False),\n                obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n                trajectory: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int32, is_shared=False)},\n            batch_size=torch.Size([2, 10]),\n            device=None,\n            is_shared=False)\n\n    \"\"\"\n    mask_key = None\n    if trajectory_key is not None:\n        from torchrl.envs.utils import _replace_last\n        traj_ids_key = trajectory_key\n        mask_key = _replace_last(trajectory_key, 'mask')\n    else:\n        if prefix is None and 'collector' in rollout_tensordict.keys():\n            prefix = 'collector'\n        if prefix is None:\n            traj_ids_key = 'traj_ids'\n            mask_key = 'mask'\n        else:\n            traj_ids_key = (prefix, 'traj_ids')\n            mask_key = (prefix, 'mask')\n    rollout_tensordict = rollout_tensordict.copy()\n    traj_ids = rollout_tensordict.get(traj_ids_key, None)\n    if traj_ids is None:\n        if done_key is None:\n            done_key = 'done'\n        done_key = ('next', done_key)\n        done = rollout_tensordict.get(done_key)\n        idx = (slice(None),) * (rollout_tensordict.ndim - 1) + (slice(None, -1),)\n        done_sel = done[idx]\n        pads = [1, 0]\n        pads = [0, 0] * (done.ndim - rollout_tensordict.ndim) + pads\n        done_sel = torch.nn.functional.pad(done_sel, pads)\n        if done_sel.shape != done.shape:\n            raise RuntimeError(f'done and done_sel have different shape {done.shape} - {done_sel.shape} ')\n        traj_ids = done_sel.cumsum(rollout_tensordict.ndim - 1)\n        traj_ids = traj_ids.squeeze(-1)\n        if rollout_tensordict.ndim > 1:\n            for i in range(1, rollout_tensordict.shape[0]):\n                traj_ids[i] += traj_ids[i - 1].max() + 1\n        rollout_tensordict.set(traj_ids_key, traj_ids)\n    splits = traj_ids.reshape(-1)\n    splits = [(splits == i).sum().item() for i in splits.unique_consecutive()]\n    if len(set(splits)) == 1 and splits[0] == traj_ids.shape[-1]:\n        rollout_tensordict.set(mask_key, torch.ones(rollout_tensordict.shape, device=rollout_tensordict.device, dtype=torch.bool))\n        if rollout_tensordict.ndimension() == 1:\n            rollout_tensordict = rollout_tensordict.unsqueeze(0)\n        return rollout_tensordict\n    out_splits = rollout_tensordict.reshape(-1)\n    if as_nested:\n        if hasattr(torch, '_nested_compute_contiguous_strides_offsets'):\n\n            def nest(x, splits=splits):\n                shape = torch.tensor([[int(split), *x.shape[1:]] for split in splits])\n                return torch._nested_view_from_buffer(x.reshape(-1), shape, *torch._nested_compute_contiguous_strides_offsets(shape))\n            return out_splits._fast_apply(nest, batch_size=[len(splits), -1])\n        else:\n            out_splits = out_splits.split(splits, 0)\n            layout = as_nested if as_nested is not bool else None\n            if torch.__version__ < '2.4':\n                if layout not in (True,):\n                    raise RuntimeError(f'layout={layout} is only available for torch>=v2.4')\n\n                def nest(*x):\n                    return torch.nested.nested_tensor(list(x))\n            else:\n\n                def nest(*x):\n                    return torch.nested.nested_tensor(list(x), layout=layout)\n            return out_splits[0]._fast_apply(nest, *out_splits[1:], batch_size=[len(out_splits), *out_splits[0].batch_size[:-1], -1])\n    out_splits = out_splits.split(splits, 0)\n    for out_split in out_splits:\n        out_split.set(mask_key, torch.ones(out_split.shape, dtype=torch.bool, device=out_split.device))\n    if len(out_splits) > 1:\n        MAX = max(*[out_split.shape[0] for out_split in out_splits])\n    else:\n        MAX = out_splits[0].shape[0]\n    td = torch.stack([pad(out_split, [0, MAX - out_split.shape[0]]) for out_split in out_splits], 0)\n    return td",
        "description": ""
    }
]
2024-12-03 18:11:47
collectors.py: [
    {
        "function_name": "recursive_map_to_cpu",
        "args": [
            {
                "arg_name": "dictionary",
                "return_type": "OrderedDict",
                "default_value": ""
            }
        ],
        "signature": "recursive_map_to_cpu(dictionary: OrderedDict) -> OrderedDict",
        "function_code": "def recursive_map_to_cpu(dictionary: OrderedDict) -> OrderedDict:\n    \"\"\"Maps the tensors to CPU through a nested dictionary.\"\"\"\n    return OrderedDict(**{k: recursive_map_to_cpu(item) if isinstance(item, OrderedDict) else item.cpu() if isinstance(item, torch.Tensor) else item for k, item in dictionary.items()})",
        "description": ""
    },
    {
        "class_name": "DataCollectorBase",
        "bases": [
            "IterableDataset"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "update_policy_weights_",
                "args": [
                    {
                        "arg_name": "policy_weights",
                        "return_type": "Optional[TensorDictBase]",
                        "default_value": "None"
                    }
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    \"\"\"Updates the policy weights if the policy of the data collector and the trained policy live on different devices.\n\n        Args:\n            policy_weights (TensorDictBase, optional): if provided, a TensorDict containing\n                the weights of the policy to be used for the udpdate.\n\n        \"\"\"\n    if policy_weights is not None:\n        self.policy_weights.data.update_(policy_weights)\n    elif self.get_weights_fn is not None:\n        self.policy_weights.data.update_(self.get_weights_fn())",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [
                    "NoArgs"
                ],
                "signature": "next(self)",
                "function_code": "def next(self):\n    try:\n        if self._iterator is None:\n            self._iterator = iter(self)\n        out = next(self._iterator)\n        out.clear_device_()\n        return out\n    except StopIteration:\n        return None",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [
                    "NoArgs"
                ],
                "signature": "shutdown(self)",
                "function_code": "@abc.abstractmethod\ndef shutdown(self):\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [
                    "NoArgs"
                ],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "@abc.abstractmethod\ndef iterator(self) -> Iterator[TensorDictBase]:\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "@abc.abstractmethod\ndef set_seed(self, seed: int, static_seed: bool=False) -> int:\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [
                    "NoArgs"
                ],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "@abc.abstractmethod\ndef state_dict(self) -> OrderedDict:\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "@abc.abstractmethod\ndef load_state_dict(self, state_dict: OrderedDict) -> None:\n    raise NotImplementedError",
                "description": ""
            }
        ]
    },
    {
        "class_name": "SyncDataCollector",
        "bases": [
            "DataCollectorBase"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "__init__",
                "args": [
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool|Dict[str",
                        "default_value": "None"
                    }
                ],
                "signature": "__init__(self, create_env_fn: Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]=None, *, frames_per_batch: int, total_frames: int=-1, device: DEVICE_TYPING=None, storing_device: DEVICE_TYPING=None, policy_device: DEVICE_TYPING=None, env_device: DEVICE_TYPING=None, create_env_kwargs: dict | None=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Callable[[TensorDictBase], TensorDictBase] | None=None, split_trajs: bool | None=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, return_same_td: bool=False, reset_when_done: bool=True, interruptor=None, set_truncated: bool=False, use_buffers: bool | None=None, replay_buffer: ReplayBuffer | None=None, trust_policy: bool=None, compile_policy: bool | Dict[str, Any] | None=None, cudagraph_policy: bool | Dict[str, Any] | None=None, **kwargs)",
                "function_code": "def __init__(self, create_env_fn: Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]=None, *, frames_per_batch: int, total_frames: int=-1, device: DEVICE_TYPING=None, storing_device: DEVICE_TYPING=None, policy_device: DEVICE_TYPING=None, env_device: DEVICE_TYPING=None, create_env_kwargs: dict | None=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Callable[[TensorDictBase], TensorDictBase] | None=None, split_trajs: bool | None=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, return_same_td: bool=False, reset_when_done: bool=True, interruptor=None, set_truncated: bool=False, use_buffers: bool | None=None, replay_buffer: ReplayBuffer | None=None, trust_policy: bool=None, compile_policy: bool | Dict[str, Any] | None=None, cudagraph_policy: bool | Dict[str, Any] | None=None, **kwargs):\n    from torchrl.envs.batched_envs import BatchedEnvBase\n    self.closed = True\n    if create_env_kwargs is None:\n        create_env_kwargs = {}\n    if not isinstance(create_env_fn, EnvBase):\n        env = create_env_fn(**create_env_kwargs)\n    else:\n        env = create_env_fn\n        if create_env_kwargs:\n            if not isinstance(env, BatchedEnvBase):\n                raise RuntimeError(f\"kwargs were passed to SyncDataCollector but they can't be set on environment of type {type(create_env_fn)}.\")\n            env.update_kwargs(create_env_kwargs)\n    if policy is None:\n        policy = RandomPolicy(env.full_action_spec)\n    if trust_policy is None:\n        trust_policy = isinstance(policy, (RandomPolicy, CudaGraphModule))\n    self.trust_policy = trust_policy\n    self._read_compile_kwargs(compile_policy, cudagraph_policy)\n    self._traj_pool_val = kwargs.pop('traj_pool', None)\n    if kwargs:\n        raise TypeError(f'Keys {list(kwargs.keys())} are unknown to {type(self).__name__}.')\n    storing_device, policy_device, env_device = self._get_devices(storing_device=storing_device, policy_device=policy_device, env_device=env_device, device=device)\n    self.storing_device = storing_device\n    if self.storing_device is not None and self.storing_device.type != 'cuda':\n        if torch.cuda.is_available():\n            self._sync_storage = torch.cuda.synchronize\n        elif torch.backends.mps.is_available() and hasattr(torch, 'mps'):\n            self._sync_storage = torch.mps.synchronize\n        elif self.storing_device.type == 'cpu':\n            self._sync_storage = _do_nothing\n        else:\n            raise RuntimeError('Non supported device')\n    else:\n        self._sync_storage = _do_nothing\n    self.env_device = env_device\n    if self.env_device is not None and self.env_device.type != 'cuda':\n        if torch.cuda.is_available():\n            self._sync_env = torch.cuda.synchronize\n        elif torch.backends.mps.is_available() and hasattr(torch, 'mps'):\n            self._sync_env = torch.mps.synchronize\n        elif self.env_device.type == 'cpu':\n            self._sync_env = _do_nothing\n        else:\n            raise RuntimeError('Non supported device')\n    else:\n        self._sync_env = _do_nothing\n    self.policy_device = policy_device\n    if self.policy_device is not None and self.policy_device.type != 'cuda':\n        if torch.cuda.is_available():\n            self._sync_policy = torch.cuda.synchronize\n        elif torch.backends.mps.is_available() and hasattr(torch, 'mps'):\n            self._sync_policy = torch.mps.synchronize\n        elif self.policy_device.type == 'cpu':\n            self._sync_policy = _do_nothing\n        else:\n            raise RuntimeError('Non supported device')\n    else:\n        self._sync_policy = _do_nothing\n    self.device = device\n    self._cast_to_policy_device = self.policy_device != self.env_device\n    self.env: EnvBase = env\n    del env\n    self.replay_buffer = replay_buffer\n    if self.replay_buffer is not None:\n        if postproc is not None:\n            raise TypeError('postproc must be None when a replay buffer is passed.')\n        if use_buffers:\n            raise TypeError('replay_buffer is exclusive with use_buffers.')\n    if use_buffers is None:\n        use_buffers = not self.env._has_dynamic_specs and self.replay_buffer is None\n    self._use_buffers = use_buffers\n    self.replay_buffer = replay_buffer\n    self.closed = False\n    if not reset_when_done:\n        raise ValueError('reset_when_done is deprectated.')\n    self.reset_when_done = reset_when_done\n    self.n_env = self.env.batch_size.numel()\n    self.policy, self.get_weights_fn = self._get_policy_and_device(policy=policy, observation_spec=self.env.observation_spec)\n    if isinstance(self.policy, nn.Module):\n        self.policy_weights = TensorDict.from_module(self.policy, as_module=True)\n    else:\n        self.policy_weights = TensorDict()\n    if self.compiled_policy:\n        self.policy = torch.compile(self.policy, **self.compiled_policy_kwargs)\n    if self.cudagraphed_policy:\n        self.policy = CudaGraphModule(self.policy, **self.cudagraphed_policy_kwargs)\n    if self.env_device:\n        self.env: EnvBase = self.env.to(self.env_device)\n    elif self.env.device is not None:\n        self.env_device = self.env.device\n    self._cast_to_env_device = self._cast_to_policy_device or self.env.device != self.storing_device\n    self.max_frames_per_traj = int(max_frames_per_traj) if max_frames_per_traj is not None else 0\n    if self.max_frames_per_traj is not None and self.max_frames_per_traj > 0:\n        for key in self.env.output_spec.keys(True, True):\n            if isinstance(key, str):\n                key = (key,)\n            if 'step_count' in key:\n                raise ValueError(\"A 'step_count' key is already present in the environment and the 'max_frames_per_traj' argument may conflict with a 'StepCounter' that has already been set. Possible solutions: Set max_frames_per_traj to 0 or remove the StepCounter limit from the environment transforms.\")\n        self.env = TransformedEnv(self.env, StepCounter(max_steps=self.max_frames_per_traj))\n    if total_frames is None or total_frames < 0:\n        total_frames = float('inf')\n    else:\n        remainder = total_frames % frames_per_batch\n        if remainder != 0 and RL_WARNINGS:\n            warnings.warn(f'total_frames ({total_frames}) is not exactly divisible by frames_per_batch ({frames_per_batch}).This means {frames_per_batch - remainder} additional frames will be collected.To silence this message, set the environment variable RL_WARNINGS to False.')\n    self.total_frames = int(total_frames) if total_frames != float('inf') else total_frames\n    self.reset_at_each_iter = reset_at_each_iter\n    self.init_random_frames = int(init_random_frames) if init_random_frames is not None else 0\n    if init_random_frames is not None and init_random_frames % frames_per_batch != 0 and RL_WARNINGS:\n        warnings.warn(f'init_random_frames ({init_random_frames}) is not exactly a multiple of frames_per_batch ({frames_per_batch}),  this results in more init_random_frames than requested ({-(-init_random_frames // frames_per_batch) * frames_per_batch}).To silence this message, set the environment variable RL_WARNINGS to False.')\n    self.postproc = postproc\n    if self.postproc is not None and hasattr(self.postproc, 'to') and self.storing_device:\n        self.postproc.to(self.storing_device)\n    if frames_per_batch % self.n_env != 0 and RL_WARNINGS:\n        warnings.warn(f'frames_per_batch ({frames_per_batch}) is not exactly divisible by the number of batched environments ({self.n_env}),  this results in more frames_per_batch per iteration that requested ({-(-frames_per_batch // self.n_env) * self.n_env}).To silence this message, set the environment variable RL_WARNINGS to False.')\n    self.requested_frames_per_batch = int(frames_per_batch)\n    self.frames_per_batch = -(-frames_per_batch // self.n_env)\n    self.exploration_type = exploration_type if exploration_type else DEFAULT_EXPLORATION_TYPE\n    self.return_same_td = return_same_td\n    self.set_truncated = set_truncated\n    self._make_shuttle()\n    if self._use_buffers:\n        self._make_final_rollout()\n    self._set_truncated_keys()\n    if split_trajs is None:\n        split_trajs = False\n    self.split_trajs = split_trajs\n    self._exclude_private_keys = True\n    self.interruptor = interruptor\n    self._frames = 0\n    self._iter = -1",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [
                    "NoArgs"
                ],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "update_policy_weights_",
                "args": [
                    {
                        "arg_name": "policy_weights",
                        "return_type": "Optional[TensorDictBase]",
                        "default_value": "None"
                    }
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    super().update_policy_weights_(policy_weights)",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    \"\"\"Sets the seeds of the environments stored in the DataCollector.\n\n        Args:\n            seed (int): integer representing the seed to be used for the environment.\n            static_seed(bool, optional): if ``True``, the seed is not incremented.\n                Defaults to False\n\n        Returns:\n            Output seed. This is useful when more than one environment is contained in the DataCollector, as the\n            seed will be incremented for each of these. The resulting seed is the seed of the last environment.\n\n        Examples:\n            >>> from torchrl.envs import ParallelEnv\n            >>> from torchrl.envs.libs.gym import GymEnv\n            >>> from tensordict.nn import TensorDictModule\n            >>> from torch import nn\n            >>> env_fn = lambda: GymEnv(\"Pendulum-v1\")\n            >>> env_fn_parallel = ParallelEnv(6, env_fn)\n            >>> policy = TensorDictModule(nn.Linear(3, 1), in_keys=[\"observation\"], out_keys=[\"action\"])\n            >>> collector = SyncDataCollector(env_fn_parallel, policy, total_frames=300, frames_per_batch=100)\n            >>> out_seed = collector.set_seed(1)  # out_seed = 6\n\n        \"\"\"\n    out = self.env.set_seed(seed, static_seed=static_seed)\n    return out",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [
                    "NoArgs"
                ],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "def iterator(self) -> Iterator[TensorDictBase]:\n    \"\"\"Iterates through the DataCollector.\n\n        Yields: TensorDictBase objects containing (chunks of) trajectories\n\n        \"\"\"\n    if self.storing_device and self.storing_device.type == 'cuda':\n        stream = torch.cuda.Stream(self.storing_device, priority=-1)\n        event = stream.record_event()\n        streams = [stream]\n        events = [event]\n    elif self.storing_device is None:\n        streams = []\n        events = []\n        cuda_devices = set()\n\n        def cuda_check(tensor: torch.Tensor):\n            if tensor.is_cuda:\n                cuda_devices.add(tensor.device)\n        if not self._use_buffers:\n            for spec in self.env.specs.values(True, True):\n                if spec.device.type == 'cuda':\n                    if ':' not in str(spec.device):\n                        raise RuntimeError(\"A cuda spec did not have a device associated. Make sure to pass `'cuda:device_num'` to each spec device.\")\n                    cuda_devices.add(spec.device)\n        else:\n            self._final_rollout.apply(cuda_check, filter_empty=True)\n        for device in cuda_devices:\n            streams.append(torch.cuda.Stream(device, priority=-1))\n            events.append(streams[-1].record_event())\n    else:\n        streams = []\n        events = []\n    with contextlib.ExitStack() as stack:\n        for stream in streams:\n            stack.enter_context(torch.cuda.stream(stream))\n        while self._frames < self.total_frames:\n            self._iter += 1\n            tensordict_out = self.rollout()\n            if tensordict_out is None:\n                yield\n                continue\n            self._increment_frames(tensordict_out.numel())\n            if self.split_trajs:\n                tensordict_out = split_trajectories(tensordict_out, prefix='collector')\n            if self.postproc is not None:\n                tensordict_out = self.postproc(tensordict_out)\n            if self._exclude_private_keys:\n\n                def is_private(key):\n                    if isinstance(key, str) and key.startswith('_'):\n                        return True\n                    if isinstance(key, tuple) and any((_key.startswith('_') for _key in key)):\n                        return True\n                    return False\n                excluded_keys = [key for key in tensordict_out.keys(True) if is_private(key)]\n                tensordict_out = tensordict_out.exclude(*excluded_keys, inplace=True)\n            if self.return_same_td:\n                if events:\n                    for event in events:\n                        event.record()\n                        event.synchronize()\n                yield tensordict_out\n            else:\n                yield tensordict_out.clone()",
                "description": ""
            },
            {
                "function_name": "rollout",
                "args": [
                    "NoArgs"
                ],
                "signature": "rollout(self) -> TensorDictBase",
                "function_code": "@torch.no_grad()\ndef rollout(self) -> TensorDictBase:\n    \"\"\"Computes a rollout in the environment using the provided policy.\n\n        Returns:\n            TensorDictBase containing the computed rollout.\n\n        \"\"\"\n    if self.reset_at_each_iter:\n        self._shuttle.update(self.env.reset())\n    if self._use_buffers:\n        self._final_rollout.fill_(('collector', 'traj_ids'), -1)\n    else:\n        pass\n    tensordicts = []\n    with set_exploration_type(self.exploration_type):\n        for t in range(self.frames_per_batch):\n            if self.init_random_frames is not None and self._frames < self.init_random_frames:\n                self.env.rand_action(self._shuttle)\n            else:\n                if self._cast_to_policy_device:\n                    if self.policy_device is not None:\n                        policy_input = self._shuttle.to(self.policy_device, non_blocking=True)\n                        self._sync_policy()\n                    elif self.policy_device is None:\n                        policy_input = self._shuttle\n                else:\n                    policy_input = self._shuttle\n                policy_output = self.policy(policy_input)\n                if self._shuttle is not policy_output:\n                    self._shuttle.update(policy_output, keys_to_update=self._policy_output_keys)\n            if self._cast_to_env_device:\n                if self.env_device is not None:\n                    env_input = self._shuttle.to(self.env_device, non_blocking=True)\n                    self._sync_env()\n                elif self.env_device is None:\n                    env_input = self._shuttle\n            else:\n                env_input = self._shuttle\n            env_output, env_next_output = self.env.step_and_maybe_reset(env_input)\n            if self._shuttle is not env_output:\n                next_data = env_output.get('next')\n                if self._shuttle_has_no_device:\n                    next_data.clear_device_()\n                self._shuttle.set('next', next_data)\n            if self.replay_buffer is not None:\n                self.replay_buffer.add(self._shuttle)\n                if self._increment_frames(self._shuttle.numel()):\n                    return\n            elif self.storing_device is not None:\n                tensordicts.append(self._shuttle.to(self.storing_device, non_blocking=True))\n                self._sync_storage()\n            else:\n                tensordicts.append(self._shuttle)\n            collector_data = self._shuttle.get('collector').copy()\n            self._shuttle = env_next_output\n            if self._shuttle_has_no_device:\n                self._shuttle.clear_device_()\n            self._shuttle.set('collector', collector_data)\n            self._update_traj_ids(env_output)\n            if self.interruptor is not None and self.interruptor.collection_stopped():\n                if self.replay_buffer is not None:\n                    return\n                result = self._final_rollout\n                if self._use_buffers:\n                    try:\n                        torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout[..., :t + 1])\n                    except RuntimeError:\n                        with self._final_rollout.unlock_():\n                            torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout[..., :t + 1])\n                else:\n                    result = TensorDict.maybe_dense_stack(tensordicts, dim=-1)\n                break\n        else:\n            if self._use_buffers:\n                result = self._final_rollout\n                try:\n                    result = torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout)\n                except RuntimeError:\n                    with self._final_rollout.unlock_():\n                        result = torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout)\n            elif self.replay_buffer is not None:\n                return\n            else:\n                result = TensorDict.maybe_dense_stack(tensordicts, dim=-1)\n                result.refine_names(..., 'time')\n    return self._maybe_set_truncated(result)",
                "description": ""
            },
            {
                "function_name": "reset",
                "args": [
                    {
                        "arg_name": "**kwargs",
                        "return_type": "",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "",
                        "default_value": "None"
                    }
                ],
                "signature": "reset(self, index=None, **kwargs) -> None",
                "function_code": "@torch.no_grad()\ndef reset(self, index=None, **kwargs) -> None:\n    \"\"\"Resets the environments to a new initial state.\"\"\"\n    collector_metadata = self._shuttle.get('collector').clone()\n    if index is not None:\n        if prod(self.env.batch_size) == 0:\n            raise RuntimeError('resetting unique env with index is not permitted.')\n        for reset_key, done_keys in zip(self.env.reset_keys, self.env.done_keys_groups):\n            _reset = torch.zeros(self.env.full_done_spec[done_keys[0]].shape, dtype=torch.bool, device=self.env.device)\n            _reset[index] = 1\n            self._shuttle.set(reset_key, _reset)\n    else:\n        _reset = None\n        self._shuttle.zero_()\n    self._shuttle.update(self.env.reset(**kwargs), inplace=True)\n    collector_metadata['traj_ids'] = collector_metadata['traj_ids'] - collector_metadata['traj_ids'].min()\n    self._shuttle['collector'] = collector_metadata",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [
                    "NoArgs"
                ],
                "signature": "shutdown(self) -> None",
                "function_code": "def shutdown(self) -> None:\n    \"\"\"Shuts down all workers and/or closes the local environment.\"\"\"\n    if not self.closed:\n        self.closed = True\n        del self._shuttle\n        if self._use_buffers:\n            del self._final_rollout\n        if not self.env.is_closed:\n            self.env.close()\n        del self.env\n    return",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [
                    "NoArgs"
                ],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    \"\"\"Returns the local state_dict of the data collector (environment and policy).\n\n        Returns:\n            an ordered dictionary with fields :obj:`\"policy_state_dict\"` and\n            `\"env_state_dict\"`.\n\n        \"\"\"\n    from torchrl.envs.batched_envs import BatchedEnvBase\n    if isinstance(self.env, TransformedEnv):\n        env_state_dict = self.env.transform.state_dict()\n    elif isinstance(self.env, BatchedEnvBase):\n        env_state_dict = self.env.state_dict()\n    else:\n        env_state_dict = OrderedDict()\n    if hasattr(self.policy, 'state_dict'):\n        policy_state_dict = self.policy.state_dict()\n        state_dict = OrderedDict(policy_state_dict=policy_state_dict, env_state_dict=env_state_dict)\n    else:\n        state_dict = OrderedDict(env_state_dict=env_state_dict)\n    state_dict.update({'frames': self._frames, 'iter': self._iter})\n    return state_dict",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "**kwargs",
                        "return_type": "OrderedDict",
                        "default_value": ""
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "OrderedDict",
                        "default_value": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict, **kwargs) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict, **kwargs) -> None:\n    \"\"\"Loads a state_dict on the environment and policy.\n\n        Args:\n            state_dict (OrderedDict): ordered dictionary containing the fields\n                `\"policy_state_dict\"` and :obj:`\"env_state_dict\"`.\n\n        \"\"\"\n    strict = kwargs.get('strict', True)\n    if strict or 'env_state_dict' in state_dict:\n        self.env.load_state_dict(state_dict['env_state_dict'], **kwargs)\n    if strict or 'policy_state_dict' in state_dict:\n        self.policy.load_state_dict(state_dict['policy_state_dict'], **kwargs)\n    self._frames = state_dict['frames']\n    self._iter = state_dict['iter']",
                "description": ""
            }
        ]
    },
    {
        "class_name": "MultiSyncDataCollector",
        "bases": [
            "_MultiDataCollector"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "next",
                "args": [
                    "NoArgs"
                ],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [
                    "NoArgs"
                ],
                "signature": "shutdown(self)",
                "function_code": "def shutdown(self):\n    if hasattr(self, 'out_buffer'):\n        del self.out_buffer\n    if hasattr(self, 'buffers'):\n        del self.buffers\n    return super().shutdown()",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    return super().set_seed(seed, static_seed)",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [
                    "NoArgs"
                ],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    return super().state_dict()",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict) -> None:\n    return super().load_state_dict(state_dict)",
                "description": ""
            },
            {
                "function_name": "update_policy_weights_",
                "args": [
                    {
                        "arg_name": "policy_weights",
                        "return_type": "Optional[TensorDictBase]",
                        "default_value": "None"
                    }
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    super().update_policy_weights_(policy_weights)",
                "description": ""
            },
            {
                "function_name": "frames_per_batch_worker",
                "args": [
                    "NoArgs"
                ],
                "signature": "frames_per_batch_worker(self)",
                "function_code": "@property\ndef frames_per_batch_worker(self):\n    if self.requested_frames_per_batch % self.num_workers != 0 and RL_WARNINGS:\n        warnings.warn(f'frames_per_batch {self.requested_frames_per_batch} is not exactly divisible by the number of collector workers {self.num_workers}, this results in more frames_per_batch per iteration that requested.To silence this message, set the environment variable RL_WARNINGS to False.')\n    frames_per_batch_worker = -(-self.requested_frames_per_batch // self.num_workers)\n    return frames_per_batch_worker",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [
                    "NoArgs"
                ],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "def iterator(self) -> Iterator[TensorDictBase]:\n    cat_results = self.cat_results\n    if cat_results is None:\n        cat_results = 'stack'\n    self.buffers = {}\n    dones = [False for _ in range(self.num_workers)]\n    workers_frames = [0 for _ in range(self.num_workers)]\n    same_device = None\n    self.out_buffer = None\n    preempt = self.interruptor is not None and self.preemptive_threshold < 1.0\n    while not all(dones) and self._frames < self.total_frames:\n        _check_for_faulty_process(self.procs)\n        if self.update_at_each_batch:\n            self.update_policy_weights_()\n        for idx in range(self.num_workers):\n            if self.init_random_frames is not None and self._frames < self.init_random_frames:\n                msg = 'continue_random'\n            else:\n                msg = 'continue'\n            self.pipes[idx].send((None, msg))\n        self._iter += 1\n        if preempt:\n            self.interruptor.start_collection()\n            while self.queue_out.qsize() < int(self.num_workers * self.preemptive_threshold):\n                continue\n            self.interruptor.stop_collection()\n            while self.queue_out.qsize() < int(self.num_workers):\n                continue\n        for _ in range(self.num_workers):\n            new_data, j = self.queue_out.get()\n            use_buffers = self._use_buffers\n            if self.replay_buffer is not None:\n                idx = new_data\n                workers_frames[idx] = workers_frames[idx] + self.frames_per_batch_worker\n                continue\n            elif j == 0 or not use_buffers:\n                try:\n                    data, idx = new_data\n                    self.buffers[idx] = data\n                    if use_buffers is None and j > 0:\n                        self._use_buffers = False\n                except TypeError:\n                    if use_buffers is None:\n                        self._use_buffers = True\n                        idx = new_data\n                    else:\n                        raise\n            else:\n                idx = new_data\n            if preempt:\n                if cat_results != 'stack':\n                    buffers = {}\n                    for idx, buffer in self.buffers.items():\n                        valid = buffer.get(('collector', 'traj_ids')) != -1\n                        if valid.ndim > 2:\n                            valid = valid.flatten(0, -2)\n                        if valid.ndim == 2:\n                            valid = valid.any(0)\n                        buffers[idx] = buffer[..., valid]\n                else:\n                    for buffer in self.buffers.values():\n                        with buffer.unlock_():\n                            buffer.set(('collector', 'mask'), buffer.get(('collector', 'traj_ids')) != -1)\n                    buffers = self.buffers\n            else:\n                buffers = self.buffers\n            workers_frames[idx] = workers_frames[idx] + buffers[idx].numel()\n            if workers_frames[idx] >= self.total_frames:\n                dones[idx] = True\n        if self.replay_buffer is not None:\n            yield\n            self._frames += self.frames_per_batch_worker * self.num_workers\n            continue\n        n_collected = 0\n        for idx in range(self.num_workers):\n            buffer = buffers[idx]\n            traj_ids = buffer.get(('collector', 'traj_ids'))\n            if preempt:\n                if cat_results == 'stack':\n                    mask_frames = buffer.get(('collector', 'traj_ids')) != -1\n                    n_collected += mask_frames.sum().cpu()\n                else:\n                    n_collected += traj_ids.numel()\n            else:\n                n_collected += traj_ids.numel()\n        if same_device is None:\n            prev_device = None\n            same_device = True\n            for item in self.buffers.values():\n                if prev_device is None:\n                    prev_device = item.device\n                else:\n                    same_device = same_device and item.device == prev_device\n        if cat_results == 'stack':\n            stack = torch.stack if self._use_buffers else TensorDict.maybe_dense_stack\n            if same_device:\n                self.out_buffer = stack(list(buffers.values()), 0)\n            else:\n                self.out_buffer = stack([item.cpu() for item in buffers.values()], 0)\n        else:\n            if self._use_buffers is None:\n                torchrl_logger.warning('use_buffer not specified and not yet inferred from data, assuming `True`.')\n            elif not self._use_buffers:\n                raise RuntimeError('Cannot concatenate results with use_buffers=False')\n            try:\n                if same_device:\n                    self.out_buffer = torch.cat(list(buffers.values()), cat_results)\n                else:\n                    self.out_buffer = torch.cat([item.cpu() for item in buffers.values()], cat_results)\n            except RuntimeError as err:\n                if preempt and cat_results != -1 and ('Sizes of tensors must match' in str(err)):\n                    raise RuntimeError(\"The value provided to cat_results isn't compatible with the collectors outputs. Consider using `cat_results=-1`.\")\n                raise\n        if self.split_trajs:\n            out = split_trajectories(self.out_buffer, prefix='collector')\n        else:\n            out = self.out_buffer\n        if cat_results in (-1, 'stack'):\n            out.refine_names(*[None] * (out.ndim - 1) + ['time'])\n        self._frames += n_collected\n        if self.postprocs:\n            self.postprocs = self.postprocs.to(out.device)\n            out = self.postprocs(out)\n        if self._exclude_private_keys:\n            excluded_keys = [key for key in out.keys() if key.startswith('_')]\n            if excluded_keys:\n                out = out.exclude(*excluded_keys)\n        yield out\n        del out\n    del self.buffers\n    self.out_buffer = None",
                "description": ""
            }
        ]
    },
    {
        "class_name": "MultiaSyncDataCollector",
        "bases": [
            "_MultiDataCollector"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "__init__",
                "args": [
                    {
                        "arg_name": "**kwargs",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "",
                        "default_value": ""
                    }
                ],
                "signature": "__init__(self, *args, **kwargs)",
                "function_code": "def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.out_tensordicts = defaultdict(lambda: None)\n    self.running = False\n    if self.postprocs is not None:\n        postproc = self.postprocs\n        self.postprocs = {}\n        for _device in self.storing_device:\n            if _device not in self.postprocs:\n                self.postprocs[_device] = deepcopy(postproc).to(_device)",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [
                    "NoArgs"
                ],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [
                    "NoArgs"
                ],
                "signature": "shutdown(self)",
                "function_code": "def shutdown(self):\n    if hasattr(self, 'out_tensordicts'):\n        del self.out_tensordicts\n    return super().shutdown()",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    return super().set_seed(seed, static_seed)",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [
                    "NoArgs"
                ],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    return super().state_dict()",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict) -> None:\n    return super().load_state_dict(state_dict)",
                "description": ""
            },
            {
                "function_name": "update_policy_weights_",
                "args": [
                    {
                        "arg_name": "policy_weights",
                        "return_type": "Optional[TensorDictBase]",
                        "default_value": "None"
                    }
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    super().update_policy_weights_(policy_weights)",
                "description": ""
            },
            {
                "function_name": "frames_per_batch_worker",
                "args": [
                    "NoArgs"
                ],
                "signature": "frames_per_batch_worker(self)",
                "function_code": "@property\ndef frames_per_batch_worker(self):\n    return self.requested_frames_per_batch",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [
                    "NoArgs"
                ],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "def iterator(self) -> Iterator[TensorDictBase]:\n    if self.update_at_each_batch:\n        self.update_policy_weights_()\n    for i in range(self.num_workers):\n        if self.init_random_frames is not None and self.init_random_frames > 0:\n            self.pipes[i].send((None, 'continue_random'))\n        else:\n            self.pipes[i].send((None, 'continue'))\n    self.running = True\n    workers_frames = [0 for _ in range(self.num_workers)]\n    while self._frames < self.total_frames:\n        self._iter += 1\n        while True:\n            try:\n                idx, j, out = self._get_from_queue(timeout=10.0)\n                break\n            except TimeoutError:\n                _check_for_faulty_process(self.procs)\n        if self.replay_buffer is None:\n            worker_frames = out.numel()\n            if self.split_trajs:\n                out = split_trajectories(out, prefix='collector')\n        else:\n            worker_frames = self.frames_per_batch_worker\n        self._frames += worker_frames\n        workers_frames[idx] = workers_frames[idx] + worker_frames\n        if self.postprocs:\n            out = self.postprocs[out.device](out)\n        if self.init_random_frames is not None and self._frames < self.init_random_frames:\n            msg = 'continue_random'\n        else:\n            msg = 'continue'\n        self.pipes[idx].send((idx, msg))\n        if out is not None and self._exclude_private_keys:\n            excluded_keys = [key for key in out.keys() if key.startswith('_')]\n            out = out.exclude(*excluded_keys)\n        yield out\n    self.running = False",
                "description": ""
            },
            {
                "function_name": "reset",
                "args": [
                    {
                        "arg_name": "reset_idx",
                        "return_type": "Optional[Sequence[bool]]",
                        "default_value": "None"
                    }
                ],
                "signature": "reset(self, reset_idx: Optional[Sequence[bool]]=None) -> None",
                "function_code": "def reset(self, reset_idx: Optional[Sequence[bool]]=None) -> None:\n    super().reset(reset_idx)\n    if self.queue_out.full():\n        time.sleep(_TIMEOUT)\n    if self.queue_out.full():\n        raise Exception('self.queue_out is full')\n    if self.running:\n        for idx in range(self.num_workers):\n            if self.init_random_frames is not None and self._frames < self.init_random_frames:\n                self.pipes[idx].send((idx, 'continue_random'))\n            else:\n                self.pipes[idx].send((idx, 'continue'))",
                "description": ""
            }
        ]
    },
    {
        "class_name": "aSyncDataCollector",
        "bases": [
            "MultiaSyncDataCollector"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "__init__",
                "args": [
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "**kwargs",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "__init__(self, create_env_fn: Callable[[], EnvBase], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]], *, frames_per_batch: int, total_frames: Optional[int]=-1, device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, storing_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, env_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, policy_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, create_env_kwargs: Optional[Sequence[dict]]=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Optional[Callable[[TensorDictBase], TensorDictBase]]=None, split_trajs: Optional[bool]=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, reset_when_done: bool=True, update_at_each_batch: bool=False, preemptive_threshold: float=None, num_threads: int=None, num_sub_threads: int=1, set_truncated: bool=False, **kwargs)",
                "function_code": "def __init__(self, create_env_fn: Callable[[], EnvBase], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]], *, frames_per_batch: int, total_frames: Optional[int]=-1, device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, storing_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, env_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, policy_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, create_env_kwargs: Optional[Sequence[dict]]=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Optional[Callable[[TensorDictBase], TensorDictBase]]=None, split_trajs: Optional[bool]=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, reset_when_done: bool=True, update_at_each_batch: bool=False, preemptive_threshold: float=None, num_threads: int=None, num_sub_threads: int=1, set_truncated: bool=False, **kwargs):\n    super().__init__(create_env_fn=[create_env_fn], policy=policy, total_frames=total_frames, create_env_kwargs=[create_env_kwargs], max_frames_per_traj=max_frames_per_traj, frames_per_batch=frames_per_batch, reset_at_each_iter=reset_at_each_iter, init_random_frames=init_random_frames, postproc=postproc, split_trajs=split_trajs, device=device, policy_device=policy_device, env_device=env_device, storing_device=storing_device, exploration_type=exploration_type, reset_when_done=reset_when_done, update_at_each_batch=update_at_each_batch, preemptive_threshold=preemptive_threshold, num_threads=num_threads, num_sub_threads=num_sub_threads, set_truncated=set_truncated)",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [
                    "NoArgs"
                ],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [
                    "NoArgs"
                ],
                "signature": "shutdown(self)",
                "function_code": "def shutdown(self):\n    return super().shutdown()",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    return super().set_seed(seed, static_seed)",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [
                    "NoArgs"
                ],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    return super().state_dict()",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict) -> None:\n    return super().load_state_dict(state_dict)",
                "description": ""
            }
        ]
    }
]
2024-12-03 18:11:47
utils.py: [
    {
        "function_name": "split_trajectories",
        "args": [
            {
                "arg_name": "as_nested",
                "return_type": "bool",
                "default_value": "False"
            },
            {
                "arg_name": "as_nested",
                "return_type": "bool",
                "default_value": "False"
            },
            {
                "arg_name": "as_nested",
                "return_type": "bool",
                "default_value": "False"
            },
            {
                "arg_name": "as_nested",
                "return_type": "bool",
                "default_value": "False"
            },
            {
                "arg_name": "as_nested",
                "return_type": "bool",
                "default_value": "False"
            },
            {
                "arg_name": "as_nested",
                "return_type": "bool",
                "default_value": "False"
            }
        ],
        "signature": "split_trajectories(rollout_tensordict: TensorDictBase, *, prefix=None, trajectory_key: NestedKey | None=None, done_key: NestedKey | None=None, as_nested: bool=False) -> TensorDictBase",
        "function_code": "@set_lazy_legacy(False)\ndef split_trajectories(rollout_tensordict: TensorDictBase, *, prefix=None, trajectory_key: NestedKey | None=None, done_key: NestedKey | None=None, as_nested: bool=False) -> TensorDictBase:\n    \"\"\"A util function for trajectory separation.\n\n    Takes a tensordict with a key traj_ids that indicates the id of each trajectory.\n\n    From there, builds a B x T x ... zero-padded tensordict with B batches on max duration T\n\n    Args:\n        rollout_tensordict (TensorDictBase): a rollout with adjacent trajectories\n            along the last dimension.\n\n    Keyword Args:\n        prefix (NestedKey, optional): the prefix used to read and write meta-data,\n            such as ``\"traj_ids\"`` (the optional integer id of each trajectory)\n            and the ``\"mask\"`` entry indicating which data are valid and which\n            aren't. Defaults to ``\"collector\"`` if the input has a ``\"collector\"``\n            entry, ``()`` (no prefix) otherwise.\n            ``prefix`` is kept as a legacy feature and will be deprecated eventually.\n            Prefer ``trajectory_key`` or ``done_key`` whenever possible.\n        trajectory_key (NestedKey, optional): the key pointing to the trajectory\n            ids. Supersedes ``done_key`` and ``prefix``. If not provided, defaults\n            to ``(prefix, \"traj_ids\")``.\n        done_key (NestedKey, optional): the key pointing to the ``\"done\"\"`` signal,\n            if the trajectory could not be directly recovered. Defaults to ``\"done\"``.\n        as_nested (bool or torch.layout, optional): whether to return the results as nested\n            tensors. Defaults to ``False``. If a ``torch.layout`` is provided, it will be used\n            to construct the nested tensor, otherwise the default layout will be used.\n\n            .. note:: Using ``split_trajectories(tensordict, as_nested=True).to_padded_tensor(mask=mask_key)``\n                should result in the exact same result as ``as_nested=False``. Since this is an experimental\n                feature and relies on nested_tensors, which API may change in the future, we made this\n                an optional feature. The runtime should be faster with ``as_nested=True``.\n\n            .. note:: Providing a layout lets the user control whether the nested tensor is to be used\n                with ``torch.strided`` or ``torch.jagged`` layout. While the former has slightly more\n                capabilities at the time of writing, the second will be the main focus of the PyTorch team\n                in the future due to its better compatibility with :func:`~torch.compile`.\n\n    Returns:\n        A new tensordict with a leading dimension corresponding to the trajectory.\n        A ``\"mask\"`` boolean entry sharing the ``trajectory_key`` prefix\n        and the tensordict shape is also added. It indicated the valid elements of the tensordict,\n        as well as a ``\"traj_ids\"`` entry if ``trajectory_key`` could not be found.\n\n    Examples:\n        >>> from tensordict import TensorDict\n        >>> import torch\n        >>> from torchrl.collectors.utils import split_trajectories\n        >>> obs = torch.cat([torch.arange(10), torch.arange(5)])\n        >>> obs_ = torch.cat([torch.arange(1, 11), torch.arange(1, 6)])\n        >>> done = torch.zeros(15, dtype=torch.bool)\n        >>> done[9] = True\n        >>> trajectory_id = torch.cat([torch.zeros(10, dtype=torch.int32),\n        ...     torch.ones(5, dtype=torch.int32)])\n        >>> data = TensorDict({\"obs\": obs, (\"next\", \"obs\"): obs_, (\"next\", \"done\"): done, \"trajectory\": trajectory_id}, batch_size=[15])\n        >>> data_split = split_trajectories(data, done_key=\"done\")\n        >>> print(data_split)\n        TensorDict(\n            fields={\n                mask: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                next: TensorDict(\n                    fields={\n                        done: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                        obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False)},\n                    batch_size=torch.Size([2, 10]),\n                    device=None,\n                    is_shared=False),\n                obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n                traj_ids: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n                trajectory: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int32, is_shared=False)},\n            batch_size=torch.Size([2, 10]),\n            device=None,\n            is_shared=False)\n        >>> # check that split_trajectories got the trajectories right with the done signal\n        >>> assert (data_split[\"traj_ids\"] == data_split[\"trajectory\"]).all()\n        >>> print(data_split[\"mask\"])\n        tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n                [ True,  True,  True,  True,  True, False, False, False, False, False]])\n        >>> data_split = split_trajectories(data, trajectory_key=\"trajectory\")\n        >>> print(data_split)\n        TensorDict(\n            fields={\n                mask: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                next: TensorDict(\n                    fields={\n                        done: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                        obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False)},\n                    batch_size=torch.Size([2, 10]),\n                    device=None,\n                    is_shared=False),\n                obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n                trajectory: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int32, is_shared=False)},\n            batch_size=torch.Size([2, 10]),\n            device=None,\n            is_shared=False)\n\n    \"\"\"\n    mask_key = None\n    if trajectory_key is not None:\n        from torchrl.envs.utils import _replace_last\n        traj_ids_key = trajectory_key\n        mask_key = _replace_last(trajectory_key, 'mask')\n    else:\n        if prefix is None and 'collector' in rollout_tensordict.keys():\n            prefix = 'collector'\n        if prefix is None:\n            traj_ids_key = 'traj_ids'\n            mask_key = 'mask'\n        else:\n            traj_ids_key = (prefix, 'traj_ids')\n            mask_key = (prefix, 'mask')\n    rollout_tensordict = rollout_tensordict.copy()\n    traj_ids = rollout_tensordict.get(traj_ids_key, None)\n    if traj_ids is None:\n        if done_key is None:\n            done_key = 'done'\n        done_key = ('next', done_key)\n        done = rollout_tensordict.get(done_key)\n        idx = (slice(None),) * (rollout_tensordict.ndim - 1) + (slice(None, -1),)\n        done_sel = done[idx]\n        pads = [1, 0]\n        pads = [0, 0] * (done.ndim - rollout_tensordict.ndim) + pads\n        done_sel = torch.nn.functional.pad(done_sel, pads)\n        if done_sel.shape != done.shape:\n            raise RuntimeError(f'done and done_sel have different shape {done.shape} - {done_sel.shape} ')\n        traj_ids = done_sel.cumsum(rollout_tensordict.ndim - 1)\n        traj_ids = traj_ids.squeeze(-1)\n        if rollout_tensordict.ndim > 1:\n            for i in range(1, rollout_tensordict.shape[0]):\n                traj_ids[i] += traj_ids[i - 1].max() + 1\n        rollout_tensordict.set(traj_ids_key, traj_ids)\n    splits = traj_ids.reshape(-1)\n    splits = [(splits == i).sum().item() for i in splits.unique_consecutive()]\n    if len(set(splits)) == 1 and splits[0] == traj_ids.shape[-1]:\n        rollout_tensordict.set(mask_key, torch.ones(rollout_tensordict.shape, device=rollout_tensordict.device, dtype=torch.bool))\n        if rollout_tensordict.ndimension() == 1:\n            rollout_tensordict = rollout_tensordict.unsqueeze(0)\n        return rollout_tensordict\n    out_splits = rollout_tensordict.reshape(-1)\n    if as_nested:\n        if hasattr(torch, '_nested_compute_contiguous_strides_offsets'):\n\n            def nest(x, splits=splits):\n                shape = torch.tensor([[int(split), *x.shape[1:]] for split in splits])\n                return torch._nested_view_from_buffer(x.reshape(-1), shape, *torch._nested_compute_contiguous_strides_offsets(shape))\n            return out_splits._fast_apply(nest, batch_size=[len(splits), -1])\n        else:\n            out_splits = out_splits.split(splits, 0)\n            layout = as_nested if as_nested is not bool else None\n            if torch.__version__ < '2.4':\n                if layout not in (True,):\n                    raise RuntimeError(f'layout={layout} is only available for torch>=v2.4')\n\n                def nest(*x):\n                    return torch.nested.nested_tensor(list(x))\n            else:\n\n                def nest(*x):\n                    return torch.nested.nested_tensor(list(x), layout=layout)\n            return out_splits[0]._fast_apply(nest, *out_splits[1:], batch_size=[len(out_splits), *out_splits[0].batch_size[:-1], -1])\n    out_splits = out_splits.split(splits, 0)\n    for out_split in out_splits:\n        out_split.set(mask_key, torch.ones(out_split.shape, dtype=torch.bool, device=out_split.device))\n    if len(out_splits) > 1:\n        MAX = max(*[out_split.shape[0] for out_split in out_splits])\n    else:\n        MAX = out_splits[0].shape[0]\n    td = torch.stack([pad(out_split, [0, MAX - out_split.shape[0]]) for out_split in out_splits], 0)\n    return td",
        "description": ""
    }
]
2024-12-03 19:09:29
collectors.py: [
    {
        "function_name": "recursive_map_to_cpu",
        "args": [
            {
                "arg_name": "dictionary",
                "return_type": "OrderedDict",
                "default_value": ""
            }
        ],
        "signature": "recursive_map_to_cpu(dictionary: OrderedDict) -> OrderedDict",
        "function_code": "def recursive_map_to_cpu(dictionary: OrderedDict) -> OrderedDict:\n    \"\"\"Maps the tensors to CPU through a nested dictionary.\"\"\"\n    return OrderedDict(**{k: recursive_map_to_cpu(item) if isinstance(item, OrderedDict) else item.cpu() if isinstance(item, torch.Tensor) else item for k, item in dictionary.items()})",
        "description": ""
    },
    {
        "class_name": "DataCollectorBase",
        "bases": [
            "IterableDataset"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "update_policy_weights_",
                "args": [
                    {
                        "arg_name": "policy_weights",
                        "return_type": "Optional[TensorDictBase]",
                        "default_value": "None"
                    }
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    \"\"\"Updates the policy weights if the policy of the data collector and the trained policy live on different devices.\n\n        Args:\n            policy_weights (TensorDictBase, optional): if provided, a TensorDict containing\n                the weights of the policy to be used for the udpdate.\n\n        \"\"\"\n    if policy_weights is not None:\n        self.policy_weights.data.update_(policy_weights)\n    elif self.get_weights_fn is not None:\n        self.policy_weights.data.update_(self.get_weights_fn())",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [],
                "signature": "next(self)",
                "function_code": "def next(self):\n    try:\n        if self._iterator is None:\n            self._iterator = iter(self)\n        out = next(self._iterator)\n        out.clear_device_()\n        return out\n    except StopIteration:\n        return None",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [],
                "signature": "shutdown(self)",
                "function_code": "@abc.abstractmethod\ndef shutdown(self):\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "@abc.abstractmethod\ndef iterator(self) -> Iterator[TensorDictBase]:\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "seed",
                        "return_type": "int",
                        "default_value": ""
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "@abc.abstractmethod\ndef set_seed(self, seed: int, static_seed: bool=False) -> int:\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "@abc.abstractmethod\ndef state_dict(self) -> OrderedDict:\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "@abc.abstractmethod\ndef load_state_dict(self, state_dict: OrderedDict) -> None:\n    raise NotImplementedError",
                "description": ""
            }
        ]
    },
    {
        "class_name": "SyncDataCollector",
        "bases": [
            "DataCollectorBase"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "__init__",
                "args": [
                    {
                        "arg_name": "create_env_fn",
                        "return_type": "Union[EnvBase",
                        "default_value": ""
                    },
                    {
                        "arg_name": "EnvCreator",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "Sequence",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "Callable",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "EnvBase",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "policy",
                        "return_type": "Optional[Union[TensorDictModule",
                        "default_value": ""
                    },
                    {
                        "arg_name": "Callable",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "TensorDictBase",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "TensorDictBase",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "None",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "frames_per_batch",
                        "return_type": "int",
                        "default_value": ""
                    },
                    {
                        "arg_name": "total_frames",
                        "return_type": "int",
                        "default_value": "-1"
                    },
                    {
                        "arg_name": "device",
                        "return_type": "DEVICE_TYPING",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "storing_device",
                        "return_type": "DEVICE_TYPING",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "policy_device",
                        "return_type": "DEVICE_TYPING",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "env_device",
                        "return_type": "DEVICE_TYPING",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "create_env_kwargs",
                        "return_type": "dict | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "max_frames_per_traj",
                        "return_type": "int | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "init_random_frames",
                        "return_type": "int | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "reset_at_each_iter",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "postproc",
                        "return_type": "Callable[[TensorDictBase]",
                        "default_value": ""
                    },
                    {
                        "arg_name": "TensorDictBase",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "None",
                        "return_type": "",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "split_trajs",
                        "return_type": "bool | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "exploration_type",
                        "return_type": "ExplorationType",
                        "default_value": "DEFAULT_EXPLORATION_TYPE"
                    },
                    {
                        "arg_name": "return_same_td",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "reset_when_done",
                        "return_type": "bool",
                        "default_value": "True"
                    },
                    {
                        "arg_name": "interruptor",
                        "return_type": "",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "set_truncated",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "use_buffers",
                        "return_type": "bool | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "replay_buffer",
                        "return_type": "ReplayBuffer | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "trust_policy",
                        "return_type": "bool",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "compile_policy",
                        "return_type": "bool | Dict[str",
                        "default_value": ""
                    },
                    {
                        "arg_name": "Any",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "None",
                        "return_type": "",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "cudagraph_policy",
                        "return_type": "bool | Dict[str",
                        "default_value": ""
                    },
                    {
                        "arg_name": "Any",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "None",
                        "return_type": "",
                        "default_value": "None"
                    }
                ],
                "signature": "__init__(self, create_env_fn: Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]=None, *, frames_per_batch: int, total_frames: int=-1, device: DEVICE_TYPING=None, storing_device: DEVICE_TYPING=None, policy_device: DEVICE_TYPING=None, env_device: DEVICE_TYPING=None, create_env_kwargs: dict | None=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Callable[[TensorDictBase], TensorDictBase] | None=None, split_trajs: bool | None=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, return_same_td: bool=False, reset_when_done: bool=True, interruptor=None, set_truncated: bool=False, use_buffers: bool | None=None, replay_buffer: ReplayBuffer | None=None, trust_policy: bool=None, compile_policy: bool | Dict[str, Any] | None=None, cudagraph_policy: bool | Dict[str, Any] | None=None, **kwargs)",
                "function_code": "def __init__(self, create_env_fn: Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]=None, *, frames_per_batch: int, total_frames: int=-1, device: DEVICE_TYPING=None, storing_device: DEVICE_TYPING=None, policy_device: DEVICE_TYPING=None, env_device: DEVICE_TYPING=None, create_env_kwargs: dict | None=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Callable[[TensorDictBase], TensorDictBase] | None=None, split_trajs: bool | None=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, return_same_td: bool=False, reset_when_done: bool=True, interruptor=None, set_truncated: bool=False, use_buffers: bool | None=None, replay_buffer: ReplayBuffer | None=None, trust_policy: bool=None, compile_policy: bool | Dict[str, Any] | None=None, cudagraph_policy: bool | Dict[str, Any] | None=None, **kwargs):\n    from torchrl.envs.batched_envs import BatchedEnvBase\n    self.closed = True\n    if create_env_kwargs is None:\n        create_env_kwargs = {}\n    if not isinstance(create_env_fn, EnvBase):\n        env = create_env_fn(**create_env_kwargs)\n    else:\n        env = create_env_fn\n        if create_env_kwargs:\n            if not isinstance(env, BatchedEnvBase):\n                raise RuntimeError(f\"kwargs were passed to SyncDataCollector but they can't be set on environment of type {type(create_env_fn)}.\")\n            env.update_kwargs(create_env_kwargs)\n    if policy is None:\n        policy = RandomPolicy(env.full_action_spec)\n    if trust_policy is None:\n        trust_policy = isinstance(policy, (RandomPolicy, CudaGraphModule))\n    self.trust_policy = trust_policy\n    self._read_compile_kwargs(compile_policy, cudagraph_policy)\n    self._traj_pool_val = kwargs.pop('traj_pool', None)\n    if kwargs:\n        raise TypeError(f'Keys {list(kwargs.keys())} are unknown to {type(self).__name__}.')\n    storing_device, policy_device, env_device = self._get_devices(storing_device=storing_device, policy_device=policy_device, env_device=env_device, device=device)\n    self.storing_device = storing_device\n    if self.storing_device is not None and self.storing_device.type != 'cuda':\n        if torch.cuda.is_available():\n            self._sync_storage = torch.cuda.synchronize\n        elif torch.backends.mps.is_available() and hasattr(torch, 'mps'):\n            self._sync_storage = torch.mps.synchronize\n        elif self.storing_device.type == 'cpu':\n            self._sync_storage = _do_nothing\n        else:\n            raise RuntimeError('Non supported device')\n    else:\n        self._sync_storage = _do_nothing\n    self.env_device = env_device\n    if self.env_device is not None and self.env_device.type != 'cuda':\n        if torch.cuda.is_available():\n            self._sync_env = torch.cuda.synchronize\n        elif torch.backends.mps.is_available() and hasattr(torch, 'mps'):\n            self._sync_env = torch.mps.synchronize\n        elif self.env_device.type == 'cpu':\n            self._sync_env = _do_nothing\n        else:\n            raise RuntimeError('Non supported device')\n    else:\n        self._sync_env = _do_nothing\n    self.policy_device = policy_device\n    if self.policy_device is not None and self.policy_device.type != 'cuda':\n        if torch.cuda.is_available():\n            self._sync_policy = torch.cuda.synchronize\n        elif torch.backends.mps.is_available() and hasattr(torch, 'mps'):\n            self._sync_policy = torch.mps.synchronize\n        elif self.policy_device.type == 'cpu':\n            self._sync_policy = _do_nothing\n        else:\n            raise RuntimeError('Non supported device')\n    else:\n        self._sync_policy = _do_nothing\n    self.device = device\n    self._cast_to_policy_device = self.policy_device != self.env_device\n    self.env: EnvBase = env\n    del env\n    self.replay_buffer = replay_buffer\n    if self.replay_buffer is not None:\n        if postproc is not None:\n            raise TypeError('postproc must be None when a replay buffer is passed.')\n        if use_buffers:\n            raise TypeError('replay_buffer is exclusive with use_buffers.')\n    if use_buffers is None:\n        use_buffers = not self.env._has_dynamic_specs and self.replay_buffer is None\n    self._use_buffers = use_buffers\n    self.replay_buffer = replay_buffer\n    self.closed = False\n    if not reset_when_done:\n        raise ValueError('reset_when_done is deprectated.')\n    self.reset_when_done = reset_when_done\n    self.n_env = self.env.batch_size.numel()\n    self.policy, self.get_weights_fn = self._get_policy_and_device(policy=policy, observation_spec=self.env.observation_spec)\n    if isinstance(self.policy, nn.Module):\n        self.policy_weights = TensorDict.from_module(self.policy, as_module=True)\n    else:\n        self.policy_weights = TensorDict()\n    if self.compiled_policy:\n        self.policy = torch.compile(self.policy, **self.compiled_policy_kwargs)\n    if self.cudagraphed_policy:\n        self.policy = CudaGraphModule(self.policy, **self.cudagraphed_policy_kwargs)\n    if self.env_device:\n        self.env: EnvBase = self.env.to(self.env_device)\n    elif self.env.device is not None:\n        self.env_device = self.env.device\n    self._cast_to_env_device = self._cast_to_policy_device or self.env.device != self.storing_device\n    self.max_frames_per_traj = int(max_frames_per_traj) if max_frames_per_traj is not None else 0\n    if self.max_frames_per_traj is not None and self.max_frames_per_traj > 0:\n        for key in self.env.output_spec.keys(True, True):\n            if isinstance(key, str):\n                key = (key,)\n            if 'step_count' in key:\n                raise ValueError(\"A 'step_count' key is already present in the environment and the 'max_frames_per_traj' argument may conflict with a 'StepCounter' that has already been set. Possible solutions: Set max_frames_per_traj to 0 or remove the StepCounter limit from the environment transforms.\")\n        self.env = TransformedEnv(self.env, StepCounter(max_steps=self.max_frames_per_traj))\n    if total_frames is None or total_frames < 0:\n        total_frames = float('inf')\n    else:\n        remainder = total_frames % frames_per_batch\n        if remainder != 0 and RL_WARNINGS:\n            warnings.warn(f'total_frames ({total_frames}) is not exactly divisible by frames_per_batch ({frames_per_batch}).This means {frames_per_batch - remainder} additional frames will be collected.To silence this message, set the environment variable RL_WARNINGS to False.')\n    self.total_frames = int(total_frames) if total_frames != float('inf') else total_frames\n    self.reset_at_each_iter = reset_at_each_iter\n    self.init_random_frames = int(init_random_frames) if init_random_frames is not None else 0\n    if init_random_frames is not None and init_random_frames % frames_per_batch != 0 and RL_WARNINGS:\n        warnings.warn(f'init_random_frames ({init_random_frames}) is not exactly a multiple of frames_per_batch ({frames_per_batch}),  this results in more init_random_frames than requested ({-(-init_random_frames // frames_per_batch) * frames_per_batch}).To silence this message, set the environment variable RL_WARNINGS to False.')\n    self.postproc = postproc\n    if self.postproc is not None and hasattr(self.postproc, 'to') and self.storing_device:\n        self.postproc.to(self.storing_device)\n    if frames_per_batch % self.n_env != 0 and RL_WARNINGS:\n        warnings.warn(f'frames_per_batch ({frames_per_batch}) is not exactly divisible by the number of batched environments ({self.n_env}),  this results in more frames_per_batch per iteration that requested ({-(-frames_per_batch // self.n_env) * self.n_env}).To silence this message, set the environment variable RL_WARNINGS to False.')\n    self.requested_frames_per_batch = int(frames_per_batch)\n    self.frames_per_batch = -(-frames_per_batch // self.n_env)\n    self.exploration_type = exploration_type if exploration_type else DEFAULT_EXPLORATION_TYPE\n    self.return_same_td = return_same_td\n    self.set_truncated = set_truncated\n    self._make_shuttle()\n    if self._use_buffers:\n        self._make_final_rollout()\n    self._set_truncated_keys()\n    if split_trajs is None:\n        split_trajs = False\n    self.split_trajs = split_trajs\n    self._exclude_private_keys = True\n    self.interruptor = interruptor\n    self._frames = 0\n    self._iter = -1",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "update_policy_weights_",
                "args": [
                    {
                        "arg_name": "policy_weights",
                        "return_type": "Optional[TensorDictBase]",
                        "default_value": "None"
                    }
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    super().update_policy_weights_(policy_weights)",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "seed",
                        "return_type": "int",
                        "default_value": ""
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    \"\"\"Sets the seeds of the environments stored in the DataCollector.\n\n        Args:\n            seed (int): integer representing the seed to be used for the environment.\n            static_seed(bool, optional): if ``True``, the seed is not incremented.\n                Defaults to False\n\n        Returns:\n            Output seed. This is useful when more than one environment is contained in the DataCollector, as the\n            seed will be incremented for each of these. The resulting seed is the seed of the last environment.\n\n        Examples:\n            >>> from torchrl.envs import ParallelEnv\n            >>> from torchrl.envs.libs.gym import GymEnv\n            >>> from tensordict.nn import TensorDictModule\n            >>> from torch import nn\n            >>> env_fn = lambda: GymEnv(\"Pendulum-v1\")\n            >>> env_fn_parallel = ParallelEnv(6, env_fn)\n            >>> policy = TensorDictModule(nn.Linear(3, 1), in_keys=[\"observation\"], out_keys=[\"action\"])\n            >>> collector = SyncDataCollector(env_fn_parallel, policy, total_frames=300, frames_per_batch=100)\n            >>> out_seed = collector.set_seed(1)  # out_seed = 6\n\n        \"\"\"\n    out = self.env.set_seed(seed, static_seed=static_seed)\n    return out",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "def iterator(self) -> Iterator[TensorDictBase]:\n    \"\"\"Iterates through the DataCollector.\n\n        Yields: TensorDictBase objects containing (chunks of) trajectories\n\n        \"\"\"\n    if self.storing_device and self.storing_device.type == 'cuda':\n        stream = torch.cuda.Stream(self.storing_device, priority=-1)\n        event = stream.record_event()\n        streams = [stream]\n        events = [event]\n    elif self.storing_device is None:\n        streams = []\n        events = []\n        cuda_devices = set()\n\n        def cuda_check(tensor: torch.Tensor):\n            if tensor.is_cuda:\n                cuda_devices.add(tensor.device)\n        if not self._use_buffers:\n            for spec in self.env.specs.values(True, True):\n                if spec.device.type == 'cuda':\n                    if ':' not in str(spec.device):\n                        raise RuntimeError(\"A cuda spec did not have a device associated. Make sure to pass `'cuda:device_num'` to each spec device.\")\n                    cuda_devices.add(spec.device)\n        else:\n            self._final_rollout.apply(cuda_check, filter_empty=True)\n        for device in cuda_devices:\n            streams.append(torch.cuda.Stream(device, priority=-1))\n            events.append(streams[-1].record_event())\n    else:\n        streams = []\n        events = []\n    with contextlib.ExitStack() as stack:\n        for stream in streams:\n            stack.enter_context(torch.cuda.stream(stream))\n        while self._frames < self.total_frames:\n            self._iter += 1\n            tensordict_out = self.rollout()\n            if tensordict_out is None:\n                yield\n                continue\n            self._increment_frames(tensordict_out.numel())\n            if self.split_trajs:\n                tensordict_out = split_trajectories(tensordict_out, prefix='collector')\n            if self.postproc is not None:\n                tensordict_out = self.postproc(tensordict_out)\n            if self._exclude_private_keys:\n\n                def is_private(key):\n                    if isinstance(key, str) and key.startswith('_'):\n                        return True\n                    if isinstance(key, tuple) and any((_key.startswith('_') for _key in key)):\n                        return True\n                    return False\n                excluded_keys = [key for key in tensordict_out.keys(True) if is_private(key)]\n                tensordict_out = tensordict_out.exclude(*excluded_keys, inplace=True)\n            if self.return_same_td:\n                if events:\n                    for event in events:\n                        event.record()\n                        event.synchronize()\n                yield tensordict_out\n            else:\n                yield tensordict_out.clone()",
                "description": ""
            },
            {
                "function_name": "rollout",
                "args": [],
                "signature": "rollout(self) -> TensorDictBase",
                "function_code": "@torch.no_grad()\ndef rollout(self) -> TensorDictBase:\n    \"\"\"Computes a rollout in the environment using the provided policy.\n\n        Returns:\n            TensorDictBase containing the computed rollout.\n\n        \"\"\"\n    if self.reset_at_each_iter:\n        self._shuttle.update(self.env.reset())\n    if self._use_buffers:\n        self._final_rollout.fill_(('collector', 'traj_ids'), -1)\n    else:\n        pass\n    tensordicts = []\n    with set_exploration_type(self.exploration_type):\n        for t in range(self.frames_per_batch):\n            if self.init_random_frames is not None and self._frames < self.init_random_frames:\n                self.env.rand_action(self._shuttle)\n            else:\n                if self._cast_to_policy_device:\n                    if self.policy_device is not None:\n                        policy_input = self._shuttle.to(self.policy_device, non_blocking=True)\n                        self._sync_policy()\n                    elif self.policy_device is None:\n                        policy_input = self._shuttle\n                else:\n                    policy_input = self._shuttle\n                policy_output = self.policy(policy_input)\n                if self._shuttle is not policy_output:\n                    self._shuttle.update(policy_output, keys_to_update=self._policy_output_keys)\n            if self._cast_to_env_device:\n                if self.env_device is not None:\n                    env_input = self._shuttle.to(self.env_device, non_blocking=True)\n                    self._sync_env()\n                elif self.env_device is None:\n                    env_input = self._shuttle\n            else:\n                env_input = self._shuttle\n            env_output, env_next_output = self.env.step_and_maybe_reset(env_input)\n            if self._shuttle is not env_output:\n                next_data = env_output.get('next')\n                if self._shuttle_has_no_device:\n                    next_data.clear_device_()\n                self._shuttle.set('next', next_data)\n            if self.replay_buffer is not None:\n                self.replay_buffer.add(self._shuttle)\n                if self._increment_frames(self._shuttle.numel()):\n                    return\n            elif self.storing_device is not None:\n                tensordicts.append(self._shuttle.to(self.storing_device, non_blocking=True))\n                self._sync_storage()\n            else:\n                tensordicts.append(self._shuttle)\n            collector_data = self._shuttle.get('collector').copy()\n            self._shuttle = env_next_output\n            if self._shuttle_has_no_device:\n                self._shuttle.clear_device_()\n            self._shuttle.set('collector', collector_data)\n            self._update_traj_ids(env_output)\n            if self.interruptor is not None and self.interruptor.collection_stopped():\n                if self.replay_buffer is not None:\n                    return\n                result = self._final_rollout\n                if self._use_buffers:\n                    try:\n                        torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout[..., :t + 1])\n                    except RuntimeError:\n                        with self._final_rollout.unlock_():\n                            torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout[..., :t + 1])\n                else:\n                    result = TensorDict.maybe_dense_stack(tensordicts, dim=-1)\n                break\n        else:\n            if self._use_buffers:\n                result = self._final_rollout\n                try:\n                    result = torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout)\n                except RuntimeError:\n                    with self._final_rollout.unlock_():\n                        result = torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout)\n            elif self.replay_buffer is not None:\n                return\n            else:\n                result = TensorDict.maybe_dense_stack(tensordicts, dim=-1)\n                result.refine_names(..., 'time')\n    return self._maybe_set_truncated(result)",
                "description": ""
            },
            {
                "function_name": "reset",
                "args": [
                    {
                        "arg_name": "index",
                        "return_type": "",
                        "default_value": "None"
                    }
                ],
                "signature": "reset(self, index=None, **kwargs) -> None",
                "function_code": "@torch.no_grad()\ndef reset(self, index=None, **kwargs) -> None:\n    \"\"\"Resets the environments to a new initial state.\"\"\"\n    collector_metadata = self._shuttle.get('collector').clone()\n    if index is not None:\n        if prod(self.env.batch_size) == 0:\n            raise RuntimeError('resetting unique env with index is not permitted.')\n        for reset_key, done_keys in zip(self.env.reset_keys, self.env.done_keys_groups):\n            _reset = torch.zeros(self.env.full_done_spec[done_keys[0]].shape, dtype=torch.bool, device=self.env.device)\n            _reset[index] = 1\n            self._shuttle.set(reset_key, _reset)\n    else:\n        _reset = None\n        self._shuttle.zero_()\n    self._shuttle.update(self.env.reset(**kwargs), inplace=True)\n    collector_metadata['traj_ids'] = collector_metadata['traj_ids'] - collector_metadata['traj_ids'].min()\n    self._shuttle['collector'] = collector_metadata",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [],
                "signature": "shutdown(self) -> None",
                "function_code": "def shutdown(self) -> None:\n    \"\"\"Shuts down all workers and/or closes the local environment.\"\"\"\n    if not self.closed:\n        self.closed = True\n        del self._shuttle\n        if self._use_buffers:\n            del self._final_rollout\n        if not self.env.is_closed:\n            self.env.close()\n        del self.env\n    return",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    \"\"\"Returns the local state_dict of the data collector (environment and policy).\n\n        Returns:\n            an ordered dictionary with fields :obj:`\"policy_state_dict\"` and\n            `\"env_state_dict\"`.\n\n        \"\"\"\n    from torchrl.envs.batched_envs import BatchedEnvBase\n    if isinstance(self.env, TransformedEnv):\n        env_state_dict = self.env.transform.state_dict()\n    elif isinstance(self.env, BatchedEnvBase):\n        env_state_dict = self.env.state_dict()\n    else:\n        env_state_dict = OrderedDict()\n    if hasattr(self.policy, 'state_dict'):\n        policy_state_dict = self.policy.state_dict()\n        state_dict = OrderedDict(policy_state_dict=policy_state_dict, env_state_dict=env_state_dict)\n    else:\n        state_dict = OrderedDict(env_state_dict=env_state_dict)\n    state_dict.update({'frames': self._frames, 'iter': self._iter})\n    return state_dict",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict, **kwargs) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict, **kwargs) -> None:\n    \"\"\"Loads a state_dict on the environment and policy.\n\n        Args:\n            state_dict (OrderedDict): ordered dictionary containing the fields\n                `\"policy_state_dict\"` and :obj:`\"env_state_dict\"`.\n\n        \"\"\"\n    strict = kwargs.get('strict', True)\n    if strict or 'env_state_dict' in state_dict:\n        self.env.load_state_dict(state_dict['env_state_dict'], **kwargs)\n    if strict or 'policy_state_dict' in state_dict:\n        self.policy.load_state_dict(state_dict['policy_state_dict'], **kwargs)\n    self._frames = state_dict['frames']\n    self._iter = state_dict['iter']",
                "description": ""
            }
        ]
    },
    {
        "class_name": "MultiSyncDataCollector",
        "bases": [
            "_MultiDataCollector"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "next",
                "args": [],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [],
                "signature": "shutdown(self)",
                "function_code": "def shutdown(self):\n    if hasattr(self, 'out_buffer'):\n        del self.out_buffer\n    if hasattr(self, 'buffers'):\n        del self.buffers\n    return super().shutdown()",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "seed",
                        "return_type": "int",
                        "default_value": ""
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    return super().set_seed(seed, static_seed)",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    return super().state_dict()",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict) -> None:\n    return super().load_state_dict(state_dict)",
                "description": ""
            },
            {
                "function_name": "update_policy_weights_",
                "args": [
                    {
                        "arg_name": "policy_weights",
                        "return_type": "Optional[TensorDictBase]",
                        "default_value": "None"
                    }
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    super().update_policy_weights_(policy_weights)",
                "description": ""
            },
            {
                "function_name": "frames_per_batch_worker",
                "args": [],
                "signature": "frames_per_batch_worker(self)",
                "function_code": "@property\ndef frames_per_batch_worker(self):\n    if self.requested_frames_per_batch % self.num_workers != 0 and RL_WARNINGS:\n        warnings.warn(f'frames_per_batch {self.requested_frames_per_batch} is not exactly divisible by the number of collector workers {self.num_workers}, this results in more frames_per_batch per iteration that requested.To silence this message, set the environment variable RL_WARNINGS to False.')\n    frames_per_batch_worker = -(-self.requested_frames_per_batch // self.num_workers)\n    return frames_per_batch_worker",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "def iterator(self) -> Iterator[TensorDictBase]:\n    cat_results = self.cat_results\n    if cat_results is None:\n        cat_results = 'stack'\n    self.buffers = {}\n    dones = [False for _ in range(self.num_workers)]\n    workers_frames = [0 for _ in range(self.num_workers)]\n    same_device = None\n    self.out_buffer = None\n    preempt = self.interruptor is not None and self.preemptive_threshold < 1.0\n    while not all(dones) and self._frames < self.total_frames:\n        _check_for_faulty_process(self.procs)\n        if self.update_at_each_batch:\n            self.update_policy_weights_()\n        for idx in range(self.num_workers):\n            if self.init_random_frames is not None and self._frames < self.init_random_frames:\n                msg = 'continue_random'\n            else:\n                msg = 'continue'\n            self.pipes[idx].send((None, msg))\n        self._iter += 1\n        if preempt:\n            self.interruptor.start_collection()\n            while self.queue_out.qsize() < int(self.num_workers * self.preemptive_threshold):\n                continue\n            self.interruptor.stop_collection()\n            while self.queue_out.qsize() < int(self.num_workers):\n                continue\n        for _ in range(self.num_workers):\n            new_data, j = self.queue_out.get()\n            use_buffers = self._use_buffers\n            if self.replay_buffer is not None:\n                idx = new_data\n                workers_frames[idx] = workers_frames[idx] + self.frames_per_batch_worker\n                continue\n            elif j == 0 or not use_buffers:\n                try:\n                    data, idx = new_data\n                    self.buffers[idx] = data\n                    if use_buffers is None and j > 0:\n                        self._use_buffers = False\n                except TypeError:\n                    if use_buffers is None:\n                        self._use_buffers = True\n                        idx = new_data\n                    else:\n                        raise\n            else:\n                idx = new_data\n            if preempt:\n                if cat_results != 'stack':\n                    buffers = {}\n                    for idx, buffer in self.buffers.items():\n                        valid = buffer.get(('collector', 'traj_ids')) != -1\n                        if valid.ndim > 2:\n                            valid = valid.flatten(0, -2)\n                        if valid.ndim == 2:\n                            valid = valid.any(0)\n                        buffers[idx] = buffer[..., valid]\n                else:\n                    for buffer in self.buffers.values():\n                        with buffer.unlock_():\n                            buffer.set(('collector', 'mask'), buffer.get(('collector', 'traj_ids')) != -1)\n                    buffers = self.buffers\n            else:\n                buffers = self.buffers\n            workers_frames[idx] = workers_frames[idx] + buffers[idx].numel()\n            if workers_frames[idx] >= self.total_frames:\n                dones[idx] = True\n        if self.replay_buffer is not None:\n            yield\n            self._frames += self.frames_per_batch_worker * self.num_workers\n            continue\n        n_collected = 0\n        for idx in range(self.num_workers):\n            buffer = buffers[idx]\n            traj_ids = buffer.get(('collector', 'traj_ids'))\n            if preempt:\n                if cat_results == 'stack':\n                    mask_frames = buffer.get(('collector', 'traj_ids')) != -1\n                    n_collected += mask_frames.sum().cpu()\n                else:\n                    n_collected += traj_ids.numel()\n            else:\n                n_collected += traj_ids.numel()\n        if same_device is None:\n            prev_device = None\n            same_device = True\n            for item in self.buffers.values():\n                if prev_device is None:\n                    prev_device = item.device\n                else:\n                    same_device = same_device and item.device == prev_device\n        if cat_results == 'stack':\n            stack = torch.stack if self._use_buffers else TensorDict.maybe_dense_stack\n            if same_device:\n                self.out_buffer = stack(list(buffers.values()), 0)\n            else:\n                self.out_buffer = stack([item.cpu() for item in buffers.values()], 0)\n        else:\n            if self._use_buffers is None:\n                torchrl_logger.warning('use_buffer not specified and not yet inferred from data, assuming `True`.')\n            elif not self._use_buffers:\n                raise RuntimeError('Cannot concatenate results with use_buffers=False')\n            try:\n                if same_device:\n                    self.out_buffer = torch.cat(list(buffers.values()), cat_results)\n                else:\n                    self.out_buffer = torch.cat([item.cpu() for item in buffers.values()], cat_results)\n            except RuntimeError as err:\n                if preempt and cat_results != -1 and ('Sizes of tensors must match' in str(err)):\n                    raise RuntimeError(\"The value provided to cat_results isn't compatible with the collectors outputs. Consider using `cat_results=-1`.\")\n                raise\n        if self.split_trajs:\n            out = split_trajectories(self.out_buffer, prefix='collector')\n        else:\n            out = self.out_buffer\n        if cat_results in (-1, 'stack'):\n            out.refine_names(*[None] * (out.ndim - 1) + ['time'])\n        self._frames += n_collected\n        if self.postprocs:\n            self.postprocs = self.postprocs.to(out.device)\n            out = self.postprocs(out)\n        if self._exclude_private_keys:\n            excluded_keys = [key for key in out.keys() if key.startswith('_')]\n            if excluded_keys:\n                out = out.exclude(*excluded_keys)\n        yield out\n        del out\n    del self.buffers\n    self.out_buffer = None",
                "description": ""
            }
        ]
    },
    {
        "class_name": "MultiaSyncDataCollector",
        "bases": [
            "_MultiDataCollector"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "__init__",
                "args": [],
                "signature": "__init__(self, *args, **kwargs)",
                "function_code": "def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.out_tensordicts = defaultdict(lambda: None)\n    self.running = False\n    if self.postprocs is not None:\n        postproc = self.postprocs\n        self.postprocs = {}\n        for _device in self.storing_device:\n            if _device not in self.postprocs:\n                self.postprocs[_device] = deepcopy(postproc).to(_device)",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [],
                "signature": "shutdown(self)",
                "function_code": "def shutdown(self):\n    if hasattr(self, 'out_tensordicts'):\n        del self.out_tensordicts\n    return super().shutdown()",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "seed",
                        "return_type": "int",
                        "default_value": ""
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    return super().set_seed(seed, static_seed)",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    return super().state_dict()",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict) -> None:\n    return super().load_state_dict(state_dict)",
                "description": ""
            },
            {
                "function_name": "update_policy_weights_",
                "args": [
                    {
                        "arg_name": "policy_weights",
                        "return_type": "Optional[TensorDictBase]",
                        "default_value": "None"
                    }
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    super().update_policy_weights_(policy_weights)",
                "description": ""
            },
            {
                "function_name": "frames_per_batch_worker",
                "args": [],
                "signature": "frames_per_batch_worker(self)",
                "function_code": "@property\ndef frames_per_batch_worker(self):\n    return self.requested_frames_per_batch",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "def iterator(self) -> Iterator[TensorDictBase]:\n    if self.update_at_each_batch:\n        self.update_policy_weights_()\n    for i in range(self.num_workers):\n        if self.init_random_frames is not None and self.init_random_frames > 0:\n            self.pipes[i].send((None, 'continue_random'))\n        else:\n            self.pipes[i].send((None, 'continue'))\n    self.running = True\n    workers_frames = [0 for _ in range(self.num_workers)]\n    while self._frames < self.total_frames:\n        self._iter += 1\n        while True:\n            try:\n                idx, j, out = self._get_from_queue(timeout=10.0)\n                break\n            except TimeoutError:\n                _check_for_faulty_process(self.procs)\n        if self.replay_buffer is None:\n            worker_frames = out.numel()\n            if self.split_trajs:\n                out = split_trajectories(out, prefix='collector')\n        else:\n            worker_frames = self.frames_per_batch_worker\n        self._frames += worker_frames\n        workers_frames[idx] = workers_frames[idx] + worker_frames\n        if self.postprocs:\n            out = self.postprocs[out.device](out)\n        if self.init_random_frames is not None and self._frames < self.init_random_frames:\n            msg = 'continue_random'\n        else:\n            msg = 'continue'\n        self.pipes[idx].send((idx, msg))\n        if out is not None and self._exclude_private_keys:\n            excluded_keys = [key for key in out.keys() if key.startswith('_')]\n            out = out.exclude(*excluded_keys)\n        yield out\n    self.running = False",
                "description": ""
            },
            {
                "function_name": "reset",
                "args": [
                    {
                        "arg_name": "reset_idx",
                        "return_type": "Optional[Sequence[bool]]",
                        "default_value": "None"
                    }
                ],
                "signature": "reset(self, reset_idx: Optional[Sequence[bool]]=None) -> None",
                "function_code": "def reset(self, reset_idx: Optional[Sequence[bool]]=None) -> None:\n    super().reset(reset_idx)\n    if self.queue_out.full():\n        time.sleep(_TIMEOUT)\n    if self.queue_out.full():\n        raise Exception('self.queue_out is full')\n    if self.running:\n        for idx in range(self.num_workers):\n            if self.init_random_frames is not None and self._frames < self.init_random_frames:\n                self.pipes[idx].send((idx, 'continue_random'))\n            else:\n                self.pipes[idx].send((idx, 'continue'))",
                "description": ""
            }
        ]
    },
    {
        "class_name": "aSyncDataCollector",
        "bases": [
            "MultiaSyncDataCollector"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "__init__",
                "args": [
                    {
                        "arg_name": "create_env_fn",
                        "return_type": "Callable[[]",
                        "default_value": ""
                    },
                    {
                        "arg_name": "EnvBase",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "policy",
                        "return_type": "Optional[Union[TensorDictModule",
                        "default_value": ""
                    },
                    {
                        "arg_name": "Callable",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "TensorDictBase",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "TensorDictBase",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "frames_per_batch",
                        "return_type": "int",
                        "default_value": ""
                    },
                    {
                        "arg_name": "total_frames",
                        "return_type": "Optional[int]",
                        "default_value": "-1"
                    },
                    {
                        "arg_name": "device",
                        "return_type": "DEVICE_TYPING | Sequence[DEVICE_TYPING] | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "storing_device",
                        "return_type": "DEVICE_TYPING | Sequence[DEVICE_TYPING] | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "env_device",
                        "return_type": "DEVICE_TYPING | Sequence[DEVICE_TYPING] | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "policy_device",
                        "return_type": "DEVICE_TYPING | Sequence[DEVICE_TYPING] | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "create_env_kwargs",
                        "return_type": "Optional[Sequence[dict]]",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "max_frames_per_traj",
                        "return_type": "int | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "init_random_frames",
                        "return_type": "int | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "reset_at_each_iter",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "postproc",
                        "return_type": "Optional[Callable[[TensorDictBase]",
                        "default_value": ""
                    },
                    {
                        "arg_name": "TensorDictBase",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "None",
                        "return_type": "",
                        "default_value": ""
                    },
                    {
                        "arg_name": "split_trajs",
                        "return_type": "Optional[bool]",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "exploration_type",
                        "return_type": "ExplorationType",
                        "default_value": "DEFAULT_EXPLORATION_TYPE"
                    },
                    {
                        "arg_name": "reset_when_done",
                        "return_type": "bool",
                        "default_value": "True"
                    },
                    {
                        "arg_name": "update_at_each_batch",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "preemptive_threshold",
                        "return_type": "float",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "num_threads",
                        "return_type": "int",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "num_sub_threads",
                        "return_type": "int",
                        "default_value": "1"
                    },
                    {
                        "arg_name": "set_truncated",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "__init__(self, create_env_fn: Callable[[], EnvBase], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]], *, frames_per_batch: int, total_frames: Optional[int]=-1, device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, storing_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, env_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, policy_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, create_env_kwargs: Optional[Sequence[dict]]=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Optional[Callable[[TensorDictBase], TensorDictBase]]=None, split_trajs: Optional[bool]=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, reset_when_done: bool=True, update_at_each_batch: bool=False, preemptive_threshold: float=None, num_threads: int=None, num_sub_threads: int=1, set_truncated: bool=False, **kwargs)",
                "function_code": "def __init__(self, create_env_fn: Callable[[], EnvBase], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]], *, frames_per_batch: int, total_frames: Optional[int]=-1, device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, storing_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, env_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, policy_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, create_env_kwargs: Optional[Sequence[dict]]=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Optional[Callable[[TensorDictBase], TensorDictBase]]=None, split_trajs: Optional[bool]=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, reset_when_done: bool=True, update_at_each_batch: bool=False, preemptive_threshold: float=None, num_threads: int=None, num_sub_threads: int=1, set_truncated: bool=False, **kwargs):\n    super().__init__(create_env_fn=[create_env_fn], policy=policy, total_frames=total_frames, create_env_kwargs=[create_env_kwargs], max_frames_per_traj=max_frames_per_traj, frames_per_batch=frames_per_batch, reset_at_each_iter=reset_at_each_iter, init_random_frames=init_random_frames, postproc=postproc, split_trajs=split_trajs, device=device, policy_device=policy_device, env_device=env_device, storing_device=storing_device, exploration_type=exploration_type, reset_when_done=reset_when_done, update_at_each_batch=update_at_each_batch, preemptive_threshold=preemptive_threshold, num_threads=num_threads, num_sub_threads=num_sub_threads, set_truncated=set_truncated)",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [],
                "signature": "shutdown(self)",
                "function_code": "def shutdown(self):\n    return super().shutdown()",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "seed",
                        "return_type": "int",
                        "default_value": ""
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    return super().set_seed(seed, static_seed)",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    return super().state_dict()",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict) -> None:\n    return super().load_state_dict(state_dict)",
                "description": ""
            }
        ]
    }
]
2024-12-03 19:09:29
utils.py: [
    {
        "function_name": "split_trajectories",
        "args": [
            {
                "arg_name": "rollout_tensordict",
                "return_type": "TensorDictBase",
                "default_value": ""
            },
            {
                "arg_name": "prefix",
                "return_type": "",
                "default_value": "None"
            },
            {
                "arg_name": "trajectory_key",
                "return_type": "NestedKey | None",
                "default_value": "None"
            },
            {
                "arg_name": "done_key",
                "return_type": "NestedKey | None",
                "default_value": "None"
            },
            {
                "arg_name": "as_nested",
                "return_type": "bool",
                "default_value": "False"
            }
        ],
        "signature": "split_trajectories(rollout_tensordict: TensorDictBase, *, prefix=None, trajectory_key: NestedKey | None=None, done_key: NestedKey | None=None, as_nested: bool=False) -> TensorDictBase",
        "function_code": "@set_lazy_legacy(False)\ndef split_trajectories(rollout_tensordict: TensorDictBase, *, prefix=None, trajectory_key: NestedKey | None=None, done_key: NestedKey | None=None, as_nested: bool=False) -> TensorDictBase:\n    \"\"\"A util function for trajectory separation.\n\n    Takes a tensordict with a key traj_ids that indicates the id of each trajectory.\n\n    From there, builds a B x T x ... zero-padded tensordict with B batches on max duration T\n\n    Args:\n        rollout_tensordict (TensorDictBase): a rollout with adjacent trajectories\n            along the last dimension.\n\n    Keyword Args:\n        prefix (NestedKey, optional): the prefix used to read and write meta-data,\n            such as ``\"traj_ids\"`` (the optional integer id of each trajectory)\n            and the ``\"mask\"`` entry indicating which data are valid and which\n            aren't. Defaults to ``\"collector\"`` if the input has a ``\"collector\"``\n            entry, ``()`` (no prefix) otherwise.\n            ``prefix`` is kept as a legacy feature and will be deprecated eventually.\n            Prefer ``trajectory_key`` or ``done_key`` whenever possible.\n        trajectory_key (NestedKey, optional): the key pointing to the trajectory\n            ids. Supersedes ``done_key`` and ``prefix``. If not provided, defaults\n            to ``(prefix, \"traj_ids\")``.\n        done_key (NestedKey, optional): the key pointing to the ``\"done\"\"`` signal,\n            if the trajectory could not be directly recovered. Defaults to ``\"done\"``.\n        as_nested (bool or torch.layout, optional): whether to return the results as nested\n            tensors. Defaults to ``False``. If a ``torch.layout`` is provided, it will be used\n            to construct the nested tensor, otherwise the default layout will be used.\n\n            .. note:: Using ``split_trajectories(tensordict, as_nested=True).to_padded_tensor(mask=mask_key)``\n                should result in the exact same result as ``as_nested=False``. Since this is an experimental\n                feature and relies on nested_tensors, which API may change in the future, we made this\n                an optional feature. The runtime should be faster with ``as_nested=True``.\n\n            .. note:: Providing a layout lets the user control whether the nested tensor is to be used\n                with ``torch.strided`` or ``torch.jagged`` layout. While the former has slightly more\n                capabilities at the time of writing, the second will be the main focus of the PyTorch team\n                in the future due to its better compatibility with :func:`~torch.compile`.\n\n    Returns:\n        A new tensordict with a leading dimension corresponding to the trajectory.\n        A ``\"mask\"`` boolean entry sharing the ``trajectory_key`` prefix\n        and the tensordict shape is also added. It indicated the valid elements of the tensordict,\n        as well as a ``\"traj_ids\"`` entry if ``trajectory_key`` could not be found.\n\n    Examples:\n        >>> from tensordict import TensorDict\n        >>> import torch\n        >>> from torchrl.collectors.utils import split_trajectories\n        >>> obs = torch.cat([torch.arange(10), torch.arange(5)])\n        >>> obs_ = torch.cat([torch.arange(1, 11), torch.arange(1, 6)])\n        >>> done = torch.zeros(15, dtype=torch.bool)\n        >>> done[9] = True\n        >>> trajectory_id = torch.cat([torch.zeros(10, dtype=torch.int32),\n        ...     torch.ones(5, dtype=torch.int32)])\n        >>> data = TensorDict({\"obs\": obs, (\"next\", \"obs\"): obs_, (\"next\", \"done\"): done, \"trajectory\": trajectory_id}, batch_size=[15])\n        >>> data_split = split_trajectories(data, done_key=\"done\")\n        >>> print(data_split)\n        TensorDict(\n            fields={\n                mask: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                next: TensorDict(\n                    fields={\n                        done: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                        obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False)},\n                    batch_size=torch.Size([2, 10]),\n                    device=None,\n                    is_shared=False),\n                obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n                traj_ids: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n                trajectory: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int32, is_shared=False)},\n            batch_size=torch.Size([2, 10]),\n            device=None,\n            is_shared=False)\n        >>> # check that split_trajectories got the trajectories right with the done signal\n        >>> assert (data_split[\"traj_ids\"] == data_split[\"trajectory\"]).all()\n        >>> print(data_split[\"mask\"])\n        tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n                [ True,  True,  True,  True,  True, False, False, False, False, False]])\n        >>> data_split = split_trajectories(data, trajectory_key=\"trajectory\")\n        >>> print(data_split)\n        TensorDict(\n            fields={\n                mask: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                next: TensorDict(\n                    fields={\n                        done: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                        obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False)},\n                    batch_size=torch.Size([2, 10]),\n                    device=None,\n                    is_shared=False),\n                obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n                trajectory: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int32, is_shared=False)},\n            batch_size=torch.Size([2, 10]),\n            device=None,\n            is_shared=False)\n\n    \"\"\"\n    mask_key = None\n    if trajectory_key is not None:\n        from torchrl.envs.utils import _replace_last\n        traj_ids_key = trajectory_key\n        mask_key = _replace_last(trajectory_key, 'mask')\n    else:\n        if prefix is None and 'collector' in rollout_tensordict.keys():\n            prefix = 'collector'\n        if prefix is None:\n            traj_ids_key = 'traj_ids'\n            mask_key = 'mask'\n        else:\n            traj_ids_key = (prefix, 'traj_ids')\n            mask_key = (prefix, 'mask')\n    rollout_tensordict = rollout_tensordict.copy()\n    traj_ids = rollout_tensordict.get(traj_ids_key, None)\n    if traj_ids is None:\n        if done_key is None:\n            done_key = 'done'\n        done_key = ('next', done_key)\n        done = rollout_tensordict.get(done_key)\n        idx = (slice(None),) * (rollout_tensordict.ndim - 1) + (slice(None, -1),)\n        done_sel = done[idx]\n        pads = [1, 0]\n        pads = [0, 0] * (done.ndim - rollout_tensordict.ndim) + pads\n        done_sel = torch.nn.functional.pad(done_sel, pads)\n        if done_sel.shape != done.shape:\n            raise RuntimeError(f'done and done_sel have different shape {done.shape} - {done_sel.shape} ')\n        traj_ids = done_sel.cumsum(rollout_tensordict.ndim - 1)\n        traj_ids = traj_ids.squeeze(-1)\n        if rollout_tensordict.ndim > 1:\n            for i in range(1, rollout_tensordict.shape[0]):\n                traj_ids[i] += traj_ids[i - 1].max() + 1\n        rollout_tensordict.set(traj_ids_key, traj_ids)\n    splits = traj_ids.reshape(-1)\n    splits = [(splits == i).sum().item() for i in splits.unique_consecutive()]\n    if len(set(splits)) == 1 and splits[0] == traj_ids.shape[-1]:\n        rollout_tensordict.set(mask_key, torch.ones(rollout_tensordict.shape, device=rollout_tensordict.device, dtype=torch.bool))\n        if rollout_tensordict.ndimension() == 1:\n            rollout_tensordict = rollout_tensordict.unsqueeze(0)\n        return rollout_tensordict\n    out_splits = rollout_tensordict.reshape(-1)\n    if as_nested:\n        if hasattr(torch, '_nested_compute_contiguous_strides_offsets'):\n\n            def nest(x, splits=splits):\n                shape = torch.tensor([[int(split), *x.shape[1:]] for split in splits])\n                return torch._nested_view_from_buffer(x.reshape(-1), shape, *torch._nested_compute_contiguous_strides_offsets(shape))\n            return out_splits._fast_apply(nest, batch_size=[len(splits), -1])\n        else:\n            out_splits = out_splits.split(splits, 0)\n            layout = as_nested if as_nested is not bool else None\n            if torch.__version__ < '2.4':\n                if layout not in (True,):\n                    raise RuntimeError(f'layout={layout} is only available for torch>=v2.4')\n\n                def nest(*x):\n                    return torch.nested.nested_tensor(list(x))\n            else:\n\n                def nest(*x):\n                    return torch.nested.nested_tensor(list(x), layout=layout)\n            return out_splits[0]._fast_apply(nest, *out_splits[1:], batch_size=[len(out_splits), *out_splits[0].batch_size[:-1], -1])\n    out_splits = out_splits.split(splits, 0)\n    for out_split in out_splits:\n        out_split.set(mask_key, torch.ones(out_split.shape, dtype=torch.bool, device=out_split.device))\n    if len(out_splits) > 1:\n        MAX = max(*[out_split.shape[0] for out_split in out_splits])\n    else:\n        MAX = out_splits[0].shape[0]\n    td = torch.stack([pad(out_split, [0, MAX - out_split.shape[0]]) for out_split in out_splits], 0)\n    return td",
        "description": ""
    }
]
2024-12-03 21:45:15
collectors.py: [
    {
        "function_name": "recursive_map_to_cpu",
        "args": [
            {
                "arg_name": "dictionary",
                "return_type": "OrderedDict",
                "default_value": ""
            }
        ],
        "signature": "recursive_map_to_cpu(dictionary: OrderedDict) -> OrderedDict",
        "function_code": "def recursive_map_to_cpu(dictionary: OrderedDict) -> OrderedDict:\n    \"\"\"Maps the tensors to CPU through a nested dictionary.\"\"\"\n    return OrderedDict(**{k: recursive_map_to_cpu(item) if isinstance(item, OrderedDict) else item.cpu() if isinstance(item, torch.Tensor) else item for k, item in dictionary.items()})",
        "description": ""
    },
    {
        "class_name": "DataCollectorBase",
        "bases": [
            "IterableDataset"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "update_policy_weights_",
                "args": [
                    {
                        "arg_name": "policy_weights",
                        "return_type": "Optional[TensorDictBase]",
                        "default_value": "None"
                    }
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    \"\"\"Updates the policy weights if the policy of the data collector and the trained policy live on different devices.\n\n        Args:\n            policy_weights (TensorDictBase, optional): if provided, a TensorDict containing\n                the weights of the policy to be used for the udpdate.\n\n        \"\"\"\n    if policy_weights is not None:\n        self.policy_weights.data.update_(policy_weights)\n    elif self.get_weights_fn is not None:\n        self.policy_weights.data.update_(self.get_weights_fn())",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [],
                "signature": "next(self)",
                "function_code": "def next(self):\n    try:\n        if self._iterator is None:\n            self._iterator = iter(self)\n        out = next(self._iterator)\n        out.clear_device_()\n        return out\n    except StopIteration:\n        return None",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [],
                "signature": "shutdown(self)",
                "function_code": "@abc.abstractmethod\ndef shutdown(self):\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "@abc.abstractmethod\ndef iterator(self) -> Iterator[TensorDictBase]:\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "seed",
                        "return_type": "int",
                        "default_value": ""
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "@abc.abstractmethod\ndef set_seed(self, seed: int, static_seed: bool=False) -> int:\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "@abc.abstractmethod\ndef state_dict(self) -> OrderedDict:\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "@abc.abstractmethod\ndef load_state_dict(self, state_dict: OrderedDict) -> None:\n    raise NotImplementedError",
                "description": ""
            }
        ]
    },
    {
        "class_name": "SyncDataCollector",
        "bases": [
            "DataCollectorBase"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "__init__",
                "args": [
                    {
                        "arg_name": "create_env_fn",
                        "return_type": "Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]]",
                        "default_value": ""
                    },
                    {
                        "arg_name": "policy",
                        "return_type": "Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "frames_per_batch",
                        "return_type": "int",
                        "default_value": ""
                    },
                    {
                        "arg_name": "total_frames",
                        "return_type": "int",
                        "default_value": "-1"
                    },
                    {
                        "arg_name": "device",
                        "return_type": "DEVICE_TYPING",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "storing_device",
                        "return_type": "DEVICE_TYPING",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "policy_device",
                        "return_type": "DEVICE_TYPING",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "env_device",
                        "return_type": "DEVICE_TYPING",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "create_env_kwargs",
                        "return_type": "dict | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "max_frames_per_traj",
                        "return_type": "int | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "init_random_frames",
                        "return_type": "int | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "reset_at_each_iter",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "postproc",
                        "return_type": "Callable[[TensorDictBase], TensorDictBase] | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "split_trajs",
                        "return_type": "bool | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "exploration_type",
                        "return_type": "ExplorationType",
                        "default_value": "DEFAULT_EXPLORATION_TYPE"
                    },
                    {
                        "arg_name": "return_same_td",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "reset_when_done",
                        "return_type": "bool",
                        "default_value": "True"
                    },
                    {
                        "arg_name": "interruptor",
                        "return_type": "",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "set_truncated",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "use_buffers",
                        "return_type": "bool | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "replay_buffer",
                        "return_type": "ReplayBuffer | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "trust_policy",
                        "return_type": "bool",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "compile_policy",
                        "return_type": "bool | Dict[str, Any] | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "cudagraph_policy",
                        "return_type": "bool | Dict[str, Any] | None",
                        "default_value": "None"
                    }
                ],
                "signature": "__init__(self, create_env_fn: Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]=None, *, frames_per_batch: int, total_frames: int=-1, device: DEVICE_TYPING=None, storing_device: DEVICE_TYPING=None, policy_device: DEVICE_TYPING=None, env_device: DEVICE_TYPING=None, create_env_kwargs: dict | None=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Callable[[TensorDictBase], TensorDictBase] | None=None, split_trajs: bool | None=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, return_same_td: bool=False, reset_when_done: bool=True, interruptor=None, set_truncated: bool=False, use_buffers: bool | None=None, replay_buffer: ReplayBuffer | None=None, trust_policy: bool=None, compile_policy: bool | Dict[str, Any] | None=None, cudagraph_policy: bool | Dict[str, Any] | None=None, **kwargs)",
                "function_code": "def __init__(self, create_env_fn: Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]=None, *, frames_per_batch: int, total_frames: int=-1, device: DEVICE_TYPING=None, storing_device: DEVICE_TYPING=None, policy_device: DEVICE_TYPING=None, env_device: DEVICE_TYPING=None, create_env_kwargs: dict | None=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Callable[[TensorDictBase], TensorDictBase] | None=None, split_trajs: bool | None=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, return_same_td: bool=False, reset_when_done: bool=True, interruptor=None, set_truncated: bool=False, use_buffers: bool | None=None, replay_buffer: ReplayBuffer | None=None, trust_policy: bool=None, compile_policy: bool | Dict[str, Any] | None=None, cudagraph_policy: bool | Dict[str, Any] | None=None, **kwargs):\n    from torchrl.envs.batched_envs import BatchedEnvBase\n    self.closed = True\n    if create_env_kwargs is None:\n        create_env_kwargs = {}\n    if not isinstance(create_env_fn, EnvBase):\n        env = create_env_fn(**create_env_kwargs)\n    else:\n        env = create_env_fn\n        if create_env_kwargs:\n            if not isinstance(env, BatchedEnvBase):\n                raise RuntimeError(f\"kwargs were passed to SyncDataCollector but they can't be set on environment of type {type(create_env_fn)}.\")\n            env.update_kwargs(create_env_kwargs)\n    if policy is None:\n        policy = RandomPolicy(env.full_action_spec)\n    if trust_policy is None:\n        trust_policy = isinstance(policy, (RandomPolicy, CudaGraphModule))\n    self.trust_policy = trust_policy\n    self._read_compile_kwargs(compile_policy, cudagraph_policy)\n    self._traj_pool_val = kwargs.pop('traj_pool', None)\n    if kwargs:\n        raise TypeError(f'Keys {list(kwargs.keys())} are unknown to {type(self).__name__}.')\n    storing_device, policy_device, env_device = self._get_devices(storing_device=storing_device, policy_device=policy_device, env_device=env_device, device=device)\n    self.storing_device = storing_device\n    if self.storing_device is not None and self.storing_device.type != 'cuda':\n        if torch.cuda.is_available():\n            self._sync_storage = torch.cuda.synchronize\n        elif torch.backends.mps.is_available() and hasattr(torch, 'mps'):\n            self._sync_storage = torch.mps.synchronize\n        elif self.storing_device.type == 'cpu':\n            self._sync_storage = _do_nothing\n        else:\n            raise RuntimeError('Non supported device')\n    else:\n        self._sync_storage = _do_nothing\n    self.env_device = env_device\n    if self.env_device is not None and self.env_device.type != 'cuda':\n        if torch.cuda.is_available():\n            self._sync_env = torch.cuda.synchronize\n        elif torch.backends.mps.is_available() and hasattr(torch, 'mps'):\n            self._sync_env = torch.mps.synchronize\n        elif self.env_device.type == 'cpu':\n            self._sync_env = _do_nothing\n        else:\n            raise RuntimeError('Non supported device')\n    else:\n        self._sync_env = _do_nothing\n    self.policy_device = policy_device\n    if self.policy_device is not None and self.policy_device.type != 'cuda':\n        if torch.cuda.is_available():\n            self._sync_policy = torch.cuda.synchronize\n        elif torch.backends.mps.is_available() and hasattr(torch, 'mps'):\n            self._sync_policy = torch.mps.synchronize\n        elif self.policy_device.type == 'cpu':\n            self._sync_policy = _do_nothing\n        else:\n            raise RuntimeError('Non supported device')\n    else:\n        self._sync_policy = _do_nothing\n    self.device = device\n    self._cast_to_policy_device = self.policy_device != self.env_device\n    self.env: EnvBase = env\n    del env\n    self.replay_buffer = replay_buffer\n    if self.replay_buffer is not None:\n        if postproc is not None:\n            raise TypeError('postproc must be None when a replay buffer is passed.')\n        if use_buffers:\n            raise TypeError('replay_buffer is exclusive with use_buffers.')\n    if use_buffers is None:\n        use_buffers = not self.env._has_dynamic_specs and self.replay_buffer is None\n    self._use_buffers = use_buffers\n    self.replay_buffer = replay_buffer\n    self.closed = False\n    if not reset_when_done:\n        raise ValueError('reset_when_done is deprectated.')\n    self.reset_when_done = reset_when_done\n    self.n_env = self.env.batch_size.numel()\n    self.policy, self.get_weights_fn = self._get_policy_and_device(policy=policy, observation_spec=self.env.observation_spec)\n    if isinstance(self.policy, nn.Module):\n        self.policy_weights = TensorDict.from_module(self.policy, as_module=True)\n    else:\n        self.policy_weights = TensorDict()\n    if self.compiled_policy:\n        self.policy = torch.compile(self.policy, **self.compiled_policy_kwargs)\n    if self.cudagraphed_policy:\n        self.policy = CudaGraphModule(self.policy, **self.cudagraphed_policy_kwargs)\n    if self.env_device:\n        self.env: EnvBase = self.env.to(self.env_device)\n    elif self.env.device is not None:\n        self.env_device = self.env.device\n    self._cast_to_env_device = self._cast_to_policy_device or self.env.device != self.storing_device\n    self.max_frames_per_traj = int(max_frames_per_traj) if max_frames_per_traj is not None else 0\n    if self.max_frames_per_traj is not None and self.max_frames_per_traj > 0:\n        for key in self.env.output_spec.keys(True, True):\n            if isinstance(key, str):\n                key = (key,)\n            if 'step_count' in key:\n                raise ValueError(\"A 'step_count' key is already present in the environment and the 'max_frames_per_traj' argument may conflict with a 'StepCounter' that has already been set. Possible solutions: Set max_frames_per_traj to 0 or remove the StepCounter limit from the environment transforms.\")\n        self.env = TransformedEnv(self.env, StepCounter(max_steps=self.max_frames_per_traj))\n    if total_frames is None or total_frames < 0:\n        total_frames = float('inf')\n    else:\n        remainder = total_frames % frames_per_batch\n        if remainder != 0 and RL_WARNINGS:\n            warnings.warn(f'total_frames ({total_frames}) is not exactly divisible by frames_per_batch ({frames_per_batch}).This means {frames_per_batch - remainder} additional frames will be collected.To silence this message, set the environment variable RL_WARNINGS to False.')\n    self.total_frames = int(total_frames) if total_frames != float('inf') else total_frames\n    self.reset_at_each_iter = reset_at_each_iter\n    self.init_random_frames = int(init_random_frames) if init_random_frames is not None else 0\n    if init_random_frames is not None and init_random_frames % frames_per_batch != 0 and RL_WARNINGS:\n        warnings.warn(f'init_random_frames ({init_random_frames}) is not exactly a multiple of frames_per_batch ({frames_per_batch}),  this results in more init_random_frames than requested ({-(-init_random_frames // frames_per_batch) * frames_per_batch}).To silence this message, set the environment variable RL_WARNINGS to False.')\n    self.postproc = postproc\n    if self.postproc is not None and hasattr(self.postproc, 'to') and self.storing_device:\n        self.postproc.to(self.storing_device)\n    if frames_per_batch % self.n_env != 0 and RL_WARNINGS:\n        warnings.warn(f'frames_per_batch ({frames_per_batch}) is not exactly divisible by the number of batched environments ({self.n_env}),  this results in more frames_per_batch per iteration that requested ({-(-frames_per_batch // self.n_env) * self.n_env}).To silence this message, set the environment variable RL_WARNINGS to False.')\n    self.requested_frames_per_batch = int(frames_per_batch)\n    self.frames_per_batch = -(-frames_per_batch // self.n_env)\n    self.exploration_type = exploration_type if exploration_type else DEFAULT_EXPLORATION_TYPE\n    self.return_same_td = return_same_td\n    self.set_truncated = set_truncated\n    self._make_shuttle()\n    if self._use_buffers:\n        self._make_final_rollout()\n    self._set_truncated_keys()\n    if split_trajs is None:\n        split_trajs = False\n    self.split_trajs = split_trajs\n    self._exclude_private_keys = True\n    self.interruptor = interruptor\n    self._frames = 0\n    self._iter = -1",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "update_policy_weights_",
                "args": [
                    {
                        "arg_name": "policy_weights",
                        "return_type": "Optional[TensorDictBase]",
                        "default_value": "None"
                    }
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    super().update_policy_weights_(policy_weights)",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "seed",
                        "return_type": "int",
                        "default_value": ""
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    \"\"\"Sets the seeds of the environments stored in the DataCollector.\n\n        Args:\n            seed (int): integer representing the seed to be used for the environment.\n            static_seed(bool, optional): if ``True``, the seed is not incremented.\n                Defaults to False\n\n        Returns:\n            Output seed. This is useful when more than one environment is contained in the DataCollector, as the\n            seed will be incremented for each of these. The resulting seed is the seed of the last environment.\n\n        Examples:\n            >>> from torchrl.envs import ParallelEnv\n            >>> from torchrl.envs.libs.gym import GymEnv\n            >>> from tensordict.nn import TensorDictModule\n            >>> from torch import nn\n            >>> env_fn = lambda: GymEnv(\"Pendulum-v1\")\n            >>> env_fn_parallel = ParallelEnv(6, env_fn)\n            >>> policy = TensorDictModule(nn.Linear(3, 1), in_keys=[\"observation\"], out_keys=[\"action\"])\n            >>> collector = SyncDataCollector(env_fn_parallel, policy, total_frames=300, frames_per_batch=100)\n            >>> out_seed = collector.set_seed(1)  # out_seed = 6\n\n        \"\"\"\n    out = self.env.set_seed(seed, static_seed=static_seed)\n    return out",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "def iterator(self) -> Iterator[TensorDictBase]:\n    \"\"\"Iterates through the DataCollector.\n\n        Yields: TensorDictBase objects containing (chunks of) trajectories\n\n        \"\"\"\n    if self.storing_device and self.storing_device.type == 'cuda':\n        stream = torch.cuda.Stream(self.storing_device, priority=-1)\n        event = stream.record_event()\n        streams = [stream]\n        events = [event]\n    elif self.storing_device is None:\n        streams = []\n        events = []\n        cuda_devices = set()\n\n        def cuda_check(tensor: torch.Tensor):\n            if tensor.is_cuda:\n                cuda_devices.add(tensor.device)\n        if not self._use_buffers:\n            for spec in self.env.specs.values(True, True):\n                if spec.device.type == 'cuda':\n                    if ':' not in str(spec.device):\n                        raise RuntimeError(\"A cuda spec did not have a device associated. Make sure to pass `'cuda:device_num'` to each spec device.\")\n                    cuda_devices.add(spec.device)\n        else:\n            self._final_rollout.apply(cuda_check, filter_empty=True)\n        for device in cuda_devices:\n            streams.append(torch.cuda.Stream(device, priority=-1))\n            events.append(streams[-1].record_event())\n    else:\n        streams = []\n        events = []\n    with contextlib.ExitStack() as stack:\n        for stream in streams:\n            stack.enter_context(torch.cuda.stream(stream))\n        while self._frames < self.total_frames:\n            self._iter += 1\n            tensordict_out = self.rollout()\n            if tensordict_out is None:\n                yield\n                continue\n            self._increment_frames(tensordict_out.numel())\n            if self.split_trajs:\n                tensordict_out = split_trajectories(tensordict_out, prefix='collector')\n            if self.postproc is not None:\n                tensordict_out = self.postproc(tensordict_out)\n            if self._exclude_private_keys:\n\n                def is_private(key):\n                    if isinstance(key, str) and key.startswith('_'):\n                        return True\n                    if isinstance(key, tuple) and any((_key.startswith('_') for _key in key)):\n                        return True\n                    return False\n                excluded_keys = [key for key in tensordict_out.keys(True) if is_private(key)]\n                tensordict_out = tensordict_out.exclude(*excluded_keys, inplace=True)\n            if self.return_same_td:\n                if events:\n                    for event in events:\n                        event.record()\n                        event.synchronize()\n                yield tensordict_out\n            else:\n                yield tensordict_out.clone()",
                "description": ""
            },
            {
                "function_name": "rollout",
                "args": [],
                "signature": "rollout(self) -> TensorDictBase",
                "function_code": "@torch.no_grad()\ndef rollout(self) -> TensorDictBase:\n    \"\"\"Computes a rollout in the environment using the provided policy.\n\n        Returns:\n            TensorDictBase containing the computed rollout.\n\n        \"\"\"\n    if self.reset_at_each_iter:\n        self._shuttle.update(self.env.reset())\n    if self._use_buffers:\n        self._final_rollout.fill_(('collector', 'traj_ids'), -1)\n    else:\n        pass\n    tensordicts = []\n    with set_exploration_type(self.exploration_type):\n        for t in range(self.frames_per_batch):\n            if self.init_random_frames is not None and self._frames < self.init_random_frames:\n                self.env.rand_action(self._shuttle)\n            else:\n                if self._cast_to_policy_device:\n                    if self.policy_device is not None:\n                        policy_input = self._shuttle.to(self.policy_device, non_blocking=True)\n                        self._sync_policy()\n                    elif self.policy_device is None:\n                        policy_input = self._shuttle\n                else:\n                    policy_input = self._shuttle\n                policy_output = self.policy(policy_input)\n                if self._shuttle is not policy_output:\n                    self._shuttle.update(policy_output, keys_to_update=self._policy_output_keys)\n            if self._cast_to_env_device:\n                if self.env_device is not None:\n                    env_input = self._shuttle.to(self.env_device, non_blocking=True)\n                    self._sync_env()\n                elif self.env_device is None:\n                    env_input = self._shuttle\n            else:\n                env_input = self._shuttle\n            env_output, env_next_output = self.env.step_and_maybe_reset(env_input)\n            if self._shuttle is not env_output:\n                next_data = env_output.get('next')\n                if self._shuttle_has_no_device:\n                    next_data.clear_device_()\n                self._shuttle.set('next', next_data)\n            if self.replay_buffer is not None:\n                self.replay_buffer.add(self._shuttle)\n                if self._increment_frames(self._shuttle.numel()):\n                    return\n            elif self.storing_device is not None:\n                tensordicts.append(self._shuttle.to(self.storing_device, non_blocking=True))\n                self._sync_storage()\n            else:\n                tensordicts.append(self._shuttle)\n            collector_data = self._shuttle.get('collector').copy()\n            self._shuttle = env_next_output\n            if self._shuttle_has_no_device:\n                self._shuttle.clear_device_()\n            self._shuttle.set('collector', collector_data)\n            self._update_traj_ids(env_output)\n            if self.interruptor is not None and self.interruptor.collection_stopped():\n                if self.replay_buffer is not None:\n                    return\n                result = self._final_rollout\n                if self._use_buffers:\n                    try:\n                        torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout[..., :t + 1])\n                    except RuntimeError:\n                        with self._final_rollout.unlock_():\n                            torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout[..., :t + 1])\n                else:\n                    result = TensorDict.maybe_dense_stack(tensordicts, dim=-1)\n                break\n        else:\n            if self._use_buffers:\n                result = self._final_rollout\n                try:\n                    result = torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout)\n                except RuntimeError:\n                    with self._final_rollout.unlock_():\n                        result = torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout)\n            elif self.replay_buffer is not None:\n                return\n            else:\n                result = TensorDict.maybe_dense_stack(tensordicts, dim=-1)\n                result.refine_names(..., 'time')\n    return self._maybe_set_truncated(result)",
                "description": ""
            },
            {
                "function_name": "reset",
                "args": [
                    {
                        "arg_name": "index",
                        "return_type": "",
                        "default_value": "None"
                    }
                ],
                "signature": "reset(self, index=None, **kwargs) -> None",
                "function_code": "@torch.no_grad()\ndef reset(self, index=None, **kwargs) -> None:\n    \"\"\"Resets the environments to a new initial state.\"\"\"\n    collector_metadata = self._shuttle.get('collector').clone()\n    if index is not None:\n        if prod(self.env.batch_size) == 0:\n            raise RuntimeError('resetting unique env with index is not permitted.')\n        for reset_key, done_keys in zip(self.env.reset_keys, self.env.done_keys_groups):\n            _reset = torch.zeros(self.env.full_done_spec[done_keys[0]].shape, dtype=torch.bool, device=self.env.device)\n            _reset[index] = 1\n            self._shuttle.set(reset_key, _reset)\n    else:\n        _reset = None\n        self._shuttle.zero_()\n    self._shuttle.update(self.env.reset(**kwargs), inplace=True)\n    collector_metadata['traj_ids'] = collector_metadata['traj_ids'] - collector_metadata['traj_ids'].min()\n    self._shuttle['collector'] = collector_metadata",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [],
                "signature": "shutdown(self) -> None",
                "function_code": "def shutdown(self) -> None:\n    \"\"\"Shuts down all workers and/or closes the local environment.\"\"\"\n    if not self.closed:\n        self.closed = True\n        del self._shuttle\n        if self._use_buffers:\n            del self._final_rollout\n        if not self.env.is_closed:\n            self.env.close()\n        del self.env\n    return",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    \"\"\"Returns the local state_dict of the data collector (environment and policy).\n\n        Returns:\n            an ordered dictionary with fields :obj:`\"policy_state_dict\"` and\n            `\"env_state_dict\"`.\n\n        \"\"\"\n    from torchrl.envs.batched_envs import BatchedEnvBase\n    if isinstance(self.env, TransformedEnv):\n        env_state_dict = self.env.transform.state_dict()\n    elif isinstance(self.env, BatchedEnvBase):\n        env_state_dict = self.env.state_dict()\n    else:\n        env_state_dict = OrderedDict()\n    if hasattr(self.policy, 'state_dict'):\n        policy_state_dict = self.policy.state_dict()\n        state_dict = OrderedDict(policy_state_dict=policy_state_dict, env_state_dict=env_state_dict)\n    else:\n        state_dict = OrderedDict(env_state_dict=env_state_dict)\n    state_dict.update({'frames': self._frames, 'iter': self._iter})\n    return state_dict",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict, **kwargs) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict, **kwargs) -> None:\n    \"\"\"Loads a state_dict on the environment and policy.\n\n        Args:\n            state_dict (OrderedDict): ordered dictionary containing the fields\n                `\"policy_state_dict\"` and :obj:`\"env_state_dict\"`.\n\n        \"\"\"\n    strict = kwargs.get('strict', True)\n    if strict or 'env_state_dict' in state_dict:\n        self.env.load_state_dict(state_dict['env_state_dict'], **kwargs)\n    if strict or 'policy_state_dict' in state_dict:\n        self.policy.load_state_dict(state_dict['policy_state_dict'], **kwargs)\n    self._frames = state_dict['frames']\n    self._iter = state_dict['iter']",
                "description": ""
            }
        ]
    },
    {
        "class_name": "MultiSyncDataCollector",
        "bases": [
            "_MultiDataCollector"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "next",
                "args": [],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [],
                "signature": "shutdown(self)",
                "function_code": "def shutdown(self):\n    if hasattr(self, 'out_buffer'):\n        del self.out_buffer\n    if hasattr(self, 'buffers'):\n        del self.buffers\n    return super().shutdown()",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "seed",
                        "return_type": "int",
                        "default_value": ""
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    return super().set_seed(seed, static_seed)",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    return super().state_dict()",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict) -> None:\n    return super().load_state_dict(state_dict)",
                "description": ""
            },
            {
                "function_name": "update_policy_weights_",
                "args": [
                    {
                        "arg_name": "policy_weights",
                        "return_type": "Optional[TensorDictBase]",
                        "default_value": "None"
                    }
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    super().update_policy_weights_(policy_weights)",
                "description": ""
            },
            {
                "function_name": "frames_per_batch_worker",
                "args": [],
                "signature": "frames_per_batch_worker(self)",
                "function_code": "@property\ndef frames_per_batch_worker(self):\n    if self.requested_frames_per_batch % self.num_workers != 0 and RL_WARNINGS:\n        warnings.warn(f'frames_per_batch {self.requested_frames_per_batch} is not exactly divisible by the number of collector workers {self.num_workers}, this results in more frames_per_batch per iteration that requested.To silence this message, set the environment variable RL_WARNINGS to False.')\n    frames_per_batch_worker = -(-self.requested_frames_per_batch // self.num_workers)\n    return frames_per_batch_worker",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "def iterator(self) -> Iterator[TensorDictBase]:\n    cat_results = self.cat_results\n    if cat_results is None:\n        cat_results = 'stack'\n    self.buffers = {}\n    dones = [False for _ in range(self.num_workers)]\n    workers_frames = [0 for _ in range(self.num_workers)]\n    same_device = None\n    self.out_buffer = None\n    preempt = self.interruptor is not None and self.preemptive_threshold < 1.0\n    while not all(dones) and self._frames < self.total_frames:\n        _check_for_faulty_process(self.procs)\n        if self.update_at_each_batch:\n            self.update_policy_weights_()\n        for idx in range(self.num_workers):\n            if self.init_random_frames is not None and self._frames < self.init_random_frames:\n                msg = 'continue_random'\n            else:\n                msg = 'continue'\n            self.pipes[idx].send((None, msg))\n        self._iter += 1\n        if preempt:\n            self.interruptor.start_collection()\n            while self.queue_out.qsize() < int(self.num_workers * self.preemptive_threshold):\n                continue\n            self.interruptor.stop_collection()\n            while self.queue_out.qsize() < int(self.num_workers):\n                continue\n        for _ in range(self.num_workers):\n            new_data, j = self.queue_out.get()\n            use_buffers = self._use_buffers\n            if self.replay_buffer is not None:\n                idx = new_data\n                workers_frames[idx] = workers_frames[idx] + self.frames_per_batch_worker\n                continue\n            elif j == 0 or not use_buffers:\n                try:\n                    data, idx = new_data\n                    self.buffers[idx] = data\n                    if use_buffers is None and j > 0:\n                        self._use_buffers = False\n                except TypeError:\n                    if use_buffers is None:\n                        self._use_buffers = True\n                        idx = new_data\n                    else:\n                        raise\n            else:\n                idx = new_data\n            if preempt:\n                if cat_results != 'stack':\n                    buffers = {}\n                    for idx, buffer in self.buffers.items():\n                        valid = buffer.get(('collector', 'traj_ids')) != -1\n                        if valid.ndim > 2:\n                            valid = valid.flatten(0, -2)\n                        if valid.ndim == 2:\n                            valid = valid.any(0)\n                        buffers[idx] = buffer[..., valid]\n                else:\n                    for buffer in self.buffers.values():\n                        with buffer.unlock_():\n                            buffer.set(('collector', 'mask'), buffer.get(('collector', 'traj_ids')) != -1)\n                    buffers = self.buffers\n            else:\n                buffers = self.buffers\n            workers_frames[idx] = workers_frames[idx] + buffers[idx].numel()\n            if workers_frames[idx] >= self.total_frames:\n                dones[idx] = True\n        if self.replay_buffer is not None:\n            yield\n            self._frames += self.frames_per_batch_worker * self.num_workers\n            continue\n        n_collected = 0\n        for idx in range(self.num_workers):\n            buffer = buffers[idx]\n            traj_ids = buffer.get(('collector', 'traj_ids'))\n            if preempt:\n                if cat_results == 'stack':\n                    mask_frames = buffer.get(('collector', 'traj_ids')) != -1\n                    n_collected += mask_frames.sum().cpu()\n                else:\n                    n_collected += traj_ids.numel()\n            else:\n                n_collected += traj_ids.numel()\n        if same_device is None:\n            prev_device = None\n            same_device = True\n            for item in self.buffers.values():\n                if prev_device is None:\n                    prev_device = item.device\n                else:\n                    same_device = same_device and item.device == prev_device\n        if cat_results == 'stack':\n            stack = torch.stack if self._use_buffers else TensorDict.maybe_dense_stack\n            if same_device:\n                self.out_buffer = stack(list(buffers.values()), 0)\n            else:\n                self.out_buffer = stack([item.cpu() for item in buffers.values()], 0)\n        else:\n            if self._use_buffers is None:\n                torchrl_logger.warning('use_buffer not specified and not yet inferred from data, assuming `True`.')\n            elif not self._use_buffers:\n                raise RuntimeError('Cannot concatenate results with use_buffers=False')\n            try:\n                if same_device:\n                    self.out_buffer = torch.cat(list(buffers.values()), cat_results)\n                else:\n                    self.out_buffer = torch.cat([item.cpu() for item in buffers.values()], cat_results)\n            except RuntimeError as err:\n                if preempt and cat_results != -1 and ('Sizes of tensors must match' in str(err)):\n                    raise RuntimeError(\"The value provided to cat_results isn't compatible with the collectors outputs. Consider using `cat_results=-1`.\")\n                raise\n        if self.split_trajs:\n            out = split_trajectories(self.out_buffer, prefix='collector')\n        else:\n            out = self.out_buffer\n        if cat_results in (-1, 'stack'):\n            out.refine_names(*[None] * (out.ndim - 1) + ['time'])\n        self._frames += n_collected\n        if self.postprocs:\n            self.postprocs = self.postprocs.to(out.device)\n            out = self.postprocs(out)\n        if self._exclude_private_keys:\n            excluded_keys = [key for key in out.keys() if key.startswith('_')]\n            if excluded_keys:\n                out = out.exclude(*excluded_keys)\n        yield out\n        del out\n    del self.buffers\n    self.out_buffer = None",
                "description": ""
            }
        ]
    },
    {
        "class_name": "MultiaSyncDataCollector",
        "bases": [
            "_MultiDataCollector"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "__init__",
                "args": [],
                "signature": "__init__(self, *args, **kwargs)",
                "function_code": "def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.out_tensordicts = defaultdict(lambda: None)\n    self.running = False\n    if self.postprocs is not None:\n        postproc = self.postprocs\n        self.postprocs = {}\n        for _device in self.storing_device:\n            if _device not in self.postprocs:\n                self.postprocs[_device] = deepcopy(postproc).to(_device)",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [],
                "signature": "shutdown(self)",
                "function_code": "def shutdown(self):\n    if hasattr(self, 'out_tensordicts'):\n        del self.out_tensordicts\n    return super().shutdown()",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "seed",
                        "return_type": "int",
                        "default_value": ""
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    return super().set_seed(seed, static_seed)",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    return super().state_dict()",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict) -> None:\n    return super().load_state_dict(state_dict)",
                "description": ""
            },
            {
                "function_name": "update_policy_weights_",
                "args": [
                    {
                        "arg_name": "policy_weights",
                        "return_type": "Optional[TensorDictBase]",
                        "default_value": "None"
                    }
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    super().update_policy_weights_(policy_weights)",
                "description": ""
            },
            {
                "function_name": "frames_per_batch_worker",
                "args": [],
                "signature": "frames_per_batch_worker(self)",
                "function_code": "@property\ndef frames_per_batch_worker(self):\n    return self.requested_frames_per_batch",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "def iterator(self) -> Iterator[TensorDictBase]:\n    if self.update_at_each_batch:\n        self.update_policy_weights_()\n    for i in range(self.num_workers):\n        if self.init_random_frames is not None and self.init_random_frames > 0:\n            self.pipes[i].send((None, 'continue_random'))\n        else:\n            self.pipes[i].send((None, 'continue'))\n    self.running = True\n    workers_frames = [0 for _ in range(self.num_workers)]\n    while self._frames < self.total_frames:\n        self._iter += 1\n        while True:\n            try:\n                idx, j, out = self._get_from_queue(timeout=10.0)\n                break\n            except TimeoutError:\n                _check_for_faulty_process(self.procs)\n        if self.replay_buffer is None:\n            worker_frames = out.numel()\n            if self.split_trajs:\n                out = split_trajectories(out, prefix='collector')\n        else:\n            worker_frames = self.frames_per_batch_worker\n        self._frames += worker_frames\n        workers_frames[idx] = workers_frames[idx] + worker_frames\n        if self.postprocs:\n            out = self.postprocs[out.device](out)\n        if self.init_random_frames is not None and self._frames < self.init_random_frames:\n            msg = 'continue_random'\n        else:\n            msg = 'continue'\n        self.pipes[idx].send((idx, msg))\n        if out is not None and self._exclude_private_keys:\n            excluded_keys = [key for key in out.keys() if key.startswith('_')]\n            out = out.exclude(*excluded_keys)\n        yield out\n    self.running = False",
                "description": ""
            },
            {
                "function_name": "reset",
                "args": [
                    {
                        "arg_name": "reset_idx",
                        "return_type": "Optional[Sequence[bool]]",
                        "default_value": "None"
                    }
                ],
                "signature": "reset(self, reset_idx: Optional[Sequence[bool]]=None) -> None",
                "function_code": "def reset(self, reset_idx: Optional[Sequence[bool]]=None) -> None:\n    super().reset(reset_idx)\n    if self.queue_out.full():\n        time.sleep(_TIMEOUT)\n    if self.queue_out.full():\n        raise Exception('self.queue_out is full')\n    if self.running:\n        for idx in range(self.num_workers):\n            if self.init_random_frames is not None and self._frames < self.init_random_frames:\n                self.pipes[idx].send((idx, 'continue_random'))\n            else:\n                self.pipes[idx].send((idx, 'continue'))",
                "description": ""
            }
        ]
    },
    {
        "class_name": "aSyncDataCollector",
        "bases": [
            "MultiaSyncDataCollector"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "__init__",
                "args": [
                    {
                        "arg_name": "create_env_fn",
                        "return_type": "Callable[[], EnvBase]",
                        "default_value": ""
                    },
                    {
                        "arg_name": "policy",
                        "return_type": "Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]",
                        "default_value": ""
                    },
                    {
                        "arg_name": "frames_per_batch",
                        "return_type": "int",
                        "default_value": ""
                    },
                    {
                        "arg_name": "total_frames",
                        "return_type": "Optional[int]",
                        "default_value": "-1"
                    },
                    {
                        "arg_name": "device",
                        "return_type": "DEVICE_TYPING | Sequence[DEVICE_TYPING] | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "storing_device",
                        "return_type": "DEVICE_TYPING | Sequence[DEVICE_TYPING] | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "env_device",
                        "return_type": "DEVICE_TYPING | Sequence[DEVICE_TYPING] | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "policy_device",
                        "return_type": "DEVICE_TYPING | Sequence[DEVICE_TYPING] | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "create_env_kwargs",
                        "return_type": "Optional[Sequence[dict]]",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "max_frames_per_traj",
                        "return_type": "int | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "init_random_frames",
                        "return_type": "int | None",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "reset_at_each_iter",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "postproc",
                        "return_type": "Optional[Callable[[TensorDictBase], TensorDictBase]]",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "split_trajs",
                        "return_type": "Optional[bool]",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "exploration_type",
                        "return_type": "ExplorationType",
                        "default_value": "DEFAULT_EXPLORATION_TYPE"
                    },
                    {
                        "arg_name": "reset_when_done",
                        "return_type": "bool",
                        "default_value": "True"
                    },
                    {
                        "arg_name": "update_at_each_batch",
                        "return_type": "bool",
                        "default_value": "False"
                    },
                    {
                        "arg_name": "preemptive_threshold",
                        "return_type": "float",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "num_threads",
                        "return_type": "int",
                        "default_value": "None"
                    },
                    {
                        "arg_name": "num_sub_threads",
                        "return_type": "int",
                        "default_value": "1"
                    },
                    {
                        "arg_name": "set_truncated",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "__init__(self, create_env_fn: Callable[[], EnvBase], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]], *, frames_per_batch: int, total_frames: Optional[int]=-1, device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, storing_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, env_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, policy_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, create_env_kwargs: Optional[Sequence[dict]]=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Optional[Callable[[TensorDictBase], TensorDictBase]]=None, split_trajs: Optional[bool]=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, reset_when_done: bool=True, update_at_each_batch: bool=False, preemptive_threshold: float=None, num_threads: int=None, num_sub_threads: int=1, set_truncated: bool=False, **kwargs)",
                "function_code": "def __init__(self, create_env_fn: Callable[[], EnvBase], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]], *, frames_per_batch: int, total_frames: Optional[int]=-1, device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, storing_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, env_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, policy_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, create_env_kwargs: Optional[Sequence[dict]]=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Optional[Callable[[TensorDictBase], TensorDictBase]]=None, split_trajs: Optional[bool]=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, reset_when_done: bool=True, update_at_each_batch: bool=False, preemptive_threshold: float=None, num_threads: int=None, num_sub_threads: int=1, set_truncated: bool=False, **kwargs):\n    super().__init__(create_env_fn=[create_env_fn], policy=policy, total_frames=total_frames, create_env_kwargs=[create_env_kwargs], max_frames_per_traj=max_frames_per_traj, frames_per_batch=frames_per_batch, reset_at_each_iter=reset_at_each_iter, init_random_frames=init_random_frames, postproc=postproc, split_trajs=split_trajs, device=device, policy_device=policy_device, env_device=env_device, storing_device=storing_device, exploration_type=exploration_type, reset_when_done=reset_when_done, update_at_each_batch=update_at_each_batch, preemptive_threshold=preemptive_threshold, num_threads=num_threads, num_sub_threads=num_sub_threads, set_truncated=set_truncated)",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [],
                "signature": "shutdown(self)",
                "function_code": "def shutdown(self):\n    return super().shutdown()",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "seed",
                        "return_type": "int",
                        "default_value": ""
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False"
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    return super().set_seed(seed, static_seed)",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    return super().state_dict()",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict) -> None:\n    return super().load_state_dict(state_dict)",
                "description": ""
            }
        ]
    }
]
2024-12-03 21:45:15
utils.py: [
    {
        "function_name": "split_trajectories",
        "args": [
            {
                "arg_name": "rollout_tensordict",
                "return_type": "TensorDictBase",
                "default_value": ""
            },
            {
                "arg_name": "prefix",
                "return_type": "",
                "default_value": "None"
            },
            {
                "arg_name": "trajectory_key",
                "return_type": "NestedKey | None",
                "default_value": "None"
            },
            {
                "arg_name": "done_key",
                "return_type": "NestedKey | None",
                "default_value": "None"
            },
            {
                "arg_name": "as_nested",
                "return_type": "bool",
                "default_value": "False"
            }
        ],
        "signature": "split_trajectories(rollout_tensordict: TensorDictBase, *, prefix=None, trajectory_key: NestedKey | None=None, done_key: NestedKey | None=None, as_nested: bool=False) -> TensorDictBase",
        "function_code": "@set_lazy_legacy(False)\ndef split_trajectories(rollout_tensordict: TensorDictBase, *, prefix=None, trajectory_key: NestedKey | None=None, done_key: NestedKey | None=None, as_nested: bool=False) -> TensorDictBase:\n    \"\"\"A util function for trajectory separation.\n\n    Takes a tensordict with a key traj_ids that indicates the id of each trajectory.\n\n    From there, builds a B x T x ... zero-padded tensordict with B batches on max duration T\n\n    Args:\n        rollout_tensordict (TensorDictBase): a rollout with adjacent trajectories\n            along the last dimension.\n\n    Keyword Args:\n        prefix (NestedKey, optional): the prefix used to read and write meta-data,\n            such as ``\"traj_ids\"`` (the optional integer id of each trajectory)\n            and the ``\"mask\"`` entry indicating which data are valid and which\n            aren't. Defaults to ``\"collector\"`` if the input has a ``\"collector\"``\n            entry, ``()`` (no prefix) otherwise.\n            ``prefix`` is kept as a legacy feature and will be deprecated eventually.\n            Prefer ``trajectory_key`` or ``done_key`` whenever possible.\n        trajectory_key (NestedKey, optional): the key pointing to the trajectory\n            ids. Supersedes ``done_key`` and ``prefix``. If not provided, defaults\n            to ``(prefix, \"traj_ids\")``.\n        done_key (NestedKey, optional): the key pointing to the ``\"done\"\"`` signal,\n            if the trajectory could not be directly recovered. Defaults to ``\"done\"``.\n        as_nested (bool or torch.layout, optional): whether to return the results as nested\n            tensors. Defaults to ``False``. If a ``torch.layout`` is provided, it will be used\n            to construct the nested tensor, otherwise the default layout will be used.\n\n            .. note:: Using ``split_trajectories(tensordict, as_nested=True).to_padded_tensor(mask=mask_key)``\n                should result in the exact same result as ``as_nested=False``. Since this is an experimental\n                feature and relies on nested_tensors, which API may change in the future, we made this\n                an optional feature. The runtime should be faster with ``as_nested=True``.\n\n            .. note:: Providing a layout lets the user control whether the nested tensor is to be used\n                with ``torch.strided`` or ``torch.jagged`` layout. While the former has slightly more\n                capabilities at the time of writing, the second will be the main focus of the PyTorch team\n                in the future due to its better compatibility with :func:`~torch.compile`.\n\n    Returns:\n        A new tensordict with a leading dimension corresponding to the trajectory.\n        A ``\"mask\"`` boolean entry sharing the ``trajectory_key`` prefix\n        and the tensordict shape is also added. It indicated the valid elements of the tensordict,\n        as well as a ``\"traj_ids\"`` entry if ``trajectory_key`` could not be found.\n\n    Examples:\n        >>> from tensordict import TensorDict\n        >>> import torch\n        >>> from torchrl.collectors.utils import split_trajectories\n        >>> obs = torch.cat([torch.arange(10), torch.arange(5)])\n        >>> obs_ = torch.cat([torch.arange(1, 11), torch.arange(1, 6)])\n        >>> done = torch.zeros(15, dtype=torch.bool)\n        >>> done[9] = True\n        >>> trajectory_id = torch.cat([torch.zeros(10, dtype=torch.int32),\n        ...     torch.ones(5, dtype=torch.int32)])\n        >>> data = TensorDict({\"obs\": obs, (\"next\", \"obs\"): obs_, (\"next\", \"done\"): done, \"trajectory\": trajectory_id}, batch_size=[15])\n        >>> data_split = split_trajectories(data, done_key=\"done\")\n        >>> print(data_split)\n        TensorDict(\n            fields={\n                mask: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                next: TensorDict(\n                    fields={\n                        done: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                        obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False)},\n                    batch_size=torch.Size([2, 10]),\n                    device=None,\n                    is_shared=False),\n                obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n                traj_ids: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n                trajectory: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int32, is_shared=False)},\n            batch_size=torch.Size([2, 10]),\n            device=None,\n            is_shared=False)\n        >>> # check that split_trajectories got the trajectories right with the done signal\n        >>> assert (data_split[\"traj_ids\"] == data_split[\"trajectory\"]).all()\n        >>> print(data_split[\"mask\"])\n        tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n                [ True,  True,  True,  True,  True, False, False, False, False, False]])\n        >>> data_split = split_trajectories(data, trajectory_key=\"trajectory\")\n        >>> print(data_split)\n        TensorDict(\n            fields={\n                mask: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                next: TensorDict(\n                    fields={\n                        done: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                        obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False)},\n                    batch_size=torch.Size([2, 10]),\n                    device=None,\n                    is_shared=False),\n                obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n                trajectory: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int32, is_shared=False)},\n            batch_size=torch.Size([2, 10]),\n            device=None,\n            is_shared=False)\n\n    \"\"\"\n    mask_key = None\n    if trajectory_key is not None:\n        from torchrl.envs.utils import _replace_last\n        traj_ids_key = trajectory_key\n        mask_key = _replace_last(trajectory_key, 'mask')\n    else:\n        if prefix is None and 'collector' in rollout_tensordict.keys():\n            prefix = 'collector'\n        if prefix is None:\n            traj_ids_key = 'traj_ids'\n            mask_key = 'mask'\n        else:\n            traj_ids_key = (prefix, 'traj_ids')\n            mask_key = (prefix, 'mask')\n    rollout_tensordict = rollout_tensordict.copy()\n    traj_ids = rollout_tensordict.get(traj_ids_key, None)\n    if traj_ids is None:\n        if done_key is None:\n            done_key = 'done'\n        done_key = ('next', done_key)\n        done = rollout_tensordict.get(done_key)\n        idx = (slice(None),) * (rollout_tensordict.ndim - 1) + (slice(None, -1),)\n        done_sel = done[idx]\n        pads = [1, 0]\n        pads = [0, 0] * (done.ndim - rollout_tensordict.ndim) + pads\n        done_sel = torch.nn.functional.pad(done_sel, pads)\n        if done_sel.shape != done.shape:\n            raise RuntimeError(f'done and done_sel have different shape {done.shape} - {done_sel.shape} ')\n        traj_ids = done_sel.cumsum(rollout_tensordict.ndim - 1)\n        traj_ids = traj_ids.squeeze(-1)\n        if rollout_tensordict.ndim > 1:\n            for i in range(1, rollout_tensordict.shape[0]):\n                traj_ids[i] += traj_ids[i - 1].max() + 1\n        rollout_tensordict.set(traj_ids_key, traj_ids)\n    splits = traj_ids.reshape(-1)\n    splits = [(splits == i).sum().item() for i in splits.unique_consecutive()]\n    if len(set(splits)) == 1 and splits[0] == traj_ids.shape[-1]:\n        rollout_tensordict.set(mask_key, torch.ones(rollout_tensordict.shape, device=rollout_tensordict.device, dtype=torch.bool))\n        if rollout_tensordict.ndimension() == 1:\n            rollout_tensordict = rollout_tensordict.unsqueeze(0)\n        return rollout_tensordict\n    out_splits = rollout_tensordict.reshape(-1)\n    if as_nested:\n        if hasattr(torch, '_nested_compute_contiguous_strides_offsets'):\n\n            def nest(x, splits=splits):\n                shape = torch.tensor([[int(split), *x.shape[1:]] for split in splits])\n                return torch._nested_view_from_buffer(x.reshape(-1), shape, *torch._nested_compute_contiguous_strides_offsets(shape))\n            return out_splits._fast_apply(nest, batch_size=[len(splits), -1])\n        else:\n            out_splits = out_splits.split(splits, 0)\n            layout = as_nested if as_nested is not bool else None\n            if torch.__version__ < '2.4':\n                if layout not in (True,):\n                    raise RuntimeError(f'layout={layout} is only available for torch>=v2.4')\n\n                def nest(*x):\n                    return torch.nested.nested_tensor(list(x))\n            else:\n\n                def nest(*x):\n                    return torch.nested.nested_tensor(list(x), layout=layout)\n            return out_splits[0]._fast_apply(nest, *out_splits[1:], batch_size=[len(out_splits), *out_splits[0].batch_size[:-1], -1])\n    out_splits = out_splits.split(splits, 0)\n    for out_split in out_splits:\n        out_split.set(mask_key, torch.ones(out_split.shape, dtype=torch.bool, device=out_split.device))\n    if len(out_splits) > 1:\n        MAX = max(*[out_split.shape[0] for out_split in out_splits])\n    else:\n        MAX = out_splits[0].shape[0]\n    td = torch.stack([pad(out_split, [0, MAX - out_split.shape[0]]) for out_split in out_splits], 0)\n    return td",
        "description": ""
    }
]
2024-12-03 22:13:58
function recursive_map_to_cpu description: 
[blue]<purpose>
Generate a description for the function "recursive_map_to_cpu" based on the instructions, example output, and contextual content.
</purpose>

<instructions>
    <instruction>Describe the purpose and functionality of the function: "recursive_map_to_cpu"</instruction>
    <instruction>Start the "class" or "function" description with: "This function is "</instruction>
</instructions>

<example-output>
    <function-description-output>This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.</function-description-output>
    <arg-description-output>(bool) - Whether the static seed is used. (default: False)</arg-description-output>
    <arg-description-output>(int) - The random seed to be used. (default: None)</arg-description-output>
</example-output>

<content>
For context, use this information to help you:
def recursive_map_to_cpu(dictionary: OrderedDict) -> OrderedDict:
    """Maps the tensors to CPU through a nested dictionary."""
    return OrderedDict(**{k: recursive_map_to_cpu(item) if isinstance(item, OrderedDict) else item.cpu() if isinstance(item, torch.Tensor) else item for k, item in dictionary.items()})
</content>

<description>
This function is used to map tensors to the CPU through a nested dictionary. It recursively traverses the input dictionary and applies the `cpu()` method to any PyTorch tensors it encounters, while leaving other items unchanged. The resulting dictionary contains all elements from the original dictionary, but with all tensors converted to their CPU representations. This function is particularly useful in the context of multi-agent reinforcement learning models, where efficient training requires minimizing computational overhead on the GPU. By using this function, developers can ensure that their models are properly optimized for CPU execution and take advantage of the lower latency and higher throughput offered by modern CPUs.[/blue]
2024-12-03 22:15:33
parameter recursive_map_to_cpu description: 
[blue]<purpose>
Generate a description for the "recursive_map_to_cpu" parameter based on the instructions, example output, and contextual content.
</purpose>

<instructions>
    <instruction>Describe the purpose and functionality of the parameter: "recursive_map_to_cpu"</instruction>
    <instruction>Start the "class" or "function" description with: "This parameter is "</instruction>
</instructions>

<example-output>
    <class-desription-output>This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models.</class-description-output>
    <function-description-output>This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.</function-description-output>
    <arg-description-output>(bool) - Whether the static seed is used. (default: False)</arg-description-output>
    <arg-description-output>(int) - The random seed to be used. (default: None)</arg-description-output>
</example-output>

<content>
For context, use this information to help you:
recursive_map_to_cpu(dictionary: OrderedDict) -> OrderedDict

This parameter is a function that takes an ordered dictionary as input and returns the same dictionary with its elements mapped to CPU resources. The purpose of this function is likely to optimize the usage of computational resources in a multi-agent environment by offloading computations from the GPU to the CPU, thereby improving performance and efficiency.

By default, the function does not use static seeds, but it can be configured to do so by passing a boolean value indicating whether to use a static seed. Additionally, the function takes an optional integer argument representing the random seed to be used for reproducibility purposes.[/blue]
2024-12-03 22:17:21
class DataCollectorBase description: 
[blue]<purpose>
Generate a description for the "DataCollectorBase" class based on the instructions, example output, and contextual content.
</purpose>

<instructions>
    <instruction>Describe the purpose and functionality of the class: "DataCollectorBase"</instruction>
    <instruction>Start the "class" or "function" description with: "This class is "</instruction>
</instructions>

<example-output>
    <class-description>This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models.</class-description>
    <function-description>This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.</function-description>
    <argument-description>(bool) - Whether the static seed is used. (default: False)</argument-description>
    <argument-description>(int) - The random seed to be used. (default: None)</argument-description>
</example-output>

<content>
For context, this class is part of a larger framework for training multi-agent reinforcement learning models. It provides a standardized way to collect and store experiences from multiple agents in parallel, allowing for efficient training and scalability.

The `DataCollectorBase` class is designed to work with various types of environments, including grid worlds, discrete action spaces, and continuous action spaces. It supports both synchronous and asynchronous data collection methods, enabling users to choose the best approach for their specific use case.

Some key features of this class include:

*   The ability to collect experiences from multiple agents in parallel, reducing the overall training time.
*   Support for various types of environments, including grid worlds, discrete action spaces, and continuous action spaces.
*   The option to use a static seed or generate a new random seed for each episode.

Overall, the `DataCollectorBase` class provides a flexible and efficient way to collect and store experiences from multi-agent environments, enabling users to train high-performance reinforcement learning models.[/blue]
2024-12-03 22:18:48
function update_policy_weights_ description: 
[blue]<purpose>
Generate a description for the function "update_policy_weights_" based on the instructions, example output, and contextual content.
</purpose>

<description>
This function is used to update the policy weights of a data collector in a multi-agent environment. It allows for efficient training of multi-agent reinforcement learning models by synchronizing the policy weights between different devices.

If a TensorDict containing the new policy weights is provided, it updates the existing policy weights with the latest values. Otherwise, if the data collector has a function to retrieve its own policy weights (get_weights_fn), it uses this function to update the policy weights.

The function takes two optional arguments: a boolean indicating whether to use a static seed and an integer representing the random seed to be used. However, these arguments are not currently utilized in the provided implementation.

Example usage:
```python
data_collector = DataCollector()
# ... train the model ...
data_collector.update_policy_weights_()  # updates policy weights with default values
data_collector.update_policy_weights_(policy_weights=new_weights)  # updates policy weights with new values
```
</description>[/blue]
2024-12-04 00:11:16
function recursive_map_to_cpu description: 
[blue]<purpose>
Generate a description for the function "recursive_map_to_cpu".
</purpose>

<instructions>
    <instruction>Describe the purpose and functionality of the function: "recursive_map_to_cpu"</instruction>
</instructions>

<example-output>
    <arg-description-output>(bool) - Whether the static seed is used. (default: False)</arg-description-output>
    <arg-description-output>(int) - The random seed to be used. (default: None)</arg-description-output>
</example-output>

<content>
The function "recursive_map_to_cpu" recursively traverses a nested dictionary, mapping tensors to the CPU and applying any necessary transformations or operations. It takes an OrderedDict as input and returns a new OrderedDict with the same structure but with all tensors converted to the CPU.

This function is likely used in a deep learning context, where data needs to be processed on the CPU for various reasons such as debugging, testing, or compatibility issues. The recursive nature of this function allows it to handle nested dictionaries with arbitrary levels of depth.

By default, the function does not use a static seed, but if one is provided, it will be used to ensure reproducibility of results.
</content>[/blue]
2024-12-04 00:17:21
function recursive_map_to_cpu description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 00:17:59
parameter recursive_map_to_cpu description: 
[blue]This parameter is used to map a dictionary from the recursive data structure to a CPU-friendly format. It takes an OrderedDict as input and returns the same ordered dictionary, but with any nested dictionaries or lists converted to their CPU-friendly representations (e.g., NumPy arrays). This allows for efficient storage and processing of large datasets on multi-core CPUs.[/blue]
2024-12-04 00:18:32
class DataCollectorBase description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models.[/blue]
2024-12-04 00:19:23
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in a multi-agent environment, specifically when the policy and data collector live on different devices. It allows for efficient training of reinforcement learning models by synchronizing the policy weights between the data collector and the trained policy.[/blue]
2024-12-04 00:28:35
collectors.py: [
    {
        "function_name": "recursive_map_to_cpu",
        "args": [
            {
                "arg_name": "dictionary",
                "return_type": "OrderedDict",
                "default_value": "",
                "description": ""
            }
        ],
        "signature": "recursive_map_to_cpu(dictionary: OrderedDict) -> OrderedDict",
        "function_code": "def recursive_map_to_cpu(dictionary: OrderedDict) -> OrderedDict:\n    \"\"\"Maps the tensors to CPU through a nested dictionary.\"\"\"\n    return OrderedDict(**{k: recursive_map_to_cpu(item) if isinstance(item, OrderedDict) else item.cpu() if isinstance(item, torch.Tensor) else item for k, item in dictionary.items()})",
        "description": ""
    },
    {
        "class_name": "DataCollectorBase",
        "bases": [
            "IterableDataset"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "update_policy_weights_",
                "args": [
                    {
                        "arg_name": "policy_weights",
                        "return_type": "Optional[TensorDictBase]",
                        "default_value": "None",
                        "description": ""
                    }
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    \"\"\"Updates the policy weights if the policy of the data collector and the trained policy live on different devices.\n\n        Args:\n            policy_weights (TensorDictBase, optional): if provided, a TensorDict containing\n                the weights of the policy to be used for the udpdate.\n\n        \"\"\"\n    if policy_weights is not None:\n        self.policy_weights.data.update_(policy_weights)\n    elif self.get_weights_fn is not None:\n        self.policy_weights.data.update_(self.get_weights_fn())",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [],
                "signature": "next(self)",
                "function_code": "def next(self):\n    try:\n        if self._iterator is None:\n            self._iterator = iter(self)\n        out = next(self._iterator)\n        out.clear_device_()\n        return out\n    except StopIteration:\n        return None",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [],
                "signature": "shutdown(self)",
                "function_code": "@abc.abstractmethod\ndef shutdown(self):\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "@abc.abstractmethod\ndef iterator(self) -> Iterator[TensorDictBase]:\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "seed",
                        "return_type": "int",
                        "default_value": "",
                        "description": ""
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False",
                        "description": ""
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "@abc.abstractmethod\ndef set_seed(self, seed: int, static_seed: bool=False) -> int:\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "@abc.abstractmethod\ndef state_dict(self) -> OrderedDict:\n    raise NotImplementedError",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": "",
                        "description": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "@abc.abstractmethod\ndef load_state_dict(self, state_dict: OrderedDict) -> None:\n    raise NotImplementedError",
                "description": ""
            }
        ]
    },
    {
        "class_name": "SyncDataCollector",
        "bases": [
            "DataCollectorBase"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "__init__",
                "args": [
                    {
                        "arg_name": "create_env_fn",
                        "return_type": "Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]]",
                        "default_value": "",
                        "description": ""
                    },
                    {
                        "arg_name": "policy",
                        "return_type": "Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "frames_per_batch",
                        "return_type": "int",
                        "default_value": "",
                        "description": ""
                    },
                    {
                        "arg_name": "total_frames",
                        "return_type": "int",
                        "default_value": "-1",
                        "description": ""
                    },
                    {
                        "arg_name": "device",
                        "return_type": "DEVICE_TYPING",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "storing_device",
                        "return_type": "DEVICE_TYPING",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "policy_device",
                        "return_type": "DEVICE_TYPING",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "env_device",
                        "return_type": "DEVICE_TYPING",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "create_env_kwargs",
                        "return_type": "dict | None",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "max_frames_per_traj",
                        "return_type": "int | None",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "init_random_frames",
                        "return_type": "int | None",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "reset_at_each_iter",
                        "return_type": "bool",
                        "default_value": "False",
                        "description": ""
                    },
                    {
                        "arg_name": "postproc",
                        "return_type": "Callable[[TensorDictBase], TensorDictBase] | None",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "split_trajs",
                        "return_type": "bool | None",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "exploration_type",
                        "return_type": "ExplorationType",
                        "default_value": "DEFAULT_EXPLORATION_TYPE",
                        "description": ""
                    },
                    {
                        "arg_name": "return_same_td",
                        "return_type": "bool",
                        "default_value": "False",
                        "description": ""
                    },
                    {
                        "arg_name": "reset_when_done",
                        "return_type": "bool",
                        "default_value": "True",
                        "description": ""
                    },
                    {
                        "arg_name": "interruptor",
                        "return_type": "",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "set_truncated",
                        "return_type": "bool",
                        "default_value": "False",
                        "description": ""
                    },
                    {
                        "arg_name": "use_buffers",
                        "return_type": "bool | None",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "replay_buffer",
                        "return_type": "ReplayBuffer | None",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "trust_policy",
                        "return_type": "bool",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "compile_policy",
                        "return_type": "bool | Dict[str, Any] | None",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "cudagraph_policy",
                        "return_type": "bool | Dict[str, Any] | None",
                        "default_value": "None",
                        "description": ""
                    }
                ],
                "signature": "__init__(self, create_env_fn: Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]=None, *, frames_per_batch: int, total_frames: int=-1, device: DEVICE_TYPING=None, storing_device: DEVICE_TYPING=None, policy_device: DEVICE_TYPING=None, env_device: DEVICE_TYPING=None, create_env_kwargs: dict | None=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Callable[[TensorDictBase], TensorDictBase] | None=None, split_trajs: bool | None=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, return_same_td: bool=False, reset_when_done: bool=True, interruptor=None, set_truncated: bool=False, use_buffers: bool | None=None, replay_buffer: ReplayBuffer | None=None, trust_policy: bool=None, compile_policy: bool | Dict[str, Any] | None=None, cudagraph_policy: bool | Dict[str, Any] | None=None, **kwargs)",
                "function_code": "def __init__(self, create_env_fn: Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]=None, *, frames_per_batch: int, total_frames: int=-1, device: DEVICE_TYPING=None, storing_device: DEVICE_TYPING=None, policy_device: DEVICE_TYPING=None, env_device: DEVICE_TYPING=None, create_env_kwargs: dict | None=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Callable[[TensorDictBase], TensorDictBase] | None=None, split_trajs: bool | None=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, return_same_td: bool=False, reset_when_done: bool=True, interruptor=None, set_truncated: bool=False, use_buffers: bool | None=None, replay_buffer: ReplayBuffer | None=None, trust_policy: bool=None, compile_policy: bool | Dict[str, Any] | None=None, cudagraph_policy: bool | Dict[str, Any] | None=None, **kwargs):\n    from torchrl.envs.batched_envs import BatchedEnvBase\n    self.closed = True\n    if create_env_kwargs is None:\n        create_env_kwargs = {}\n    if not isinstance(create_env_fn, EnvBase):\n        env = create_env_fn(**create_env_kwargs)\n    else:\n        env = create_env_fn\n        if create_env_kwargs:\n            if not isinstance(env, BatchedEnvBase):\n                raise RuntimeError(f\"kwargs were passed to SyncDataCollector but they can't be set on environment of type {type(create_env_fn)}.\")\n            env.update_kwargs(create_env_kwargs)\n    if policy is None:\n        policy = RandomPolicy(env.full_action_spec)\n    if trust_policy is None:\n        trust_policy = isinstance(policy, (RandomPolicy, CudaGraphModule))\n    self.trust_policy = trust_policy\n    self._read_compile_kwargs(compile_policy, cudagraph_policy)\n    self._traj_pool_val = kwargs.pop('traj_pool', None)\n    if kwargs:\n        raise TypeError(f'Keys {list(kwargs.keys())} are unknown to {type(self).__name__}.')\n    storing_device, policy_device, env_device = self._get_devices(storing_device=storing_device, policy_device=policy_device, env_device=env_device, device=device)\n    self.storing_device = storing_device\n    if self.storing_device is not None and self.storing_device.type != 'cuda':\n        if torch.cuda.is_available():\n            self._sync_storage = torch.cuda.synchronize\n        elif torch.backends.mps.is_available() and hasattr(torch, 'mps'):\n            self._sync_storage = torch.mps.synchronize\n        elif self.storing_device.type == 'cpu':\n            self._sync_storage = _do_nothing\n        else:\n            raise RuntimeError('Non supported device')\n    else:\n        self._sync_storage = _do_nothing\n    self.env_device = env_device\n    if self.env_device is not None and self.env_device.type != 'cuda':\n        if torch.cuda.is_available():\n            self._sync_env = torch.cuda.synchronize\n        elif torch.backends.mps.is_available() and hasattr(torch, 'mps'):\n            self._sync_env = torch.mps.synchronize\n        elif self.env_device.type == 'cpu':\n            self._sync_env = _do_nothing\n        else:\n            raise RuntimeError('Non supported device')\n    else:\n        self._sync_env = _do_nothing\n    self.policy_device = policy_device\n    if self.policy_device is not None and self.policy_device.type != 'cuda':\n        if torch.cuda.is_available():\n            self._sync_policy = torch.cuda.synchronize\n        elif torch.backends.mps.is_available() and hasattr(torch, 'mps'):\n            self._sync_policy = torch.mps.synchronize\n        elif self.policy_device.type == 'cpu':\n            self._sync_policy = _do_nothing\n        else:\n            raise RuntimeError('Non supported device')\n    else:\n        self._sync_policy = _do_nothing\n    self.device = device\n    self._cast_to_policy_device = self.policy_device != self.env_device\n    self.env: EnvBase = env\n    del env\n    self.replay_buffer = replay_buffer\n    if self.replay_buffer is not None:\n        if postproc is not None:\n            raise TypeError('postproc must be None when a replay buffer is passed.')\n        if use_buffers:\n            raise TypeError('replay_buffer is exclusive with use_buffers.')\n    if use_buffers is None:\n        use_buffers = not self.env._has_dynamic_specs and self.replay_buffer is None\n    self._use_buffers = use_buffers\n    self.replay_buffer = replay_buffer\n    self.closed = False\n    if not reset_when_done:\n        raise ValueError('reset_when_done is deprectated.')\n    self.reset_when_done = reset_when_done\n    self.n_env = self.env.batch_size.numel()\n    self.policy, self.get_weights_fn = self._get_policy_and_device(policy=policy, observation_spec=self.env.observation_spec)\n    if isinstance(self.policy, nn.Module):\n        self.policy_weights = TensorDict.from_module(self.policy, as_module=True)\n    else:\n        self.policy_weights = TensorDict()\n    if self.compiled_policy:\n        self.policy = torch.compile(self.policy, **self.compiled_policy_kwargs)\n    if self.cudagraphed_policy:\n        self.policy = CudaGraphModule(self.policy, **self.cudagraphed_policy_kwargs)\n    if self.env_device:\n        self.env: EnvBase = self.env.to(self.env_device)\n    elif self.env.device is not None:\n        self.env_device = self.env.device\n    self._cast_to_env_device = self._cast_to_policy_device or self.env.device != self.storing_device\n    self.max_frames_per_traj = int(max_frames_per_traj) if max_frames_per_traj is not None else 0\n    if self.max_frames_per_traj is not None and self.max_frames_per_traj > 0:\n        for key in self.env.output_spec.keys(True, True):\n            if isinstance(key, str):\n                key = (key,)\n            if 'step_count' in key:\n                raise ValueError(\"A 'step_count' key is already present in the environment and the 'max_frames_per_traj' argument may conflict with a 'StepCounter' that has already been set. Possible solutions: Set max_frames_per_traj to 0 or remove the StepCounter limit from the environment transforms.\")\n        self.env = TransformedEnv(self.env, StepCounter(max_steps=self.max_frames_per_traj))\n    if total_frames is None or total_frames < 0:\n        total_frames = float('inf')\n    else:\n        remainder = total_frames % frames_per_batch\n        if remainder != 0 and RL_WARNINGS:\n            warnings.warn(f'total_frames ({total_frames}) is not exactly divisible by frames_per_batch ({frames_per_batch}).This means {frames_per_batch - remainder} additional frames will be collected.To silence this message, set the environment variable RL_WARNINGS to False.')\n    self.total_frames = int(total_frames) if total_frames != float('inf') else total_frames\n    self.reset_at_each_iter = reset_at_each_iter\n    self.init_random_frames = int(init_random_frames) if init_random_frames is not None else 0\n    if init_random_frames is not None and init_random_frames % frames_per_batch != 0 and RL_WARNINGS:\n        warnings.warn(f'init_random_frames ({init_random_frames}) is not exactly a multiple of frames_per_batch ({frames_per_batch}),  this results in more init_random_frames than requested ({-(-init_random_frames // frames_per_batch) * frames_per_batch}).To silence this message, set the environment variable RL_WARNINGS to False.')\n    self.postproc = postproc\n    if self.postproc is not None and hasattr(self.postproc, 'to') and self.storing_device:\n        self.postproc.to(self.storing_device)\n    if frames_per_batch % self.n_env != 0 and RL_WARNINGS:\n        warnings.warn(f'frames_per_batch ({frames_per_batch}) is not exactly divisible by the number of batched environments ({self.n_env}),  this results in more frames_per_batch per iteration that requested ({-(-frames_per_batch // self.n_env) * self.n_env}).To silence this message, set the environment variable RL_WARNINGS to False.')\n    self.requested_frames_per_batch = int(frames_per_batch)\n    self.frames_per_batch = -(-frames_per_batch // self.n_env)\n    self.exploration_type = exploration_type if exploration_type else DEFAULT_EXPLORATION_TYPE\n    self.return_same_td = return_same_td\n    self.set_truncated = set_truncated\n    self._make_shuttle()\n    if self._use_buffers:\n        self._make_final_rollout()\n    self._set_truncated_keys()\n    if split_trajs is None:\n        split_trajs = False\n    self.split_trajs = split_trajs\n    self._exclude_private_keys = True\n    self.interruptor = interruptor\n    self._frames = 0\n    self._iter = -1",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "update_policy_weights_",
                "args": [
                    {
                        "arg_name": "policy_weights",
                        "return_type": "Optional[TensorDictBase]",
                        "default_value": "None",
                        "description": ""
                    }
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    super().update_policy_weights_(policy_weights)",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "seed",
                        "return_type": "int",
                        "default_value": "",
                        "description": ""
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False",
                        "description": ""
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    \"\"\"Sets the seeds of the environments stored in the DataCollector.\n\n        Args:\n            seed (int): integer representing the seed to be used for the environment.\n            static_seed(bool, optional): if ``True``, the seed is not incremented.\n                Defaults to False\n\n        Returns:\n            Output seed. This is useful when more than one environment is contained in the DataCollector, as the\n            seed will be incremented for each of these. The resulting seed is the seed of the last environment.\n\n        Examples:\n            >>> from torchrl.envs import ParallelEnv\n            >>> from torchrl.envs.libs.gym import GymEnv\n            >>> from tensordict.nn import TensorDictModule\n            >>> from torch import nn\n            >>> env_fn = lambda: GymEnv(\"Pendulum-v1\")\n            >>> env_fn_parallel = ParallelEnv(6, env_fn)\n            >>> policy = TensorDictModule(nn.Linear(3, 1), in_keys=[\"observation\"], out_keys=[\"action\"])\n            >>> collector = SyncDataCollector(env_fn_parallel, policy, total_frames=300, frames_per_batch=100)\n            >>> out_seed = collector.set_seed(1)  # out_seed = 6\n\n        \"\"\"\n    out = self.env.set_seed(seed, static_seed=static_seed)\n    return out",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "def iterator(self) -> Iterator[TensorDictBase]:\n    \"\"\"Iterates through the DataCollector.\n\n        Yields: TensorDictBase objects containing (chunks of) trajectories\n\n        \"\"\"\n    if self.storing_device and self.storing_device.type == 'cuda':\n        stream = torch.cuda.Stream(self.storing_device, priority=-1)\n        event = stream.record_event()\n        streams = [stream]\n        events = [event]\n    elif self.storing_device is None:\n        streams = []\n        events = []\n        cuda_devices = set()\n\n        def cuda_check(tensor: torch.Tensor):\n            if tensor.is_cuda:\n                cuda_devices.add(tensor.device)\n        if not self._use_buffers:\n            for spec in self.env.specs.values(True, True):\n                if spec.device.type == 'cuda':\n                    if ':' not in str(spec.device):\n                        raise RuntimeError(\"A cuda spec did not have a device associated. Make sure to pass `'cuda:device_num'` to each spec device.\")\n                    cuda_devices.add(spec.device)\n        else:\n            self._final_rollout.apply(cuda_check, filter_empty=True)\n        for device in cuda_devices:\n            streams.append(torch.cuda.Stream(device, priority=-1))\n            events.append(streams[-1].record_event())\n    else:\n        streams = []\n        events = []\n    with contextlib.ExitStack() as stack:\n        for stream in streams:\n            stack.enter_context(torch.cuda.stream(stream))\n        while self._frames < self.total_frames:\n            self._iter += 1\n            tensordict_out = self.rollout()\n            if tensordict_out is None:\n                yield\n                continue\n            self._increment_frames(tensordict_out.numel())\n            if self.split_trajs:\n                tensordict_out = split_trajectories(tensordict_out, prefix='collector')\n            if self.postproc is not None:\n                tensordict_out = self.postproc(tensordict_out)\n            if self._exclude_private_keys:\n\n                def is_private(key):\n                    if isinstance(key, str) and key.startswith('_'):\n                        return True\n                    if isinstance(key, tuple) and any((_key.startswith('_') for _key in key)):\n                        return True\n                    return False\n                excluded_keys = [key for key in tensordict_out.keys(True) if is_private(key)]\n                tensordict_out = tensordict_out.exclude(*excluded_keys, inplace=True)\n            if self.return_same_td:\n                if events:\n                    for event in events:\n                        event.record()\n                        event.synchronize()\n                yield tensordict_out\n            else:\n                yield tensordict_out.clone()",
                "description": ""
            },
            {
                "function_name": "rollout",
                "args": [],
                "signature": "rollout(self) -> TensorDictBase",
                "function_code": "@torch.no_grad()\ndef rollout(self) -> TensorDictBase:\n    \"\"\"Computes a rollout in the environment using the provided policy.\n\n        Returns:\n            TensorDictBase containing the computed rollout.\n\n        \"\"\"\n    if self.reset_at_each_iter:\n        self._shuttle.update(self.env.reset())\n    if self._use_buffers:\n        self._final_rollout.fill_(('collector', 'traj_ids'), -1)\n    else:\n        pass\n    tensordicts = []\n    with set_exploration_type(self.exploration_type):\n        for t in range(self.frames_per_batch):\n            if self.init_random_frames is not None and self._frames < self.init_random_frames:\n                self.env.rand_action(self._shuttle)\n            else:\n                if self._cast_to_policy_device:\n                    if self.policy_device is not None:\n                        policy_input = self._shuttle.to(self.policy_device, non_blocking=True)\n                        self._sync_policy()\n                    elif self.policy_device is None:\n                        policy_input = self._shuttle\n                else:\n                    policy_input = self._shuttle\n                policy_output = self.policy(policy_input)\n                if self._shuttle is not policy_output:\n                    self._shuttle.update(policy_output, keys_to_update=self._policy_output_keys)\n            if self._cast_to_env_device:\n                if self.env_device is not None:\n                    env_input = self._shuttle.to(self.env_device, non_blocking=True)\n                    self._sync_env()\n                elif self.env_device is None:\n                    env_input = self._shuttle\n            else:\n                env_input = self._shuttle\n            env_output, env_next_output = self.env.step_and_maybe_reset(env_input)\n            if self._shuttle is not env_output:\n                next_data = env_output.get('next')\n                if self._shuttle_has_no_device:\n                    next_data.clear_device_()\n                self._shuttle.set('next', next_data)\n            if self.replay_buffer is not None:\n                self.replay_buffer.add(self._shuttle)\n                if self._increment_frames(self._shuttle.numel()):\n                    return\n            elif self.storing_device is not None:\n                tensordicts.append(self._shuttle.to(self.storing_device, non_blocking=True))\n                self._sync_storage()\n            else:\n                tensordicts.append(self._shuttle)\n            collector_data = self._shuttle.get('collector').copy()\n            self._shuttle = env_next_output\n            if self._shuttle_has_no_device:\n                self._shuttle.clear_device_()\n            self._shuttle.set('collector', collector_data)\n            self._update_traj_ids(env_output)\n            if self.interruptor is not None and self.interruptor.collection_stopped():\n                if self.replay_buffer is not None:\n                    return\n                result = self._final_rollout\n                if self._use_buffers:\n                    try:\n                        torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout[..., :t + 1])\n                    except RuntimeError:\n                        with self._final_rollout.unlock_():\n                            torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout[..., :t + 1])\n                else:\n                    result = TensorDict.maybe_dense_stack(tensordicts, dim=-1)\n                break\n        else:\n            if self._use_buffers:\n                result = self._final_rollout\n                try:\n                    result = torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout)\n                except RuntimeError:\n                    with self._final_rollout.unlock_():\n                        result = torch.stack(tensordicts, self._final_rollout.ndim - 1, out=self._final_rollout)\n            elif self.replay_buffer is not None:\n                return\n            else:\n                result = TensorDict.maybe_dense_stack(tensordicts, dim=-1)\n                result.refine_names(..., 'time')\n    return self._maybe_set_truncated(result)",
                "description": ""
            },
            {
                "function_name": "reset",
                "args": [
                    {
                        "arg_name": "index",
                        "return_type": "",
                        "default_value": "None",
                        "description": ""
                    }
                ],
                "signature": "reset(self, index=None, **kwargs) -> None",
                "function_code": "@torch.no_grad()\ndef reset(self, index=None, **kwargs) -> None:\n    \"\"\"Resets the environments to a new initial state.\"\"\"\n    collector_metadata = self._shuttle.get('collector').clone()\n    if index is not None:\n        if prod(self.env.batch_size) == 0:\n            raise RuntimeError('resetting unique env with index is not permitted.')\n        for reset_key, done_keys in zip(self.env.reset_keys, self.env.done_keys_groups):\n            _reset = torch.zeros(self.env.full_done_spec[done_keys[0]].shape, dtype=torch.bool, device=self.env.device)\n            _reset[index] = 1\n            self._shuttle.set(reset_key, _reset)\n    else:\n        _reset = None\n        self._shuttle.zero_()\n    self._shuttle.update(self.env.reset(**kwargs), inplace=True)\n    collector_metadata['traj_ids'] = collector_metadata['traj_ids'] - collector_metadata['traj_ids'].min()\n    self._shuttle['collector'] = collector_metadata",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [],
                "signature": "shutdown(self) -> None",
                "function_code": "def shutdown(self) -> None:\n    \"\"\"Shuts down all workers and/or closes the local environment.\"\"\"\n    if not self.closed:\n        self.closed = True\n        del self._shuttle\n        if self._use_buffers:\n            del self._final_rollout\n        if not self.env.is_closed:\n            self.env.close()\n        del self.env\n    return",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    \"\"\"Returns the local state_dict of the data collector (environment and policy).\n\n        Returns:\n            an ordered dictionary with fields :obj:`\"policy_state_dict\"` and\n            `\"env_state_dict\"`.\n\n        \"\"\"\n    from torchrl.envs.batched_envs import BatchedEnvBase\n    if isinstance(self.env, TransformedEnv):\n        env_state_dict = self.env.transform.state_dict()\n    elif isinstance(self.env, BatchedEnvBase):\n        env_state_dict = self.env.state_dict()\n    else:\n        env_state_dict = OrderedDict()\n    if hasattr(self.policy, 'state_dict'):\n        policy_state_dict = self.policy.state_dict()\n        state_dict = OrderedDict(policy_state_dict=policy_state_dict, env_state_dict=env_state_dict)\n    else:\n        state_dict = OrderedDict(env_state_dict=env_state_dict)\n    state_dict.update({'frames': self._frames, 'iter': self._iter})\n    return state_dict",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": "",
                        "description": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict, **kwargs) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict, **kwargs) -> None:\n    \"\"\"Loads a state_dict on the environment and policy.\n\n        Args:\n            state_dict (OrderedDict): ordered dictionary containing the fields\n                `\"policy_state_dict\"` and :obj:`\"env_state_dict\"`.\n\n        \"\"\"\n    strict = kwargs.get('strict', True)\n    if strict or 'env_state_dict' in state_dict:\n        self.env.load_state_dict(state_dict['env_state_dict'], **kwargs)\n    if strict or 'policy_state_dict' in state_dict:\n        self.policy.load_state_dict(state_dict['policy_state_dict'], **kwargs)\n    self._frames = state_dict['frames']\n    self._iter = state_dict['iter']",
                "description": ""
            }
        ]
    },
    {
        "class_name": "MultiSyncDataCollector",
        "bases": [
            "_MultiDataCollector"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "next",
                "args": [],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [],
                "signature": "shutdown(self)",
                "function_code": "def shutdown(self):\n    if hasattr(self, 'out_buffer'):\n        del self.out_buffer\n    if hasattr(self, 'buffers'):\n        del self.buffers\n    return super().shutdown()",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "seed",
                        "return_type": "int",
                        "default_value": "",
                        "description": ""
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False",
                        "description": ""
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    return super().set_seed(seed, static_seed)",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    return super().state_dict()",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": "",
                        "description": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict) -> None:\n    return super().load_state_dict(state_dict)",
                "description": ""
            },
            {
                "function_name": "update_policy_weights_",
                "args": [
                    {
                        "arg_name": "policy_weights",
                        "return_type": "Optional[TensorDictBase]",
                        "default_value": "None",
                        "description": ""
                    }
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    super().update_policy_weights_(policy_weights)",
                "description": ""
            },
            {
                "function_name": "frames_per_batch_worker",
                "args": [],
                "signature": "frames_per_batch_worker(self)",
                "function_code": "@property\ndef frames_per_batch_worker(self):\n    if self.requested_frames_per_batch % self.num_workers != 0 and RL_WARNINGS:\n        warnings.warn(f'frames_per_batch {self.requested_frames_per_batch} is not exactly divisible by the number of collector workers {self.num_workers}, this results in more frames_per_batch per iteration that requested.To silence this message, set the environment variable RL_WARNINGS to False.')\n    frames_per_batch_worker = -(-self.requested_frames_per_batch // self.num_workers)\n    return frames_per_batch_worker",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "def iterator(self) -> Iterator[TensorDictBase]:\n    cat_results = self.cat_results\n    if cat_results is None:\n        cat_results = 'stack'\n    self.buffers = {}\n    dones = [False for _ in range(self.num_workers)]\n    workers_frames = [0 for _ in range(self.num_workers)]\n    same_device = None\n    self.out_buffer = None\n    preempt = self.interruptor is not None and self.preemptive_threshold < 1.0\n    while not all(dones) and self._frames < self.total_frames:\n        _check_for_faulty_process(self.procs)\n        if self.update_at_each_batch:\n            self.update_policy_weights_()\n        for idx in range(self.num_workers):\n            if self.init_random_frames is not None and self._frames < self.init_random_frames:\n                msg = 'continue_random'\n            else:\n                msg = 'continue'\n            self.pipes[idx].send((None, msg))\n        self._iter += 1\n        if preempt:\n            self.interruptor.start_collection()\n            while self.queue_out.qsize() < int(self.num_workers * self.preemptive_threshold):\n                continue\n            self.interruptor.stop_collection()\n            while self.queue_out.qsize() < int(self.num_workers):\n                continue\n        for _ in range(self.num_workers):\n            new_data, j = self.queue_out.get()\n            use_buffers = self._use_buffers\n            if self.replay_buffer is not None:\n                idx = new_data\n                workers_frames[idx] = workers_frames[idx] + self.frames_per_batch_worker\n                continue\n            elif j == 0 or not use_buffers:\n                try:\n                    data, idx = new_data\n                    self.buffers[idx] = data\n                    if use_buffers is None and j > 0:\n                        self._use_buffers = False\n                except TypeError:\n                    if use_buffers is None:\n                        self._use_buffers = True\n                        idx = new_data\n                    else:\n                        raise\n            else:\n                idx = new_data\n            if preempt:\n                if cat_results != 'stack':\n                    buffers = {}\n                    for idx, buffer in self.buffers.items():\n                        valid = buffer.get(('collector', 'traj_ids')) != -1\n                        if valid.ndim > 2:\n                            valid = valid.flatten(0, -2)\n                        if valid.ndim == 2:\n                            valid = valid.any(0)\n                        buffers[idx] = buffer[..., valid]\n                else:\n                    for buffer in self.buffers.values():\n                        with buffer.unlock_():\n                            buffer.set(('collector', 'mask'), buffer.get(('collector', 'traj_ids')) != -1)\n                    buffers = self.buffers\n            else:\n                buffers = self.buffers\n            workers_frames[idx] = workers_frames[idx] + buffers[idx].numel()\n            if workers_frames[idx] >= self.total_frames:\n                dones[idx] = True\n        if self.replay_buffer is not None:\n            yield\n            self._frames += self.frames_per_batch_worker * self.num_workers\n            continue\n        n_collected = 0\n        for idx in range(self.num_workers):\n            buffer = buffers[idx]\n            traj_ids = buffer.get(('collector', 'traj_ids'))\n            if preempt:\n                if cat_results == 'stack':\n                    mask_frames = buffer.get(('collector', 'traj_ids')) != -1\n                    n_collected += mask_frames.sum().cpu()\n                else:\n                    n_collected += traj_ids.numel()\n            else:\n                n_collected += traj_ids.numel()\n        if same_device is None:\n            prev_device = None\n            same_device = True\n            for item in self.buffers.values():\n                if prev_device is None:\n                    prev_device = item.device\n                else:\n                    same_device = same_device and item.device == prev_device\n        if cat_results == 'stack':\n            stack = torch.stack if self._use_buffers else TensorDict.maybe_dense_stack\n            if same_device:\n                self.out_buffer = stack(list(buffers.values()), 0)\n            else:\n                self.out_buffer = stack([item.cpu() for item in buffers.values()], 0)\n        else:\n            if self._use_buffers is None:\n                torchrl_logger.warning('use_buffer not specified and not yet inferred from data, assuming `True`.')\n            elif not self._use_buffers:\n                raise RuntimeError('Cannot concatenate results with use_buffers=False')\n            try:\n                if same_device:\n                    self.out_buffer = torch.cat(list(buffers.values()), cat_results)\n                else:\n                    self.out_buffer = torch.cat([item.cpu() for item in buffers.values()], cat_results)\n            except RuntimeError as err:\n                if preempt and cat_results != -1 and ('Sizes of tensors must match' in str(err)):\n                    raise RuntimeError(\"The value provided to cat_results isn't compatible with the collectors outputs. Consider using `cat_results=-1`.\")\n                raise\n        if self.split_trajs:\n            out = split_trajectories(self.out_buffer, prefix='collector')\n        else:\n            out = self.out_buffer\n        if cat_results in (-1, 'stack'):\n            out.refine_names(*[None] * (out.ndim - 1) + ['time'])\n        self._frames += n_collected\n        if self.postprocs:\n            self.postprocs = self.postprocs.to(out.device)\n            out = self.postprocs(out)\n        if self._exclude_private_keys:\n            excluded_keys = [key for key in out.keys() if key.startswith('_')]\n            if excluded_keys:\n                out = out.exclude(*excluded_keys)\n        yield out\n        del out\n    del self.buffers\n    self.out_buffer = None",
                "description": ""
            }
        ]
    },
    {
        "class_name": "MultiaSyncDataCollector",
        "bases": [
            "_MultiDataCollector"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "__init__",
                "args": [],
                "signature": "__init__(self, *args, **kwargs)",
                "function_code": "def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.out_tensordicts = defaultdict(lambda: None)\n    self.running = False\n    if self.postprocs is not None:\n        postproc = self.postprocs\n        self.postprocs = {}\n        for _device in self.storing_device:\n            if _device not in self.postprocs:\n                self.postprocs[_device] = deepcopy(postproc).to(_device)",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [],
                "signature": "shutdown(self)",
                "function_code": "def shutdown(self):\n    if hasattr(self, 'out_tensordicts'):\n        del self.out_tensordicts\n    return super().shutdown()",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "seed",
                        "return_type": "int",
                        "default_value": "",
                        "description": ""
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False",
                        "description": ""
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    return super().set_seed(seed, static_seed)",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    return super().state_dict()",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": "",
                        "description": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict) -> None:\n    return super().load_state_dict(state_dict)",
                "description": ""
            },
            {
                "function_name": "update_policy_weights_",
                "args": [
                    {
                        "arg_name": "policy_weights",
                        "return_type": "Optional[TensorDictBase]",
                        "default_value": "None",
                        "description": ""
                    }
                ],
                "signature": "update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None",
                "function_code": "def update_policy_weights_(self, policy_weights: Optional[TensorDictBase]=None) -> None:\n    super().update_policy_weights_(policy_weights)",
                "description": ""
            },
            {
                "function_name": "frames_per_batch_worker",
                "args": [],
                "signature": "frames_per_batch_worker(self)",
                "function_code": "@property\ndef frames_per_batch_worker(self):\n    return self.requested_frames_per_batch",
                "description": ""
            },
            {
                "function_name": "iterator",
                "args": [],
                "signature": "iterator(self) -> Iterator[TensorDictBase]",
                "function_code": "def iterator(self) -> Iterator[TensorDictBase]:\n    if self.update_at_each_batch:\n        self.update_policy_weights_()\n    for i in range(self.num_workers):\n        if self.init_random_frames is not None and self.init_random_frames > 0:\n            self.pipes[i].send((None, 'continue_random'))\n        else:\n            self.pipes[i].send((None, 'continue'))\n    self.running = True\n    workers_frames = [0 for _ in range(self.num_workers)]\n    while self._frames < self.total_frames:\n        self._iter += 1\n        while True:\n            try:\n                idx, j, out = self._get_from_queue(timeout=10.0)\n                break\n            except TimeoutError:\n                _check_for_faulty_process(self.procs)\n        if self.replay_buffer is None:\n            worker_frames = out.numel()\n            if self.split_trajs:\n                out = split_trajectories(out, prefix='collector')\n        else:\n            worker_frames = self.frames_per_batch_worker\n        self._frames += worker_frames\n        workers_frames[idx] = workers_frames[idx] + worker_frames\n        if self.postprocs:\n            out = self.postprocs[out.device](out)\n        if self.init_random_frames is not None and self._frames < self.init_random_frames:\n            msg = 'continue_random'\n        else:\n            msg = 'continue'\n        self.pipes[idx].send((idx, msg))\n        if out is not None and self._exclude_private_keys:\n            excluded_keys = [key for key in out.keys() if key.startswith('_')]\n            out = out.exclude(*excluded_keys)\n        yield out\n    self.running = False",
                "description": ""
            },
            {
                "function_name": "reset",
                "args": [
                    {
                        "arg_name": "reset_idx",
                        "return_type": "Optional[Sequence[bool]]",
                        "default_value": "None",
                        "description": ""
                    }
                ],
                "signature": "reset(self, reset_idx: Optional[Sequence[bool]]=None) -> None",
                "function_code": "def reset(self, reset_idx: Optional[Sequence[bool]]=None) -> None:\n    super().reset(reset_idx)\n    if self.queue_out.full():\n        time.sleep(_TIMEOUT)\n    if self.queue_out.full():\n        raise Exception('self.queue_out is full')\n    if self.running:\n        for idx in range(self.num_workers):\n            if self.init_random_frames is not None and self._frames < self.init_random_frames:\n                self.pipes[idx].send((idx, 'continue_random'))\n            else:\n                self.pipes[idx].send((idx, 'continue'))",
                "description": ""
            }
        ]
    },
    {
        "class_name": "aSyncDataCollector",
        "bases": [
            "MultiaSyncDataCollector"
        ],
        "description": "",
        "overview": "",
        "functions": [
            {
                "function_name": "__init__",
                "args": [
                    {
                        "arg_name": "create_env_fn",
                        "return_type": "Callable[[], EnvBase]",
                        "default_value": "",
                        "description": ""
                    },
                    {
                        "arg_name": "policy",
                        "return_type": "Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]]",
                        "default_value": "",
                        "description": ""
                    },
                    {
                        "arg_name": "frames_per_batch",
                        "return_type": "int",
                        "default_value": "",
                        "description": ""
                    },
                    {
                        "arg_name": "total_frames",
                        "return_type": "Optional[int]",
                        "default_value": "-1",
                        "description": ""
                    },
                    {
                        "arg_name": "device",
                        "return_type": "DEVICE_TYPING | Sequence[DEVICE_TYPING] | None",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "storing_device",
                        "return_type": "DEVICE_TYPING | Sequence[DEVICE_TYPING] | None",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "env_device",
                        "return_type": "DEVICE_TYPING | Sequence[DEVICE_TYPING] | None",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "policy_device",
                        "return_type": "DEVICE_TYPING | Sequence[DEVICE_TYPING] | None",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "create_env_kwargs",
                        "return_type": "Optional[Sequence[dict]]",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "max_frames_per_traj",
                        "return_type": "int | None",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "init_random_frames",
                        "return_type": "int | None",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "reset_at_each_iter",
                        "return_type": "bool",
                        "default_value": "False",
                        "description": ""
                    },
                    {
                        "arg_name": "postproc",
                        "return_type": "Optional[Callable[[TensorDictBase], TensorDictBase]]",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "split_trajs",
                        "return_type": "Optional[bool]",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "exploration_type",
                        "return_type": "ExplorationType",
                        "default_value": "DEFAULT_EXPLORATION_TYPE",
                        "description": ""
                    },
                    {
                        "arg_name": "reset_when_done",
                        "return_type": "bool",
                        "default_value": "True",
                        "description": ""
                    },
                    {
                        "arg_name": "update_at_each_batch",
                        "return_type": "bool",
                        "default_value": "False",
                        "description": ""
                    },
                    {
                        "arg_name": "preemptive_threshold",
                        "return_type": "float",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "num_threads",
                        "return_type": "int",
                        "default_value": "None",
                        "description": ""
                    },
                    {
                        "arg_name": "num_sub_threads",
                        "return_type": "int",
                        "default_value": "1",
                        "description": ""
                    },
                    {
                        "arg_name": "set_truncated",
                        "return_type": "bool",
                        "default_value": "False",
                        "description": ""
                    }
                ],
                "signature": "__init__(self, create_env_fn: Callable[[], EnvBase], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]], *, frames_per_batch: int, total_frames: Optional[int]=-1, device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, storing_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, env_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, policy_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, create_env_kwargs: Optional[Sequence[dict]]=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Optional[Callable[[TensorDictBase], TensorDictBase]]=None, split_trajs: Optional[bool]=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, reset_when_done: bool=True, update_at_each_batch: bool=False, preemptive_threshold: float=None, num_threads: int=None, num_sub_threads: int=1, set_truncated: bool=False, **kwargs)",
                "function_code": "def __init__(self, create_env_fn: Callable[[], EnvBase], policy: Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]]], *, frames_per_batch: int, total_frames: Optional[int]=-1, device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, storing_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, env_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, policy_device: DEVICE_TYPING | Sequence[DEVICE_TYPING] | None=None, create_env_kwargs: Optional[Sequence[dict]]=None, max_frames_per_traj: int | None=None, init_random_frames: int | None=None, reset_at_each_iter: bool=False, postproc: Optional[Callable[[TensorDictBase], TensorDictBase]]=None, split_trajs: Optional[bool]=None, exploration_type: ExplorationType=DEFAULT_EXPLORATION_TYPE, reset_when_done: bool=True, update_at_each_batch: bool=False, preemptive_threshold: float=None, num_threads: int=None, num_sub_threads: int=1, set_truncated: bool=False, **kwargs):\n    super().__init__(create_env_fn=[create_env_fn], policy=policy, total_frames=total_frames, create_env_kwargs=[create_env_kwargs], max_frames_per_traj=max_frames_per_traj, frames_per_batch=frames_per_batch, reset_at_each_iter=reset_at_each_iter, init_random_frames=init_random_frames, postproc=postproc, split_trajs=split_trajs, device=device, policy_device=policy_device, env_device=env_device, storing_device=storing_device, exploration_type=exploration_type, reset_when_done=reset_when_done, update_at_each_batch=update_at_each_batch, preemptive_threshold=preemptive_threshold, num_threads=num_threads, num_sub_threads=num_sub_threads, set_truncated=set_truncated)",
                "description": ""
            },
            {
                "function_name": "next",
                "args": [],
                "signature": "next(self)",
                "function_code": "def next(self):\n    return super().next()",
                "description": ""
            },
            {
                "function_name": "shutdown",
                "args": [],
                "signature": "shutdown(self)",
                "function_code": "def shutdown(self):\n    return super().shutdown()",
                "description": ""
            },
            {
                "function_name": "set_seed",
                "args": [
                    {
                        "arg_name": "seed",
                        "return_type": "int",
                        "default_value": "",
                        "description": ""
                    },
                    {
                        "arg_name": "static_seed",
                        "return_type": "bool",
                        "default_value": "False",
                        "description": ""
                    }
                ],
                "signature": "set_seed(self, seed: int, static_seed: bool=False) -> int",
                "function_code": "def set_seed(self, seed: int, static_seed: bool=False) -> int:\n    return super().set_seed(seed, static_seed)",
                "description": ""
            },
            {
                "function_name": "state_dict",
                "args": [],
                "signature": "state_dict(self) -> OrderedDict",
                "function_code": "def state_dict(self) -> OrderedDict:\n    return super().state_dict()",
                "description": ""
            },
            {
                "function_name": "load_state_dict",
                "args": [
                    {
                        "arg_name": "state_dict",
                        "return_type": "OrderedDict",
                        "default_value": "",
                        "description": ""
                    }
                ],
                "signature": "load_state_dict(self, state_dict: OrderedDict) -> None",
                "function_code": "def load_state_dict(self, state_dict: OrderedDict) -> None:\n    return super().load_state_dict(state_dict)",
                "description": ""
            }
        ]
    }
]
2024-12-04 00:28:35
utils.py: [
    {
        "function_name": "split_trajectories",
        "args": [
            {
                "arg_name": "rollout_tensordict",
                "return_type": "TensorDictBase",
                "default_value": "",
                "description": ""
            },
            {
                "arg_name": "prefix",
                "return_type": "",
                "default_value": "None",
                "description": ""
            },
            {
                "arg_name": "trajectory_key",
                "return_type": "NestedKey | None",
                "default_value": "None",
                "description": ""
            },
            {
                "arg_name": "done_key",
                "return_type": "NestedKey | None",
                "default_value": "None",
                "description": ""
            },
            {
                "arg_name": "as_nested",
                "return_type": "bool",
                "default_value": "False",
                "description": ""
            }
        ],
        "signature": "split_trajectories(rollout_tensordict: TensorDictBase, *, prefix=None, trajectory_key: NestedKey | None=None, done_key: NestedKey | None=None, as_nested: bool=False) -> TensorDictBase",
        "function_code": "@set_lazy_legacy(False)\ndef split_trajectories(rollout_tensordict: TensorDictBase, *, prefix=None, trajectory_key: NestedKey | None=None, done_key: NestedKey | None=None, as_nested: bool=False) -> TensorDictBase:\n    \"\"\"A util function for trajectory separation.\n\n    Takes a tensordict with a key traj_ids that indicates the id of each trajectory.\n\n    From there, builds a B x T x ... zero-padded tensordict with B batches on max duration T\n\n    Args:\n        rollout_tensordict (TensorDictBase): a rollout with adjacent trajectories\n            along the last dimension.\n\n    Keyword Args:\n        prefix (NestedKey, optional): the prefix used to read and write meta-data,\n            such as ``\"traj_ids\"`` (the optional integer id of each trajectory)\n            and the ``\"mask\"`` entry indicating which data are valid and which\n            aren't. Defaults to ``\"collector\"`` if the input has a ``\"collector\"``\n            entry, ``()`` (no prefix) otherwise.\n            ``prefix`` is kept as a legacy feature and will be deprecated eventually.\n            Prefer ``trajectory_key`` or ``done_key`` whenever possible.\n        trajectory_key (NestedKey, optional): the key pointing to the trajectory\n            ids. Supersedes ``done_key`` and ``prefix``. If not provided, defaults\n            to ``(prefix, \"traj_ids\")``.\n        done_key (NestedKey, optional): the key pointing to the ``\"done\"\"`` signal,\n            if the trajectory could not be directly recovered. Defaults to ``\"done\"``.\n        as_nested (bool or torch.layout, optional): whether to return the results as nested\n            tensors. Defaults to ``False``. If a ``torch.layout`` is provided, it will be used\n            to construct the nested tensor, otherwise the default layout will be used.\n\n            .. note:: Using ``split_trajectories(tensordict, as_nested=True).to_padded_tensor(mask=mask_key)``\n                should result in the exact same result as ``as_nested=False``. Since this is an experimental\n                feature and relies on nested_tensors, which API may change in the future, we made this\n                an optional feature. The runtime should be faster with ``as_nested=True``.\n\n            .. note:: Providing a layout lets the user control whether the nested tensor is to be used\n                with ``torch.strided`` or ``torch.jagged`` layout. While the former has slightly more\n                capabilities at the time of writing, the second will be the main focus of the PyTorch team\n                in the future due to its better compatibility with :func:`~torch.compile`.\n\n    Returns:\n        A new tensordict with a leading dimension corresponding to the trajectory.\n        A ``\"mask\"`` boolean entry sharing the ``trajectory_key`` prefix\n        and the tensordict shape is also added. It indicated the valid elements of the tensordict,\n        as well as a ``\"traj_ids\"`` entry if ``trajectory_key`` could not be found.\n\n    Examples:\n        >>> from tensordict import TensorDict\n        >>> import torch\n        >>> from torchrl.collectors.utils import split_trajectories\n        >>> obs = torch.cat([torch.arange(10), torch.arange(5)])\n        >>> obs_ = torch.cat([torch.arange(1, 11), torch.arange(1, 6)])\n        >>> done = torch.zeros(15, dtype=torch.bool)\n        >>> done[9] = True\n        >>> trajectory_id = torch.cat([torch.zeros(10, dtype=torch.int32),\n        ...     torch.ones(5, dtype=torch.int32)])\n        >>> data = TensorDict({\"obs\": obs, (\"next\", \"obs\"): obs_, (\"next\", \"done\"): done, \"trajectory\": trajectory_id}, batch_size=[15])\n        >>> data_split = split_trajectories(data, done_key=\"done\")\n        >>> print(data_split)\n        TensorDict(\n            fields={\n                mask: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                next: TensorDict(\n                    fields={\n                        done: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                        obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False)},\n                    batch_size=torch.Size([2, 10]),\n                    device=None,\n                    is_shared=False),\n                obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n                traj_ids: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n                trajectory: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int32, is_shared=False)},\n            batch_size=torch.Size([2, 10]),\n            device=None,\n            is_shared=False)\n        >>> # check that split_trajectories got the trajectories right with the done signal\n        >>> assert (data_split[\"traj_ids\"] == data_split[\"trajectory\"]).all()\n        >>> print(data_split[\"mask\"])\n        tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n                [ True,  True,  True,  True,  True, False, False, False, False, False]])\n        >>> data_split = split_trajectories(data, trajectory_key=\"trajectory\")\n        >>> print(data_split)\n        TensorDict(\n            fields={\n                mask: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                next: TensorDict(\n                    fields={\n                        done: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n                        obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False)},\n                    batch_size=torch.Size([2, 10]),\n                    device=None,\n                    is_shared=False),\n                obs: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n                trajectory: Tensor(shape=torch.Size([2, 10]), device=cpu, dtype=torch.int32, is_shared=False)},\n            batch_size=torch.Size([2, 10]),\n            device=None,\n            is_shared=False)\n\n    \"\"\"\n    mask_key = None\n    if trajectory_key is not None:\n        from torchrl.envs.utils import _replace_last\n        traj_ids_key = trajectory_key\n        mask_key = _replace_last(trajectory_key, 'mask')\n    else:\n        if prefix is None and 'collector' in rollout_tensordict.keys():\n            prefix = 'collector'\n        if prefix is None:\n            traj_ids_key = 'traj_ids'\n            mask_key = 'mask'\n        else:\n            traj_ids_key = (prefix, 'traj_ids')\n            mask_key = (prefix, 'mask')\n    rollout_tensordict = rollout_tensordict.copy()\n    traj_ids = rollout_tensordict.get(traj_ids_key, None)\n    if traj_ids is None:\n        if done_key is None:\n            done_key = 'done'\n        done_key = ('next', done_key)\n        done = rollout_tensordict.get(done_key)\n        idx = (slice(None),) * (rollout_tensordict.ndim - 1) + (slice(None, -1),)\n        done_sel = done[idx]\n        pads = [1, 0]\n        pads = [0, 0] * (done.ndim - rollout_tensordict.ndim) + pads\n        done_sel = torch.nn.functional.pad(done_sel, pads)\n        if done_sel.shape != done.shape:\n            raise RuntimeError(f'done and done_sel have different shape {done.shape} - {done_sel.shape} ')\n        traj_ids = done_sel.cumsum(rollout_tensordict.ndim - 1)\n        traj_ids = traj_ids.squeeze(-1)\n        if rollout_tensordict.ndim > 1:\n            for i in range(1, rollout_tensordict.shape[0]):\n                traj_ids[i] += traj_ids[i - 1].max() + 1\n        rollout_tensordict.set(traj_ids_key, traj_ids)\n    splits = traj_ids.reshape(-1)\n    splits = [(splits == i).sum().item() for i in splits.unique_consecutive()]\n    if len(set(splits)) == 1 and splits[0] == traj_ids.shape[-1]:\n        rollout_tensordict.set(mask_key, torch.ones(rollout_tensordict.shape, device=rollout_tensordict.device, dtype=torch.bool))\n        if rollout_tensordict.ndimension() == 1:\n            rollout_tensordict = rollout_tensordict.unsqueeze(0)\n        return rollout_tensordict\n    out_splits = rollout_tensordict.reshape(-1)\n    if as_nested:\n        if hasattr(torch, '_nested_compute_contiguous_strides_offsets'):\n\n            def nest(x, splits=splits):\n                shape = torch.tensor([[int(split), *x.shape[1:]] for split in splits])\n                return torch._nested_view_from_buffer(x.reshape(-1), shape, *torch._nested_compute_contiguous_strides_offsets(shape))\n            return out_splits._fast_apply(nest, batch_size=[len(splits), -1])\n        else:\n            out_splits = out_splits.split(splits, 0)\n            layout = as_nested if as_nested is not bool else None\n            if torch.__version__ < '2.4':\n                if layout not in (True,):\n                    raise RuntimeError(f'layout={layout} is only available for torch>=v2.4')\n\n                def nest(*x):\n                    return torch.nested.nested_tensor(list(x))\n            else:\n\n                def nest(*x):\n                    return torch.nested.nested_tensor(list(x), layout=layout)\n            return out_splits[0]._fast_apply(nest, *out_splits[1:], batch_size=[len(out_splits), *out_splits[0].batch_size[:-1], -1])\n    out_splits = out_splits.split(splits, 0)\n    for out_split in out_splits:\n        out_split.set(mask_key, torch.ones(out_split.shape, dtype=torch.bool, device=out_split.device))\n    if len(out_splits) > 1:\n        MAX = max(*[out_split.shape[0] for out_split in out_splits])\n    else:\n        MAX = out_splits[0].shape[0]\n    td = torch.stack([pad(out_split, [0, MAX - out_split.shape[0]]) for out_split in out_splits], 0)\n    return td",
        "description": ""
    }
]
2024-12-04 00:37:04
function recursive_map_to_cpu description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 00:37:41
parameter recursive_map_to_cpu description: 
[blue]This parameter is used to map a dictionary from the recursive data structure to a CPU-friendly format. It takes an OrderedDict as input and returns the same ordered dictionary, but with any nested dictionaries or lists converted to their CPU-friendly representations (e.g., NumPy arrays). This allows for efficient storage and processing of large datasets on multi-core CPUs.[/blue]
2024-12-04 00:38:14
class DataCollectorBase description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models.[/blue]
2024-12-04 00:39:01
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in a multi-agent environment, specifically when the policy and data collector live on different devices. It allows for efficient training of reinforcement learning models by synchronizing the policy weights between the data collector and the trained policy.[/blue]
2024-12-04 08:53:14
function recursive_map_to_cpu description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 08:53:42
parameter recursive_map_to_cpu description: 
[blue](bool) - Whether the static seed is used in the recursive map operation to CPU mapping. (default: False)[/blue]
2024-12-04 08:54:17
class DataCollectorBase description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models.[/blue]
2024-12-04 08:55:07
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in a multi-agent environment, specifically when the policy and data collector live on different devices. It allows for efficient training of reinforcement learning models by synchronizing the policy weights between the data collector and the trained policy.[/blue]
2024-12-04 08:57:51
function recursive_map_to_cpu description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 08:58:15
parameter dictionary description: 
[blue](dict) - A dictionary containing the mapping of CPU instructions to their corresponding machine code.[/blue]
2024-12-04 08:58:49
class DataCollectorBase description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models.[/blue]
2024-12-04 08:59:37
function update_policy_weights_ description: 
[blue]This function is used to update the weights of a policy in a multi-agent environment, specifically when the policy and data collector live on different devices. It allows for efficient training of reinforcement learning models by synchronizing the policy weights between the data collector and the trained policy.[/blue]
2024-12-04 09:00:00
parameter policy_weights description: 
[blue](bool) - Whether the static seed is used in updating the policy weights. (default: None)[/blue]
2024-12-04 09:00:33
function next description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 09:01:00
function shutdown description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 09:01:28
function iterator description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 09:01:57
function set_seed description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 09:02:21
parameter seed description: 
[blue](bool) - Whether the static seed is used. (default: False)
(int) - The random seed to be used. (default: None)[/blue]
2024-12-04 09:02:43
parameter static_seed description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 09:03:11
function state_dict description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 09:03:40
function load_state_dict description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 09:04:03
parameter state_dict description: 
[blue](bool) - Whether the static seed is used in a model loaded from a saved state dictionary. (default: False)[/blue]
2024-12-04 09:04:34
class SyncDataCollector description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models.[/blue]
2024-12-04 09:12:42
function __init__ description: 
[blue]This code appears to be a part of a reinforcement learning library, specifically designed for training agents in complex environments. Here's a breakdown of the code:

**Overview**

The code defines a class `Trainer` that takes various hyperparameters and environment configurations as input. It initializes an agent with a policy network, replay buffer, and other necessary components.

**Initialization**

The code starts by initializing the agent's policy network (`self.policy`) using the `_get_policy_and_device` method. This method returns the policy network and its device (e.g., GPU or CPU).

Next, it initializes the replay buffer (`self.replay_buffer`) based on the `use_buffers` flag.

**Environment Configuration**

The code then configures the environment (`self.env`) by setting its device to a specified value (`self.env_device`). If no device is provided, it defaults to the agent's policy device.

**Hyperparameter Tuning**

The code sets various hyperparameters for the training process, including:

* `max_frames_per_traj`: The maximum number of frames to collect per trajectory.
* `return_same_td`: A flag indicating whether to return the same TD (temporal difference) value for each frame in a trajectory.
* `set_truncated`: A flag indicating whether to truncate trajectories at a specified step count.

**Training Loop**

The code initializes an iterator (`self._iter`) and a counter (`self._frames`) to keep track of the training progress. It also sets up an interruptor (`self.interruptor`) to handle early stopping or other termination conditions.

**Final Rollout**

If `use_buffers` is True, the code performs a final rollout using the replay buffer (`self._make_final_rollout`).

**Truncated Keys**

The code sets truncated keys for the environment output spec (`self._set_truncated_keys`).

**Split Trajectories**

The code sets a flag (`split_trajs`) to indicate whether to split trajectories into separate files.

Overall, this code provides a flexible framework for training reinforcement learning agents in complex environments. It allows users to customize various hyperparameters and environment configurations to suit their specific use cases.[/blue]
2024-12-04 09:13:42
parameter create_env_fn description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 09:14:38
parameter policy description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 09:15:33
parameter frames_per_batch description: 
[blue](int) - The number of frames to include in each batch during training.[/blue]
2024-12-04 09:16:31
parameter total_frames description: 
[blue]<description>
(int) - The total number of frames to be used for training. If set to -1, it means no limit on the total number of frames.</description>[/blue]
2024-12-04 09:17:30
parameter device description: 
[blue]<description>
    (DEVICE_TYPING) - The device on which the environment is created.</description>[/blue]
2024-12-04 09:18:27
parameter storing_device description: 
[blue]<description>
    (DEVICE_TYPING) - The device on which the storing device is located.</description>[/blue]
2024-12-04 09:19:24
parameter policy_device description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 09:20:21
parameter env_device description: 
[blue]<description>
    (bool) - Whether the static seed is used. (default: False)</description>[/blue]
2024-12-04 09:21:17
parameter create_env_kwargs description: 
[blue](bool) - Whether to use the static seed when creating an environment. (default: False)[/blue]
2024-12-04 09:22:13
parameter max_frames_per_traj description: 
[blue]<description>
(int) - The maximum number of frames to consider per trajectory. (default: None)</description>[/blue]
2024-12-04 09:23:13
parameter init_random_frames description: 
[blue]<description>
(int) - The number of random frames to initialize the environment with.</description>[/blue]
2024-12-04 09:24:59
'function recursive_map_to_cpu' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 09:25:25
'parameter dictionary' description: 
[blue](bool) - Whether the dictionary is used for mapping recursive data structures to CPU representations.[/blue]
2024-12-04 09:25:56
'class DataCollectorBase' description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models.[/blue]
2024-12-04 09:26:34
'function update_policy_weights_' description: 
[blue]This function is used to update the weights of a policy in a multi-agent reinforcement learning model. It takes an optional argument `policy_weights` which can be a dictionary containing the current policy weights, and updates them according to the provided weights. The updated weights are then stored back into the policy instance for future use.[/blue]
2024-12-04 09:27:02
'parameter policy_weights' description: 
[blue](bool) - Whether the static seed is used in updating policy weights. (default: False)[/blue]
2024-12-04 09:27:29
'function next' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 09:27:55
'function shutdown' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 09:28:24
'function iterator' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 09:29:01
'function set_seed' description: 
[blue]This function is used to set the random seed for a PyTorch model or module instance before calling its parent class's `shutdown` method. It allows for reproducibility and consistency in training multi-agent reinforcement learning models by ensuring that the same sequence of random numbers is generated during each run.[/blue]
2024-12-04 09:29:32
'parameter seed' description: 
[blue](bool) - Whether the static seed is used. (default: False)
(int) - The random seed to be used. (default: None)[/blue]
2024-12-04 09:29:57
'parameter static_seed' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 09:30:27
'function state_dict' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 09:31:01
'function load_state_dict' description: 
[blue]This function is used to load a pre-trained model or module instance from a given state dictionary. It allows for the efficient transfer of weights and other model parameters between different PyTorch models or instances, making it a crucial component in various deep learning applications such as reinforcement learning and natural language processing.[/blue]
2024-12-04 09:31:24
'parameter state_dict' description: 
[blue](bool) - Whether the static seed is used.[/blue]
2024-12-04 09:31:58
'class SyncDataCollector' description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models.[/blue]
2024-12-04 09:33:09
'function __init__' description: 
[blue]This function is used to initialize and configure the data collector for multi-agent environments. It allows for efficient training of multi-agent reinforcement learning models by collecting and storing experiences from multiple agents in parallel. The function takes various parameters such as environment creation functions, policies, device specifications, and other configuration options to customize its behavior.[/blue]
2024-12-04 09:34:12
'parameter create_env_fn' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 09:35:11
'parameter policy' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 09:36:09
'parameter frames_per_batch' description: 
[blue](int) - The number of frames to include in each batch.[/blue]
2024-12-04 09:37:08
'parameter total_frames' description: 
[blue](int) - The total number of frames to be used for training. (optional, default: -1)[/blue]
2024-12-04 09:38:08
'parameter device' description: 
[blue](bool) - Whether the device is specified for environment creation. (optional)[/blue]
2024-12-04 09:39:10
'parameter storing_device' description: 
[blue](bool) - Whether the storing device is used. (default: None)[/blue]
2024-12-04 09:40:12
'parameter policy_device' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 09:41:09
'parameter env_device' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 09:43:03
'function recursive_map_to_cpu' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method. It takes an ordered dictionary as input, which represents the state of the model or module, and recursively maps over it to ensure that all necessary resources are released. The function returns the cleaned-up ordered dictionary, allowing for efficient training of multi-agent reinforcement learning models.[/blue]
2024-12-04 09:43:32
'parameter dictionary' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 09:44:16
'class DataCollectorBase' description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models. It provides methods such as `update_policy_weights_`, `next`, `shutdown`, `iterator`, `set_seed`, `state_dict`, and `load_state_dict` to facilitate the data collection process.[/blue]
2024-12-04 09:44:56
'function update_policy_weights_' description: 
[blue]This function is used to update the weights of a policy in a multi-agent environment. It takes an optional argument `policy_weights` which defaults to `None`, allowing for flexibility in how the weights are updated. The function returns `None`, indicating that it modifies the object's state directly without returning any value.[/blue]
2024-12-04 09:45:29
'parameter policy_weights' description: 
[blue](TensorDictBase) - The parameter "policy_weights" is used to update the weights of a policy in a TensorDict. (optional)[/blue]
2024-12-04 09:45:59
'function next' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 09:46:38
'function shutdown' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method. It provides an opportunity for subclasses to perform any necessary cleanup actions before the model is fully shut down, ensuring that all resources are released and the model is in a consistent state.[/blue]
2024-12-04 09:47:08
'function iterator' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 09:47:43
'function set_seed' description: 
[blue]This function is used to set the random seed for a PyTorch model or module instance. It takes an integer seed value and an optional boolean flag indicating whether to use a static seed. The function returns the same seed value that was passed in, allowing it to be used consistently throughout the training process.[/blue]
2024-12-04 09:48:10
'parameter seed' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 09:48:38
'parameter static_seed' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 09:49:12
'function state_dict' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method. It returns an ordered dictionary containing the current state of the model, which can be useful for debugging purposes.[/blue]
2024-12-04 09:49:55
'function load_state_dict' description: 
[blue]This function is used to load a pre-trained model or module instance from a given state dictionary. It allows for the efficient transfer of weights and other model parameters between different PyTorch models or instances, making it a crucial component in various deep learning applications such as reinforcement learning and natural language processing. The function takes an OrderedDict as input, which contains the model's weights and other parameters, and loads them into the current model instance, effectively restoring its state after training or saving.[/blue]
2024-12-04 09:50:23
'parameter state_dict' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 09:51:06
'class SyncDataCollector' description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models. It provides methods to iterate over the collected experiences, update policy weights, set a random seed, load and save model states, and finalize the shutdown of the collector before calling its parent class's `shutdown` method.[/blue]
2024-12-04 09:52:17
'function __init__' description: 
[blue]This function is used to initialize and configure the data collector for multi-agent environments. It allows for efficient training of multi-agent reinforcement learning models by collecting and storing experiences from multiple agents in parallel. The function takes various parameters such as environment creation functions, policies, device specifications, and other configuration options to customize its behavior.[/blue]
2024-12-04 09:53:18
'parameter create_env_fn' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 09:54:30
'parameter policy' description: 
[blue](bool) - Whether the static seed is used. (default: False)

(Optional[int]) - If provided, indicates the total number of frames returned by the collector during its lifespan. (default: -1 (never ending collector))[/blue]
2024-12-04 09:55:37
'parameter frames_per_batch' description: 
[blue](int) - If provided, indicates the total number of frames returned by the collector during its lifespan. (default: -1 (never ending collector))[/blue]
2024-12-04 09:56:40
'parameter total_frames' description: 
[blue](int) - If provided, indicates the total number of frames returned by the collector during its lifespan. (default: -1 (never ending collector))[/blue]
2024-12-04 09:57:44
'parameter device' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 09:58:52
'parameter storing_device' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 10:00:03
'parameter policy_device' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 10:01:14
'parameter env_device' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 10:02:33
'parameter create_env_kwargs' description: 
[blue](bool) - Whether the static seed is used. (default: False)

(Optional[int]) - If provided, indicates the total number of frames returned by the collector during its lifespan. (default: -1 (never ending collector))[/blue]
2024-12-04 10:03:45
'parameter max_frames_per_traj' description: 
[blue](int) - If provided, indicates the total number of frames returned by the collector during its lifespan. (default: -1 (never ending collector))[/blue]
2024-12-04 10:05:52
'function recursive_map_to_cpu' description: 
[blue]This function is used to recursively map keys in an ordered dictionary to CPU resources, allowing for efficient memory management and cleanup of PyTorch models or modules. It provides a way to handle complex data structures and optimize resource allocation during model shutdown.[/blue]
2024-12-04 10:06:23
'parameter dictionary' description: 
[blue](OrderedDict) - A dictionary data structure used to store and manipulate key-value pairs, providing an ordered view of the data. (optional)[/blue]
2024-12-04 10:06:55
'class DataCollectorBase' description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models.[/blue]
2024-12-04 10:07:32
'function update_policy_weights_' description: 
[blue]This function is used to update the weights of a policy in a multi-agent environment. It takes an optional argument `policy_weights`, which defaults to `None`. If provided, it updates the policy weights with the new values. The function does not return any value and modifies the internal state of the object it's called on.[/blue]
2024-12-04 10:08:02
'parameter policy_weights' description: 
[blue](TensorDictBase) - If provided, indicates the weights to be used for policy selection.[/blue]
2024-12-04 10:08:30
'function next' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 10:08:57
'function shutdown' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 10:09:26
'function iterator' description: 
[blue]This function is an iterator that yields TensorDictBase objects, allowing for efficient iteration over multiple agents in parallel and facilitating the training of multi-agent reinforcement learning models.[/blue]
2024-12-04 10:10:01
'function set_seed' description: 
[blue]This function is used to set the random seed for a given multi-agent environment, allowing for reproducibility and consistency in the training of reinforcement learning models. It takes an integer seed value and an optional boolean flag indicating whether to use a static seed or not. The function returns the set seed value.[/blue]
2024-12-04 10:10:29
'parameter seed' description: 
[blue](int) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 10:10:58
'parameter static_seed' description: 
[blue](bool) - Whether the static seed is used. (optional, default: False)[/blue]
2024-12-04 10:11:34
'function state_dict' description: 
[blue]This function is used to generate an ordered dictionary containing the model's state, which can be used for serialization and loading of the model. The state includes all the model's parameters and other relevant information. This functionality is commonly used in deep learning models, particularly those trained using reinforcement learning or other forms of machine learning.[/blue]
2024-12-04 10:12:06
'function load_state_dict' description: 
[blue]This function is used to load a pre-trained model's state dictionary into a given PyTorch model instance. It allows the model to resume training from a previously saved checkpoint, enabling efficient loading and reuse of existing model weights.[/blue]
2024-12-04 10:12:36
'parameter state_dict' description: 
[blue](OrderedDict) - The state dictionary, if provided, contains the model's learned parameters. (optional)[/blue]
2024-12-04 10:13:16
'class SyncDataCollector' description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models. It provides methods for iterating over the environment, updating policy weights, setting seeds, loading and saving state dictionaries, and shutting down the instance.[/blue]
2024-12-04 10:14:22
'function __init__' description: 
[blue]This function is used to initialize and configure an instance of a class designed for multi-agent environments. It allows users to customize various parameters such as environment creation, policy, device settings, and more, before creating the instance. The provided parameters enable flexible configuration options for training multi-agent reinforcement learning models.[/blue]
2024-12-04 10:15:01
'parameter create_env_fn' description: 
[blue](Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]]) - Whether to create an environment based on the provided function. If a callable is provided, it will be executed and its return value used as the environment; otherwise, if a class is provided, it will be instantiated and used as the environment; otherwise, no environment will be created.[/blue]
2024-12-04 10:15:32
'parameter policy' description: 
[blue](Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]])) - If provided, indicates the policy to be used for data collection.[/blue]
2024-12-04 10:16:00
'parameter frames_per_batch' description: 
[blue](int) - If provided, indicates the total number of frames returned by the collector during its lifespan. (default: -1 (never ending collector))[/blue]
2024-12-04 10:16:28
'parameter total_frames' description: 
[blue](int) - If provided, indicates the total number of frames returned by the collector during its lifespan. (optional, default: -1)[/blue]
2024-12-04 10:17:00
'parameter device' description: 
[blue]DEVICE_TYPING - The device parameter is used to specify the device or platform for which the static seed should be used, if applicable. (optional)[/blue]
2024-12-04 10:17:31
'parameter storing_device' description: 
[blue]DEVICE_TYPING - The storing device parameter indicates the type of storage device used to store data. (optional)[/blue]
2024-12-04 10:18:02
'parameter policy_device' description: 
[blue]DEVICE_TYPING - Specifies the device policy for the collector, indicating whether it should be used to determine the type of device.[/blue]
2024-12-04 10:18:29
'parameter env_device' description: 
[blue]DEVICE_TYPING - Specifies the device type for the environment.[/blue]
2024-12-04 10:19:00
'parameter create_env_kwargs' description: 
[blue](dict | None) - A dictionary of keyword arguments to be used when creating an environment, such as the static seed or collector settings. (optional: None)[/blue]
2024-12-04 10:19:33
'parameter max_frames_per_traj' description: 
[blue](int | None) - If provided, indicates the total number of frames returned by the collector during its lifespan. (default: None)[/blue]
2024-12-04 10:20:04
'parameter init_random_frames' description: 
[blue](int | None) - If provided, indicates the total number of frames returned by the collector during its lifespan. (default: None)[/blue]
2024-12-04 10:20:34
'parameter reset_at_each_iter' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 10:21:08
'parameter postproc' description: 
[blue](Callable[[TensorDictBase], TensorDictBase] | None) - Whether the post-processing is applied to the output of the collector. (optional: None)[/blue]
2024-12-04 10:21:36
'parameter split_trajs' description: 
[blue](bool) - Whether the static seed is used.[/blue]
2024-12-04 10:22:07
'parameter exploration_type' description: 
[blue](ExplorationType) - Whether the static seed is used. (optional: False)[/blue]
2024-12-04 10:22:36
'parameter return_same_td' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 10:23:06
'parameter reset_when_done' description: 
[blue](bool) - Whether to reset the static seed when the collector is done. (optional, default: True)[/blue]
2024-12-04 10:23:34
'parameter interruptor' description: 
[blue](bool) - Whether the static seed is used. (optional, default: False)[/blue]
2024-12-04 10:24:02
'parameter set_truncated' description: 
[blue](bool) - Whether to truncate the output of the collector. (optional, default: False)[/blue]
2024-12-04 10:24:28
'parameter use_buffers' description: 
[blue](bool) - Whether to use buffers for the static seed.[/blue]
2024-12-04 10:24:57
'parameter replay_buffer' description: 
[blue](ReplayBuffer | None) - If provided, indicates the total number of frames returned by the collector during its lifespan. (default: None)[/blue]
2024-12-04 10:25:24
'parameter trust_policy' description: 
[blue](bool) - Whether to trust the policy, indicating whether the static seed is used. (optional: None)[/blue]
2024-12-04 10:25:50
'parameter compile_policy' description: 
[blue](bool) - Whether the static seed is used. (optional: False)[/blue]
2024-12-04 10:26:18
'parameter cudagraph_policy' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 10:26:42
'function next' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 10:27:16
'function update_policy_weights_' description: 
[blue]This function is used to update the weights of a policy in a multi-agent environment. It takes an optional argument `policy_weights`, which defaults to `None`. If provided, it updates the policy weights with the new values. The function does not return any value and modifies the internal state of the object it's called on.[/blue]
2024-12-04 10:27:43
'parameter policy_weights' description: 
[blue](TensorDictBase) - If provided, indicates the weights to be used for policy selection.[/blue]
2024-12-04 10:28:16
'function set_seed' description: 
[blue]This function is used to set the random seed for a given multi-agent environment, allowing for reproducibility and consistency in the training of reinforcement learning models. It takes an integer seed value and an optional boolean flag indicating whether to use a static seed or not. The function returns the set seed value.[/blue]
2024-12-04 10:28:41
'parameter seed' description: 
[blue](int) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 10:29:07
'parameter static_seed' description: 
[blue](bool) - Whether the static seed is used. (optional, default: False)[/blue]
2024-12-04 10:29:33
'function iterator' description: 
[blue]This function is an iterator that yields TensorDictBase objects, allowing for efficient iteration over multiple agents in parallel and facilitating the training of multi-agent reinforcement learning models.[/blue]
2024-12-04 10:30:00
'function rollout' description: 
[blue]This function is used to generate experiences for training multi-agent reinforcement learning models by collecting and storing data from multiple agents in parallel. It returns a TensorDictBase object containing the collected experiences, allowing for efficient training of these complex models.[/blue]
2024-12-04 10:30:26
'function reset' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 10:30:54
'parameter index' description: 
[blue](Optional[int]) - If provided, indicates the total number of frames returned by the collector during its lifespan. (default: -1)[/blue]
2024-12-04 10:31:19
'function shutdown' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 10:31:51
'function state_dict' description: 
[blue]This function is used to generate an ordered dictionary containing the model's state, which can be used for serialization and loading of the model. The state includes all the model's parameters and other relevant information. This functionality is commonly used in deep learning models, particularly those trained using reinforcement learning or other forms of machine learning.[/blue]
2024-12-04 10:32:25
'function load_state_dict' description: 
[blue]This function is used to load a pre-trained model's state dictionary into the current model instance. It allows for efficient transfer of learned parameters from one model to another, enabling fine-tuning or updating of the existing model with new weights. The function takes an ordered dictionary containing the model's state and optional keyword arguments, and updates the model's internal state accordingly.[/blue]
2024-12-04 10:32:52
'parameter state_dict' description: 
[blue](OrderedDict) - The state dictionary, if provided, contains the model's learned parameters. (optional)[/blue]
2024-12-04 10:33:23
'class MultiSyncDataCollector' description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models.[/blue]
2024-12-04 10:33:49
'function next' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 10:34:15
'function shutdown' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 10:34:47
'function set_seed' description: 
[blue]This function is used to set the random seed for a given multi-agent environment, allowing for reproducibility and consistency in the training of reinforcement learning models. It takes an integer seed value and an optional boolean flag indicating whether to use a static seed or not. The function returns the set seed value.[/blue]
2024-12-04 10:35:13
'parameter seed' description: 
[blue](int) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 10:35:39
'parameter static_seed' description: 
[blue](bool) - Whether the static seed is used. (optional, default: False)[/blue]
2024-12-04 10:36:11
'function state_dict' description: 
[blue]This function is used to generate an ordered dictionary containing the model's state, which can be used for serialization and loading of the model. The state includes all the model's parameters and other relevant information. This functionality is commonly used in deep learning models, particularly those trained using reinforcement learning or other forms of machine learning.[/blue]
2024-12-04 10:36:41
'function load_state_dict' description: 
[blue]This function is used to load a pre-trained model's state dictionary into a given PyTorch model instance. It allows the model to resume training from a previously saved checkpoint, enabling efficient loading and reuse of existing model weights.[/blue]
2024-12-04 10:37:08
'parameter state_dict' description: 
[blue](OrderedDict) - The state dictionary, if provided, contains the model's learned parameters. (optional)[/blue]
2024-12-04 10:37:43
'function update_policy_weights_' description: 
[blue]This function is used to update the weights of a policy in a multi-agent environment. It takes an optional argument `policy_weights`, which defaults to `None`. If provided, it updates the policy weights with the new values. The function does not return any value and modifies the internal state of the object it's called on.[/blue]
2024-12-04 10:38:11
'parameter policy_weights' description: 
[blue](TensorDictBase) - If provided, indicates the weights to be used for policy selection.[/blue]
2024-12-04 10:38:44
'function frames_per_batch_worker' description: 
[blue]This function is used to generate frames per batch for a worker in a multi-agent environment, allowing for efficient training of reinforcement learning models. It likely takes into account the number of agents and the batch size to calculate the total number of frames per batch. The output can be used to optimize the training process by adjusting the frame rate or other hyperparameters.[/blue]
2024-12-04 10:39:10
'function iterator' description: 
[blue]This function is an iterator that yields TensorDictBase objects, allowing for efficient iteration over multiple agents in parallel and facilitating the training of multi-agent reinforcement learning models.[/blue]
2024-12-04 10:39:51
'class MultiaSyncDataCollector' description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models. It provides methods for initializing the collector, advancing through episodes, shutting down the collector, setting a random seed, loading saved state dictionaries, updating policy weights, controlling the number of frames per batch worker, iterating over data, resetting the collector, and accessing its internal state dictionary.[/blue]
2024-12-04 10:40:25
'function __init__' description: 
[blue]This function is used to initialize an object by setting up its attributes and preparing it for use. It allows the creation of objects with default values or custom initialization parameters, enabling flexible and efficient construction of instances. The function takes in variable arguments (`*args`) and keyword arguments (`**kwargs`), which can be used to customize the initialization process.[/blue]
2024-12-04 10:40:51
'function next' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 10:41:16
'function shutdown' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 10:41:47
'function set_seed' description: 
[blue]This function is used to set the random seed for a given multi-agent environment, allowing for reproducibility and consistency in the training of reinforcement learning models. It takes an integer seed value and an optional boolean flag indicating whether to use a static seed or not. The function returns the set seed value.[/blue]
2024-12-04 10:42:14
'parameter seed' description: 
[blue](int) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 10:42:40
'parameter static_seed' description: 
[blue](bool) - Whether the static seed is used. (optional, default: False)[/blue]
2024-12-04 10:43:11
'function state_dict' description: 
[blue]This function is used to generate an ordered dictionary containing the model's state, which can be used for serialization and loading of the model. The state includes all the model's parameters and other relevant information. This functionality is commonly used in deep learning models, particularly those trained using reinforcement learning or other forms of machine learning.[/blue]
2024-12-04 10:43:40
'function load_state_dict' description: 
[blue]This function is used to load a pre-trained model's state dictionary into a given PyTorch model instance. It allows the model to resume training from a previously saved checkpoint, enabling efficient loading and reuse of existing model weights.[/blue]
2024-12-04 10:44:08
'parameter state_dict' description: 
[blue](OrderedDict) - The state dictionary, if provided, contains the model's learned parameters. (optional)[/blue]
2024-12-04 10:44:45
'function update_policy_weights_' description: 
[blue]This function is used to update the weights of a policy in a multi-agent environment. It takes an optional argument `policy_weights`, which defaults to `None`. If provided, it updates the policy weights with the new values. The function does not return any value and modifies the internal state of the object it's called on.[/blue]
2024-12-04 10:45:17
'parameter policy_weights' description: 
[blue](TensorDictBase) - If provided, indicates the weights to be used for policy selection.[/blue]
2024-12-04 10:45:52
'function frames_per_batch_worker' description: 
[blue]This function is used to generate frames per batch for a worker in a multi-agent environment, allowing for efficient training of reinforcement learning models. It likely takes into account the number of agents and the batch size to calculate the total number of frames per batch. The output can be used to optimize the training process by adjusting the frame rate or other hyperparameters.[/blue]
2024-12-04 10:46:21
'function iterator' description: 
[blue]This function is an iterator that yields TensorDictBase objects, allowing for efficient iteration over multiple agents in parallel and facilitating the training of multi-agent reinforcement learning models.[/blue]
2024-12-04 10:46:51
'function reset' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 10:47:22
'parameter reset_idx' description: 
[blue](Optional[Sequence[bool]]) - If provided, indicates the indices of frames to reset in the collector. (default: None)[/blue]
2024-12-04 10:48:02
'class aSyncDataCollector' description: 
[blue]This class is an asynchronous data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models. It provides methods to initialize the collector, retrieve the next experience, shut down the collector, set a random seed for reproducibility, load a saved state dictionary, and finalize the shutdown process.[/blue]
2024-12-04 10:49:15
'function __init__' description: 
[blue]This function is used to initialize and configure an instance of a class designed for multi-agent environments. It allows users to specify various parameters such as environment creation functions, policy types, device configurations, and other settings that control the behavior of the class. The initialization process sets up the necessary resources and configurations for efficient training of multi-agent reinforcement learning models.[/blue]
2024-12-04 10:49:46
'parameter create_env_fn' description: 
[blue](Callable[[], EnvBase]) - A function that creates an environment object of type EnvBase.[/blue]
2024-12-04 10:50:20
'parameter policy' description: 
[blue](Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]])) - If provided, indicates the custom policy to be used for data collection.[/blue]
2024-12-04 10:50:52
'parameter frames_per_batch' description: 
[blue](int) - If provided, indicates the total number of frames returned by the collector during its lifespan. (default: -1 (never ending collector))[/blue]
2024-12-04 10:51:24
'parameter total_frames' description: 
[blue](Optional[int]) - If provided, indicates the total number of frames returned by the collector during its lifespan. (default: -1)[/blue]
2024-12-04 10:51:55
'parameter device' description: 
[blue]DEVICE_TYPING | Sequence of device types or None - Whether the static seed is used. (optional: False)[/blue]
2024-12-04 10:52:32
'parameter storing_device' description: 
[blue]DEVICE_TYPING | Sequence[DEVICE_TYPING] | None - The storing device parameter indicates the type of storage device to be used, which can be a specific device or a sequence of devices. If not provided, it defaults to None.[/blue]
2024-12-04 10:53:06
'parameter env_device' description: 
[blue]DEVICE_TYPING | Sequence[DEVICE_TYPING] | None - Specifies the device to use for environment collection, can be a single device or a sequence of devices.[/blue]
2024-12-04 10:53:40
'parameter policy_device' description: 
[blue]DEVICE_TYPING | Sequence[DEVICE_TYPING] | None - Specifies the device(s) to use for policy-related operations. If not provided, it defaults to no devices.[/blue]
2024-12-04 10:54:12
'parameter create_env_kwargs' description: 
[blue](Optional[Sequence[dict]]) - A dictionary of keyword arguments to be passed to the environment creation function.[/blue]
2024-12-04 10:54:44
'parameter max_frames_per_traj' description: 
[blue](int | None) - If provided, indicates the total number of frames returned by the collector during its lifespan. (default: None)[/blue]
2024-12-04 10:55:16
'parameter init_random_frames' description: 
[blue](int | None) - If provided, indicates the total number of frames returned by the collector during its lifespan. (default: None)[/blue]
2024-12-04 10:55:45
'parameter reset_at_each_iter' description: 
[blue](bool) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 10:56:18
'parameter postproc' description: 
[blue](Optional[Callable[[TensorDictBase], TensorDictBase]]) - Whether the post-processing function is used. (default: None)[/blue]
2024-12-04 10:56:48
'parameter split_trajs' description: 
[blue](bool) - Whether the trajectory data is split into separate files. (optional, default: None)[/blue]
2024-12-04 10:57:19
'parameter exploration_type' description: 
[blue](ExplorationType) - Whether the static seed is used. (optional: False)[/blue]
2024-12-04 10:57:50
'parameter reset_when_done' description: 
[blue](bool) - Whether to reset the static seed when the collector is done. (optional, default: True)[/blue]
2024-12-04 10:58:20
'parameter update_at_each_batch' description: 
[blue](bool) - Whether the static seed is used at each batch update. (default: False)[/blue]
2024-12-04 10:58:51
'parameter preemptive_threshold' description: 
[blue](float) - The minimum threshold value below which the system will preemptively terminate the process. (optional, default: None)[/blue]
2024-12-04 10:59:24
'parameter num_threads' description: 
[blue](int) - The total number of threads to use for the collector, indicating whether it will be a single-threaded or multi-threaded operation. (optional: None)[/blue]
2024-12-04 10:59:55
'parameter num_sub_threads' description: 
[blue](int) - The total number of sub-threads to be used for parallel processing. (optional, default: 1)[/blue]
2024-12-04 11:00:25
'parameter set_truncated' description: 
[blue](bool) - Whether to truncate the output of the collector. (optional, default: False)[/blue]
2024-12-04 11:00:53
'function next' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 11:01:20
'function shutdown' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 11:01:55
'function set_seed' description: 
[blue]This function is used to set the random seed for a given multi-agent environment, allowing for reproducibility and consistency in the training of reinforcement learning models. It takes an integer seed value and an optional boolean flag indicating whether to use a static seed or not. The function returns the set seed value.[/blue]
2024-12-04 11:02:24
'parameter seed' description: 
[blue](int) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 11:02:53
'parameter static_seed' description: 
[blue](bool) - Whether the static seed is used. (optional, default: False)[/blue]
2024-12-04 11:03:28
'function state_dict' description: 
[blue]This function is used to generate an ordered dictionary containing the model's state, which can be used for serialization and loading of the model. The state includes all the model's parameters and other relevant information. This functionality is commonly used in deep learning models, particularly those trained using reinforcement learning or other forms of machine learning.[/blue]
2024-12-04 11:04:00
'function load_state_dict' description: 
[blue]This function is used to load a pre-trained model's state dictionary into a given PyTorch model instance. It allows the model to resume training from a previously saved checkpoint, enabling efficient loading and reuse of existing model weights.[/blue]
2024-12-04 11:04:31
'parameter state_dict' description: 
[blue](OrderedDict) - The state dictionary, if provided, contains the model's learned parameters. (optional)[/blue]
2024-12-04 11:05:09
'function split_trajectories' description: 
[blue]This function is used to split a tensor dictionary representing trajectories into separate dictionaries for each agent in a multi-agent environment. It allows for efficient processing and analysis of experiences from multiple agents simultaneously, which is crucial for training multi-agent reinforcement learning models.[/blue]
2024-12-04 11:05:41
'parameter rollout_tensordict' description: 
[blue](TensorDictBase) - If provided, indicates the rollout dictionary used for training.[/blue]
2024-12-04 11:06:09
'parameter prefix' description: 
[blue](bool) - Whether a prefix is applied to the output. (optional, default: None)[/blue]
2024-12-04 11:06:39
'parameter trajectory_key' description: 
[blue](NestedKey | None) - If provided, indicates the key to use for trajectory data.[/blue]
2024-12-04 11:07:10
'parameter done_key' description: 
[blue](NestedKey | None) - If provided, indicates the key used to mark a static seed as done.[/blue]
2024-12-04 11:07:38
'parameter as_nested' description: 
[blue](bool) - Whether the static seed is used. (optional, default: False)[/blue]
2024-12-04 11:07:38
Total elapsed time (GenDescription Workflow): -62.32807078361511 min
2024-12-04 11:08:14
'function recursive_map_to_cpu' description: 
[blue]This function is used to recursively map keys in an ordered dictionary to CPU resources, allowing for efficient memory management and cleanup of PyTorch models or modules. It provides a way to handle complex data structures and optimize resource allocation during model shutdown.[/blue]
2024-12-04 11:08:43
'parameter dictionary' description: 
[blue](OrderedDict) - A dictionary data structure used to store and manipulate key-value pairs, providing an ordered view of the data. (optional)[/blue]
2024-12-04 11:09:16
'class DataCollectorBase' description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models.[/blue]
2024-12-04 11:09:51
'function update_policy_weights_' description: 
[blue]This function is used to update the weights of a policy in a multi-agent environment. It takes an optional argument `policy_weights`, which defaults to `None`. If provided, it updates the policy weights with the new values. The function does not return any value and modifies the internal state of the object it's called on.[/blue]
2024-12-04 11:10:20
'parameter policy_weights' description: 
[blue](TensorDictBase) - If provided, indicates the weights to be used for policy selection.[/blue]
2024-12-04 11:10:46
'function next' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 11:11:12
'function shutdown' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method.[/blue]
2024-12-04 11:11:39
'function iterator' description: 
[blue]This function is an iterator that yields TensorDictBase objects, allowing for efficient iteration over multiple agents in parallel and facilitating the training of multi-agent reinforcement learning models.[/blue]
2024-12-04 11:12:12
'function set_seed' description: 
[blue]This function is used to set the random seed for a given multi-agent environment, allowing for reproducibility and consistency in the training of reinforcement learning models. It takes an integer seed value and an optional boolean flag indicating whether to use a static seed or not. The function returns the set seed value.[/blue]
2024-12-04 11:12:38
'parameter seed' description: 
[blue](int) - Whether the static seed is used. (default: False)[/blue]
2024-12-04 11:13:06
'parameter static_seed' description: 
[blue](bool) - Whether the static seed is used. (optional, default: False)[/blue]
2024-12-04 11:13:38
'function state_dict' description: 
[blue]This function is used to generate an ordered dictionary containing the model's state, which can be used for serialization and loading of the model. The state includes all the model's parameters and other relevant information. This functionality is commonly used in deep learning models, particularly those trained using reinforcement learning or other forms of machine learning.[/blue]
2024-12-04 11:14:09
'function load_state_dict' description: 
[blue]This function is used to load a pre-trained model's state dictionary into a given PyTorch model instance. It allows the model to resume training from a previously saved checkpoint, enabling efficient loading and reuse of existing model weights.[/blue]
2024-12-04 11:14:37
'parameter state_dict' description: 
[blue](OrderedDict) - The state dictionary, if provided, contains the model's learned parameters. (optional)[/blue]
2024-12-04 11:15:14
'class SyncDataCollector' description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models. It provides methods for iterating over the environment, updating policy weights, setting seeds, loading and saving state dictionaries, and shutting down the instance.[/blue]
2024-12-04 11:16:21
'function __init__' description: 
[blue]This function is used to initialize and configure an instance of a class designed for multi-agent environments. It allows users to customize various parameters such as environment creation, policy, device settings, and more, before creating the instance. The provided parameters enable flexible configuration options for training multi-agent reinforcement learning models.[/blue]
2024-12-04 11:17:02
'parameter create_env_fn' description: 
[blue](Union[EnvBase, 'EnvCreator', Sequence[Callable[[], EnvBase]]]) - Whether to create an environment based on the provided function. If a callable is provided, it will be executed and its return value used as the environment; otherwise, if a class is provided, it will be instantiated and used as the environment; otherwise, no environment will be created.[/blue]
2024-12-04 11:17:34
'parameter policy' description: 
[blue](Optional[Union[TensorDictModule, Callable[[TensorDictBase], TensorDictBase]])) - If provided, indicates the policy to be used for data collection.[/blue]
2024-12-04 11:18:04
'parameter frames_per_batch' description: 
[blue](int) - If provided, indicates the total number of frames returned by the collector during its lifespan. (default: -1 (never ending collector))[/blue]
2024-12-04 11:18:33
'parameter total_frames' description: 
[blue](int) - If provided, indicates the total number of frames returned by the collector during its lifespan. (optional, default: -1)[/blue]
2024-12-04 11:19:02
'parameter device' description: 
[blue]DEVICE_TYPING - The device parameter is used to specify the device or platform for which the static seed should be used, if applicable. (optional)[/blue]
2024-12-04 11:19:31
'parameter storing_device' description: 
[blue]DEVICE_TYPING - The storing device parameter indicates the type of storage device used to store data. (optional)[/blue]
2024-12-04 11:20:00
'parameter policy_device' description: 
[blue]DEVICE_TYPING - Specifies the device policy for the collector, indicating whether it should be used to determine the type of device.[/blue]
2024-12-04 11:38:43
'function recursive_map_to_cpu' description: 
[blue]This function is used to clean up resources and finalize the shutdown of a PyTorch model or module instance before calling its parent class's `shutdown` method. It takes an ordered dictionary as input, which represents the state of the model or module, and returns a new ordered dictionary with the cleaned-up resources.[/blue]
2024-12-04 11:39:19
'parameter dictionary' description: 
[blue]This parameter is used to store and manage data in a structured format, allowing for efficient access and manipulation of key-value pairs. It provides a way to organize and retrieve data in a Python application, making it easier to implement complex logic and algorithms.[/blue]
2024-12-04 11:40:01
'class DataCollectorBase' description: 
[blue]This class is a data collector for multi-agent environments, designed to collect and store experiences from multiple agents in parallel, allowing for efficient training of multi-agent reinforcement learning models. It provides methods for updating policy weights, accessing the next experience, shutting down the collector, iterating over collected experiences, setting a seed, loading state dictionaries, and more.[/blue]
2024-12-04 11:40:40
'function update_policy_weights_' description: 
[blue]This function is used to update the weights of a policy in a multi-agent environment. It takes an optional argument `policy_weights` which can be provided to specify new weights for the policy, or it will use the default weights if not specified. The function returns nothing, indicating that it modifies the existing policy weights without returning any value.[/blue]
2024-12-04 11:41:13
'parameter policy_weights' description: 
[blue]This parameter is used to specify weights for policy updates in a multi-agent environment, allowing for control over the exploration-exploitation trade-off during training.[/blue]
2024-12-04 11:42:17
'function recursive_map_to_cpu' description: 
[blue]This function is used to recursively map values in an ordered dictionary to CPU frequencies, replacing the original values with their corresponding CPU frequency strings.[/blue]
2024-12-04 11:42:41
'parameter dictionary' description: 
[blue]This parameter is used to specify a dictionary that will be used as an ordered map. It can be used to store key-value pairs and provides methods for adding, removing, and accessing elements in the dictionary. The dictionary is initialized with an empty string if no value is provided.[/blue]
2024-12-04 11:43:04
'class DataCollectorBase' description: 
[blue]This class is a base implementation for data collectors in machine learning models, providing a foundation for collecting and storing data. It offers methods to update policy weights, iterate over the collected data, shut down the collector, set a seed for reproducibility, load state dictionaries, and load pre-trained models from saved state dictionaries.[/blue]
2024-12-04 11:43:28
'function update_policy_weights_' description: 
[blue]This function is used to update the weights of a policy in a reinforcement learning context, allowing for efficient and adaptive policy updates. It takes an optional parameter `policy_weights` which can be used to specify new weights for the policy, or it will use the existing weights if not provided. The function does not return any value, instead updating the internal state of the object it is called on.[/blue]
2024-12-04 11:43:46
'parameter policy_weights' description: 
[blue]This parameter is used to specify the weights assigned to each policy in a policy-based approach, allowing for customization of the model's behavior and decision-making process.[/blue]
2024-12-04 11:44:06
'function next' description: 
[blue]This function is used to move to the next item in a sequence or iterator, typically used when working with loops or iterating over data structures such as lists or generators. It advances the internal state of the object to point to the next element in the sequence, allowing for efficient iteration and processing of data.[/blue]
2024-12-04 11:44:23
'function shutdown' description: 
[blue]This function is used to initiate a shutdown process, likely terminating or closing various system resources, such as processes, connections, or applications, to bring the system to a safe state and prevent potential data loss or corruption.[/blue]
2024-12-04 11:44:38
'function iterator' description: 
[blue]This function is used to create an iterator for a TensorDictBase object, allowing it to be traversed and iterated over in a controlled manner.[/blue]
2024-12-04 11:44:58
'function set_seed' description: 
[blue]This function is used to initialize a random number generator with a specified seed value, allowing for reproducibility and control over the sequence of random numbers generated. The function takes an optional parameter 'static_seed' which defaults to False, indicating whether the seed should be set as static or not.[/blue]
2024-12-04 11:45:16
'parameter seed' description: 
[blue]This parameter is used to initialize the random number generator, allowing for reproducibility of results and enabling users to set a specific starting point for their random number generation.[/blue]
2024-12-04 11:45:33
'parameter static_seed' description: 
[blue]This parameter is used to enable or disable the use of a static seed for random number generation, allowing for reproducibility and deterministic behavior in simulations.[/blue]
2024-12-04 11:45:52
'function state_dict' description: 
[blue]This function is used to retrieve and return a dictionary representation of the model's state, which captures its current weights and other parameters. The returned dictionary can be used for serialization or loading purposes, allowing the model to be saved and restored at a later time.[/blue]
2024-12-04 11:46:15
'function load_state_dict' description: 
[blue]This function is used to load a pre-trained model's state dictionary into the current model instance, allowing for the restoration of previously trained weights and parameters. It takes an ordered dictionary containing the model's state as input and updates the internal state of the model with the loaded values, effectively reloading the model from a saved checkpoint or resume training from a previous point.[/blue]
2024-12-04 11:46:39
'parameter state_dict' description: 
[blue]This parameter is used to store and retrieve the model's state, which includes all the learned parameters and their values. It allows for saving and loading a trained model, enabling tasks such as model checkpointing, serialization, and deserialization. The state dictionary can be used to restore a model from a saved state, effectively reviving the model's weights and biases.[/blue]
2024-12-04 11:47:06
'class SyncDataCollector' description: 
[blue]This class is designed to collect and synchronize data from various sources, allowing for efficient and consistent data management in machine learning applications. It provides methods for iterating over data, updating policy weights, and resetting the collector, while also offering features such as seed setting and state loading. The SyncDataCollector class is intended to be a flexible and customizable tool for collecting and processing large datasets.[/blue]
2024-12-04 11:48:02
'function __init__' description: 
[blue]This function is used to initialize an instance of a class, typically in the context of reinforcement learning or deep learning environments. It allows users to customize various parameters and settings for the environment, such as the policy, device, and storage devices, before creating the environment. The function also provides options for controlling the behavior of the environment, including whether to use buffers, replay buffers, and interruptors.[/blue]
2024-12-04 11:48:27
'parameter create_env_fn' description: 
[blue]This parameter is used to specify a function that creates an environment. The function can either return an existing environment (if it exists) or create a new one if the provided name does not exist. If multiple functions are passed, they will be called in sequence until one of them returns an environment.[/blue]
2024-12-04 11:48:50
'parameter policy' description: 
[blue]This parameter is used to specify a policy that defines how the model should handle data. The policy can be either a module or a callable function that takes a tensor dictionary as input and returns a modified tensor dictionary. If no policy is provided, the default behavior will be applied.[/blue]
2024-12-04 11:49:06
'parameter frames_per_batch' description: 
[blue]This parameter is used to specify the number of frames that should be included in each batch when generating video content.[/blue]
2024-12-04 11:49:30
'parameter total_frames' description: 
[blue]This parameter is used to specify the total number of frames that should be rendered or processed by the application. A frame typically represents a single unit of animation or rendering, and this value determines how many frames will be handled in one go. If not provided, it defaults to -1, indicating that no specific limit should be enforced on the number of frames.[/blue]
2024-12-04 11:49:47
'parameter device' description: 
[blue]This parameter is used to specify the device or platform for which the code should be executed, allowing for more targeted and efficient testing or deployment.[/blue]
2024-12-04 11:50:08
'parameter storing_device' description: 
[blue]This parameter is used to specify the device where data will be stored, such as a hard drive or solid-state drive. It allows users to choose the storage location for their data, providing flexibility and control over how it is saved.[/blue]
2024-12-04 11:50:26
'parameter policy_device' description: 
[blue]This parameter is used to specify the device type for which a policy should be applied. It allows users to target specific devices, such as mobile phones or laptops, and apply policies accordingly.[/blue]
2024-12-04 11:50:47
'parameter env_device' description: 
[blue]This parameter is used to specify the device environment for a given operation, allowing users to control how their data is processed and stored. It can be set to a specific device type or left as default to use the system's default device environment.[/blue]
2024-12-04 11:51:04
'parameter create_env_kwargs' description: 
[blue]This parameter is used to pass keyword arguments to the environment creation process, allowing for customization of the environment's configuration and settings.[/blue]
2024-12-04 11:51:29
'parameter max_frames_per_traj' description: 
[blue]This parameter is used to specify the maximum number of frames allowed per trajectory in a video sequence. When set, it limits the length of each trajectory segment, effectively controlling the overall duration and complexity of the generated video. If not provided or set to its default value (None), the algorithm will generate trajectories without any frame limit.[/blue]
2024-12-04 11:51:49
'parameter init_random_frames' description: 
[blue]This parameter is used to initialize the random frames for a given process, allowing users to control the randomness of frame generation. When set to True, it will generate random frames; otherwise, it defaults to None, indicating no randomization.[/blue]
2024-12-04 11:52:10
'parameter reset_at_each_iter' description: 
[blue]This parameter is used to determine whether the iterator should be reset to its initial state at each iteration. If set to True, the iterator will be reset; otherwise, it will maintain its current state.[/blue]
2024-12-04 11:52:37
'parameter postproc' description: 
[blue]This parameter is used to specify a post-processing function that will be applied to the output of the model. The post-processing function should take a `TensorDictBase` object as input and return a modified version of it, which can then be used for further processing or output. If no post-processing function is provided, the default behavior will be to return `None`.[/blue]
2024-12-04 11:52:59
'parameter split_trajs' description: 
[blue]This parameter is used to split the trajectories into separate files, allowing for easier analysis and visualization of individual paths. When set to True, it will generate a new file for each trajectory in the input data, while when set to False (or None), it will keep all trajectories in a single file.[/blue]
2024-12-04 11:53:19
'parameter exploration_type' description: 
[blue]This parameter is used to specify the type of exploration to be performed, such as random or guided search. It determines how the algorithm will navigate through the search space and identify potential solutions.[/blue]
2024-12-04 11:53:42
'parameter return_same_td' description: 
[blue]This parameter is used to control the behavior of a tree data structure (TD) when returning values. If set to True, it indicates that the same TD should be returned for all elements in the sequence, rather than creating new instances. This can improve performance by reducing memory allocation and copying overhead.[/blue]
2024-12-04 11:53:59
'parameter reset_when_done' description: 
[blue]This parameter is used to determine whether the process should reset its state when it finishes executing, or if it should continue running with the current state.[/blue]
2024-12-04 11:54:24
'parameter interruptor' description: 
[blue]This parameter is used to control the flow of a program, allowing it to pause or terminate execution at specific points. It typically takes on a boolean value indicating whether an interrupt should be triggered or not. When set to True, the interruptor can be used to stop the program's normal execution and execute alternative code, often referred to as an "interrupt handler".[/blue]
2024-12-04 11:54:43
'parameter set_truncated' description: 
[blue]This parameter is used to specify whether the output should be truncated or not. When set to True, the output will be shortened to a specified length; otherwise, it will be returned in its entirety.[/blue]
2024-12-04 11:55:07
'parameter use_buffers' description: 
[blue]This parameter is used to enable or disable the use of buffers in a specific context, such as graphics rendering or data processing. When set to True, it allows for more efficient memory management and improved performance. If set to False or None (default value), it disables buffer usage, which may result in reduced performance but also lower memory usage.[/blue]
2024-12-04 11:55:34
'parameter replay_buffer' description: 
[blue]This parameter is used to specify a replay buffer, which is an internal data structure that stores the most recent experiences or interactions in a sequence. The replay buffer serves as a cache of past states and actions, allowing the algorithm to draw upon this cached information when generating new outputs or making predictions. By providing a replay buffer, you can control how much historical data is retained and used by the model during training or inference.[/blue]
2024-12-04 11:55:55
'parameter trust_policy' description: 
[blue]This parameter is used to specify a trust policy for the application, which determines how the application handles unknown or untrusted data. When set, it controls whether the application will accept and process untrusted data or reject it. If not specified, the default behavior applies.[/blue]
2024-12-04 11:56:18
'parameter compile_policy' description: 
[blue]This parameter is used to specify the compilation policy for a given module or package. The compilation policy determines how the compiler handles certain situations during the compilation process, such as handling exceptions or errors. It can be set to one of several predefined policies, which are typically defined in the compiler's configuration or documentation.[/blue]
2024-12-04 11:56:40
'parameter cudagraph_policy' description: 
[blue]This parameter is used to specify the policy for handling CUDA graph operations. It can take on three possible values: a boolean indicating whether to use CUDA graphs, a dictionary containing configuration options for CUDA graphs, or None to disable CUDA graph usage.[/blue]
2024-12-04 11:57:00
'function next' description: 
[blue]This function is used to move to the next item in a sequence or iterator, typically used when working with loops or iterating over data structures such as lists or generators. It advances the internal state of the object to point to the next element in the sequence, allowing for efficient iteration and processing of data.[/blue]
2024-12-04 11:57:24
'function update_policy_weights_' description: 
[blue]This function is used to update the weights of a policy in a reinforcement learning context, allowing for efficient and adaptive policy updates. It takes an optional parameter `policy_weights` which can be used to specify new weights for the policy, or it will use the existing weights if not provided. The function does not return any value, instead updating the internal state of the object it is called on.[/blue]
2024-12-04 11:57:42
'parameter policy_weights' description: 
[blue]This parameter is used to specify the weights assigned to each policy in a policy-based approach, allowing for customization of the model's behavior and decision-making process.[/blue]
2024-12-04 11:58:04
'function set_seed' description: 
[blue]This function is used to initialize a random number generator with a specified seed value, allowing for reproducibility and control over the sequence of random numbers generated. The function takes an optional parameter 'static_seed' which defaults to False, indicating whether the seed should be set as static or not.[/blue]
2024-12-04 11:58:21
'parameter seed' description: 
[blue]This parameter is used to initialize the random number generator, allowing for reproducibility of results and enabling users to set a specific starting point for their random number generation.[/blue]
2024-12-04 11:58:38
'parameter static_seed' description: 
[blue]This parameter is used to enable or disable the use of a static seed for random number generation, allowing for reproducibility and deterministic behavior in simulations.[/blue]
2024-12-04 11:58:54
'function iterator' description: 
[blue]This function is used to create an iterator for a TensorDictBase object, allowing it to be traversed and iterated over in a controlled manner.[/blue]
2024-12-04 11:59:16
'function rollout' description: 
[blue]This function is used to generate a rollout of an agent's policy, typically in the context of reinforcement learning or Markov decision processes. A rollout refers to the process of executing the agent's policy over a sequence of states and taking actions based on the policy's probabilities. The output of this function is usually a tensor representing the agent's cumulative reward over the rollout.[/blue]
2024-12-04 11:59:38
'function reset' description: 
[blue]This function is used to reset a specific element or state within an object, typically at a specified index. It allows for flexible and customizable resetting of various attributes or properties, as indicated by the presence of additional keyword arguments (kwargs). The function does not return any value and instead modifies the object's internal state directly.[/blue]
2024-12-04 11:59:58
'parameter index' description: 
[blue]This parameter is used to specify the index or position of an element within a sequence, such as a list, array, or string. It allows for efficient retrieval and manipulation of elements at specific positions in the sequence.[/blue]
2024-12-04 12:00:15
'function shutdown' description: 
[blue]This function is used to initiate a shutdown process, which may involve releasing system resources, closing connections, and terminating any ongoing operations to ensure a clean and safe exit from the program.[/blue]
2024-12-04 12:00:34
'function state_dict' description: 
[blue]This function is used to retrieve and return a dictionary representation of the model's state, which captures its current weights and other parameters. The returned dictionary can be used for serialization or loading purposes, allowing the model to be saved and restored at a later time.[/blue]
2024-12-04 12:00:57
'function load_state_dict' description: 
[blue]This function is used to load a pre-trained model's state dictionary into the current model instance, allowing for the restoration of previously trained weights and the resumption of training from a saved checkpoint. It takes an ordered dictionary representing the state of another model as input, along with any additional keyword arguments that may be required by the underlying model architecture.[/blue]
2024-12-04 12:01:21
'parameter state_dict' description: 
[blue]This parameter is used to store and retrieve the model's state, which includes all the learned parameters and their values. It allows for saving and loading a trained model, enabling tasks such as model checkpointing, serialization, and deserialization. The state dictionary can be used to restore a model from a saved state, effectively reviving the model's weights and biases.[/blue]
2024-12-04 12:01:44
'class MultiSyncDataCollector' description: 
[blue]This class is designed to collect and synchronize data from multiple sources, allowing for efficient and coordinated data collection across different environments or scenarios. It provides a flexible framework for gathering data in a structured and consistent manner, making it easier to integrate with various data pipelines and applications.[/blue]
2024-12-04 12:02:04
'function next' description: 
[blue]This function is used to move to the next item in a sequence or iterator, typically used when working with loops or iterating over data structures such as lists or generators. It advances the internal state of the object to point to the next element in the sequence, allowing for efficient iteration and processing of data.[/blue]
2024-12-04 12:02:20
'function shutdown' description: 
[blue]This function is used to initiate a shutdown process, likely terminating or closing various system resources, such as processes, connections, or applications, to bring the system to a safe state and prevent potential data loss or corruption.[/blue]
2024-12-04 12:02:41
'function set_seed' description: 
[blue]This function is used to initialize a random number generator with a specified seed value, allowing for reproducibility and control over the sequence of random numbers generated. The function takes an optional parameter 'static_seed' which defaults to False, indicating whether the seed should be set as static or not.[/blue]
2024-12-04 12:02:59
'parameter seed' description: 
[blue]This parameter is used to initialize the random number generator, allowing for reproducibility of results and enabling users to set a specific starting point for their random number generation.[/blue]
2024-12-04 12:03:16
'parameter static_seed' description: 
[blue]This parameter is used to enable or disable the use of a static seed for random number generation, allowing for reproducibility and deterministic behavior in simulations.[/blue]
2024-12-04 12:03:35
'function state_dict' description: 
[blue]This function is used to retrieve and return a dictionary representation of the model's state, which captures its current weights and other parameters. The returned dictionary can be used for serialization or loading purposes, allowing the model to be saved and restored at a later time.[/blue]
2024-12-04 12:03:58
'function load_state_dict' description: 
[blue]This function is used to load a pre-trained model's state dictionary into the current model instance, allowing for the restoration of previously trained weights and parameters. It takes an ordered dictionary containing the model's state as input and updates the internal state of the model with the loaded values, effectively reloading the model from a saved checkpoint or resume training from a previous point.[/blue]
2024-12-04 12:04:23
'parameter state_dict' description: 
[blue]This parameter is used to store and retrieve the model's state, which includes all the learned parameters and their values. It allows for saving and loading a trained model, enabling tasks such as model checkpointing, serialization, and deserialization. The state dictionary can be used to restore a model from a saved state, effectively reviving the model's weights and biases.[/blue]
2024-12-04 12:04:48
'function update_policy_weights_' description: 
[blue]This function is used to update the weights of a policy in a reinforcement learning context, allowing for efficient and adaptive policy updates. It takes an optional parameter `policy_weights` which can be used to specify new weights for the policy, or it will use the existing weights if not provided. The function does not return any value, instead updating the internal state of the object it is called on.[/blue]
2024-12-04 12:05:06
'parameter policy_weights' description: 
[blue]This parameter is used to specify the weights assigned to each policy in a policy-based approach, allowing for customization of the model's behavior and decision-making process.[/blue]
2024-12-04 12:05:23
'function frames_per_batch_worker' description: 
[blue]This function is responsible for processing a batch of frames and generating the corresponding number of frames per second, which can be used to calculate the frame rate or other related metrics.[/blue]
2024-12-04 12:05:38
'function iterator' description: 
[blue]This function is used to create an iterator for a TensorDictBase object, allowing it to be traversed and iterated over in a controlled manner.[/blue]
2024-12-04 12:06:07
'class MultiaSyncDataCollector' description: 
[blue]This class is designed to collect and synchronize data from multiple sources, allowing for efficient and coordinated data collection across different environments or simulations. It provides a flexible framework for managing data streams and ensuring consistency in the collected data. The class can be used to implement various data collection strategies, such as batch processing, streaming, or real-time data synchronization.[/blue]
2024-12-04 12:06:30
'function __init__' description: 
[blue]This function is used to initialize an object when it's created, allowing for the setting of default values and attributes before they're used in the class. It serves as a constructor, providing a way to set up the initial state of an object during its creation.[/blue]
2024-12-04 12:06:53
'function next' description: 
[blue]This function is used to move to the next item in a sequence or iterator, typically used when working with loops or iterating over data structures such as lists or generators. It advances the internal state of the object to point to the next element in the sequence, allowing for efficient iteration and processing of data.[/blue]
2024-12-04 12:07:09
'function shutdown' description: 
[blue]This function is used to initiate a shutdown process, likely terminating or closing various system resources, such as processes, connections, or applications, to bring the system to a safe state and prevent potential data loss or corruption.[/blue]
2024-12-04 12:07:30
'function set_seed' description: 
[blue]This function is used to initialize a random number generator with a specified seed value, allowing for reproducibility and control over the sequence of random numbers generated. The function takes an optional parameter 'static_seed' which defaults to False, indicating whether the seed should be set as static or not.[/blue]
2024-12-04 12:07:48
'parameter seed' description: 
[blue]This parameter is used to initialize the random number generator, allowing for reproducibility of results and enabling users to set a specific starting point for their random number generation.[/blue]
2024-12-04 12:08:06
'parameter static_seed' description: 
[blue]This parameter is used to enable or disable the use of a static seed for random number generation, allowing for reproducibility and deterministic behavior in simulations.[/blue]
2024-12-04 12:08:25
'function state_dict' description: 
[blue]This function is used to retrieve and return a dictionary representation of the model's state, which captures its current weights and other parameters. The returned dictionary can be used for serialization or loading purposes, allowing the model to be saved and restored at a later time.[/blue]
2024-12-04 12:08:48
'function load_state_dict' description: 
[blue]This function is used to load a pre-trained model's state dictionary into the current model instance, allowing for the restoration of previously trained weights and parameters. It takes an ordered dictionary containing the model's state as input and updates the internal state of the model with the loaded values, effectively reloading the model from a saved checkpoint or resume training from a previous point.[/blue]
2024-12-04 12:09:14
'parameter state_dict' description: 
[blue]This parameter is used to store and retrieve the model's state, which includes all the learned parameters and their values. It allows for saving and loading a trained model, enabling tasks such as model checkpointing, serialization, and deserialization. The state dictionary can be used to restore a model from a saved state, effectively reviving the model's weights and biases.[/blue]
2024-12-04 12:09:42
'function update_policy_weights_' description: 
[blue]This function is used to update the weights of a policy in a reinforcement learning context, allowing for efficient and adaptive policy updates. It takes an optional parameter `policy_weights` which can be used to specify new weights for the policy, or it will use the existing weights if not provided. The function does not return any value, instead updating the internal state of the object it is called on.[/blue]
2024-12-04 12:10:03
'parameter policy_weights' description: 
[blue]This parameter is used to specify the weights assigned to each policy in a policy-based approach, allowing for customization of the model's behavior and decision-making process.[/blue]
2024-12-04 12:10:21
'function frames_per_batch_worker' description: 
[blue]This function is responsible for processing a batch of frames and generating the corresponding number of frames per second, which can be used to calculate the frame rate or other related metrics.[/blue]
2024-12-04 12:10:39
'function iterator' description: 
[blue]This function is used to create an iterator for a TensorDictBase object, allowing it to be traversed and iterated over in a controlled manner.[/blue]
2024-12-04 12:11:07
'function reset' description: 
[blue]This function is used to reset the state of an object or system to its initial state, often after a specific operation or event has occurred. It takes an optional parameter `reset_idx` which can be used to specify the index or set of indices that should be reset. If no value is provided for `reset_idx`, the entire state may be reset. The function does not return any values and instead modifies the object's internal state directly.[/blue]
2024-12-04 12:11:31
'parameter reset_idx' description: 
[blue]This parameter is used to reset the index of a sequence, typically a list or array. When set to True, it will clear all existing values in the sequence and replace them with None. If not provided, the sequence remains unchanged.[/blue]
2024-12-04 12:11:58
'class aSyncDataCollector' description: 
[blue]This class is designed to asynchronously collect data from various sources, allowing for efficient and concurrent data gathering. It provides methods for initializing the collector, advancing to the next data point, shutting down the collection process, setting a random seed for reproducibility, loading pre-trained state dictionaries, and unloading state dictionaries when no longer needed.[/blue]
2024-12-04 12:12:55
'function __init__' description: 
[blue]This function is used to initialize an instance of a class, setting up the environment and policy parameters for use in reinforcement learning tasks. It takes various keyword arguments to customize its behavior, including options for device usage, environment creation, policy implementation, and more. The initialization process sets the stage for subsequent operations involving the created environment and policy.[/blue]
2024-12-04 12:13:23
'parameter create_env_fn' description: 
[blue]This parameter is used to specify a function that creates an environment object, which conforms to the EnvBase interface. The provided function should take no arguments and return an instance of an environment class that implements the EnvBase interface. If not specified, an empty string will be used as the default value for this parameter.[/blue]
2024-12-04 12:13:49
'parameter policy' description: 
[blue]This parameter is used to specify a policy that defines how the model should handle data. The policy can be either a module or a callable function that takes a tensor dictionary as input and returns a modified tensor dictionary. If no policy is provided, an empty string will be used by default.[/blue]
2024-12-04 12:14:08
'parameter frames_per_batch' description: 
[blue]This parameter is used to specify the number of frames that should be included in each batch when generating video content.[/blue]
2024-12-04 12:14:30
'parameter total_frames' description: 
[blue]This parameter is used to specify the total number of frames that should be rendered or processed by the application. If not provided, it defaults to -1, indicating that no limit on the number of frames should be enforced.[/blue]
2024-12-04 12:14:54
'parameter device' description: 
[blue]This parameter is used to specify the device or devices associated with a particular operation, allowing for more targeted and efficient execution. The value can be either a single device type or a sequence of device types, enabling flexibility in handling different hardware configurations.[/blue]
2024-12-04 12:15:17
'parameter storing_device' description: 
[blue]This parameter is used to specify the device where data will be stored, such as a file system or database. It can also accept a sequence of devices, allowing for more complex storage scenarios. If no value is provided, it defaults to None.[/blue]
2024-12-04 12:15:43
'parameter env_device' description: 
[blue]This parameter is used to specify the device environment for a given operation, allowing users to select from a list of available devices or provide a custom device typing. It can be set to either a single device type or a sequence of device types, and defaults to None if not provided.[/blue]
2024-12-04 12:16:08
'parameter policy_device' description: 
[blue]This parameter is used to specify the device type for which a policy should be applied. It can take on one of three values: a specific device type, a sequence of device types, or no value at all (in which case the policy will apply to all devices).[/blue]
2024-12-04 12:16:28
'parameter create_env_kwargs' description: 
[blue]This parameter is used to pass keyword arguments to the environment creation process, allowing for customization of the environment's configuration and settings.[/blue]
2024-12-04 12:16:55
'parameter max_frames_per_traj' description: 
[blue]This parameter is used to specify the maximum number of frames allowed per trajectory in a video sequence. When set, it limits the length of each trajectory segment, effectively controlling the overall duration and complexity of the generated video. If not provided or set to its default value (None), the algorithm will generate trajectories without any frame limit.[/blue]
2024-12-04 12:17:19
'parameter init_random_frames' description: 
[blue]This parameter is used to initialize the random frames for a given process, allowing users to control the randomness of frame generation. When set to True, it will generate random frames; otherwise, it defaults to None, indicating no randomization.[/blue]
2024-12-04 12:17:41
'parameter reset_at_each_iter' description: 
[blue]This parameter is used to determine whether the iterator should be reset to its initial state at each iteration. If set to True, the iterator will be reset; otherwise, it will maintain its current state.[/blue]
2024-12-04 12:18:10
'parameter postproc' description: 
[blue]This parameter is used to specify a post-processing function that will be applied to the output of the model. The post-processing function takes in a `TensorDictBase` object as input and returns a modified version of it, which can then be used for further processing or output. If no post-processing function is provided, the default value of `None` will be used.[/blue]
2024-12-04 12:18:30
'parameter split_trajs' description: 
[blue]This parameter is used to split the trajectories into separate files, allowing for easier analysis and processing of individual trajectory data.[/blue]
2024-12-04 12:18:52
'parameter exploration_type' description: 
[blue]This parameter is used to specify the type of exploration to be performed, such as random or guided search. It determines how the algorithm will navigate through the search space and identify potential solutions.[/blue]
2024-12-04 12:19:12
'parameter reset_when_done' description: 
[blue]This parameter is used to determine whether the process should reset its state when it finishes executing, or if it should continue running with the current state.[/blue]
2024-12-04 12:19:38
'parameter update_at_each_batch' description: 
[blue]This parameter is used to specify whether the model should update its internal state at each batch of input data during training. When set to True, the model will update its weights and biases after processing each batch; when set to False, it will only update after all batches have been processed.[/blue]
2024-12-04 12:20:01
'parameter preemptive_threshold' description: 
[blue]This parameter is used to set the threshold for preemptive scheduling, determining when the scheduler should take control of a process or thread to prevent it from consuming excessive CPU resources.[/blue]
2024-12-04 12:20:21
'parameter num_threads' description: 
[blue]This parameter is used to specify the number of threads that should be created for concurrent execution, allowing for improved performance and efficiency in multi-threaded applications.[/blue]
2024-12-04 12:20:47
'parameter num_sub_threads' description: 
[blue]This parameter is used to specify the number of sub-threads that will be created when using multi-threading functionality. The value of this parameter determines how many threads will be spawned, allowing for more efficient use of system resources and improved performance in certain applications.[/blue]
2024-12-04 12:21:10
'parameter set_truncated' description: 
[blue]This parameter is used to specify whether the output should be truncated or not. When set to True, the output will be shortened to a specified length; otherwise, it will be returned in its entirety.[/blue]
2024-12-04 12:21:33
'function next' description: 
[blue]This function is used to move to the next item in a sequence or iterator, typically used when working with loops or iterating over data structures such as lists or generators. It advances the internal state of the object to point to the next element in the sequence, allowing for efficient iteration and processing of data.[/blue]
2024-12-04 12:21:52
'function shutdown' description: 
[blue]This function is used to initiate a shutdown process, likely terminating or closing various system resources, such as processes, connections, or applications, to bring the system to a safe state and prevent potential data loss or corruption.[/blue]
2024-12-04 12:22:17
'function set_seed' description: 
[blue]This function is used to initialize a random number generator with a specified seed value, allowing for reproducibility and control over the sequence of random numbers generated. The function takes an optional parameter 'static_seed' which defaults to False, indicating whether the seed should be set as static or not.[/blue]
2024-12-04 12:22:38
'parameter seed' description: 
[blue]This parameter is used to initialize the random number generator, allowing for reproducibility of results and enabling users to set a specific starting point for their random number generation.[/blue]
2024-12-04 12:22:59
'parameter static_seed' description: 
[blue]This parameter is used to enable or disable the use of a static seed for random number generation, allowing for reproducibility and deterministic behavior in simulations.[/blue]
2024-12-04 12:23:22
'function state_dict' description: 
[blue]This function is used to retrieve and return a dictionary representation of the model's state, which captures its current weights and other parameters. The returned dictionary can be used for serialization or loading purposes, allowing the model to be saved and restored at a later time.[/blue]
2024-12-04 12:23:49
'function load_state_dict' description: 
[blue]This function is used to load a pre-trained model's state dictionary into the current model instance, allowing for the restoration of previously trained weights and parameters. It takes an ordered dictionary containing the model's state as input and updates the internal state of the model with the loaded values, effectively reloading the model from a saved checkpoint or resume training from a previous point.[/blue]
2024-12-04 12:24:18
'parameter state_dict' description: 
[blue]This parameter is used to store and retrieve the model's state, which includes all the learned parameters and their values. It allows for saving and loading a trained model, enabling tasks such as model checkpointing, serialization, and deserialization. The state dictionary can be used to restore a model from a saved state, effectively reviving the model's weights and biases.[/blue]
2024-12-04 12:24:45
'function split_trajectories' description: 
[blue]This function is used to split a tensor dictionary representing trajectories into separate dictionaries based on specific keys. It allows for customization through the use of optional parameters such as prefix and trajectory_key, enabling flexible manipulation of the input data structure.[/blue]
2024-12-04 12:25:11
'parameter rollout_tensordict' description: 
[blue]This parameter is used to specify a tensor dictionary that will be used as the rollout strategy for the model. The rollout strategy determines how the model's weights are updated during training, and this parameter allows users to customize this process by providing their own tensor dictionary.[/blue]
2024-12-04 12:25:30
'parameter prefix' description: 
[blue]This parameter is used to specify a prefix for the output, which can be useful for formatting or logging purposes.[/blue]
2024-12-04 12:25:51
'parameter trajectory_key' description: 
[blue]This parameter is used to specify a unique key for the trajectory data, allowing it to be identified and accessed within the system.[/blue]
2024-12-04 12:26:11
'parameter done_key' description: 
[blue]This parameter is used to specify a unique key that indicates the completion of a task or operation, allowing for efficient tracking and management of progress.[/blue]
2024-12-04 12:26:38
'parameter as_nested' description: 
[blue]This parameter is used to specify whether the result should be returned as a nested dictionary or not. When set to True, the result will be wrapped in an additional level of nesting, with the outermost key being "result". If False (the default), the result will be returned without any extra nesting.[/blue]
2024-12-04 12:26:38
Total elapsed time (GenDescription Workflow): -44.644157449404396 min
2024-12-04 12:27:00
'function recursive_map_to_cpu' description: 
[blue]This function is used to recursively map values in an ordered dictionary to CPU frequencies, replacing the original values with their corresponding CPU frequency strings.[/blue]
2024-12-04 12:27:26
'parameter dictionary' description: 
[blue]This parameter is used to specify a dictionary that will be used as an ordered map. It can be used to store key-value pairs and provides methods for adding, removing, and accessing elements in the dictionary. The dictionary is initialized with an empty string if no value is provided.[/blue]
